Args in experiment:
Namespace(is_training=1, model_id='electricity_720_336', model='FITS', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=321, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=258, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : electricity_720_336_FITS_custom_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=258, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4007066112.0
params:  97902.0
Trainable parameters:  97902
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8151152
	speed: 0.3022s/iter; left time: 1194.0897s
Epoch: 1 cost time: 39.87059497833252
Epoch: 1, Steps: 135 | Train Loss: 0.9550937 Vali Loss: 0.7200618 Test Loss: 0.8366548
Validation loss decreased (inf --> 0.720062).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6958449
	speed: 0.6114s/iter; left time: 2333.2012s
Epoch: 2 cost time: 36.748165130615234
Epoch: 2, Steps: 135 | Train Loss: 0.7184840 Vali Loss: 0.6335607 Test Loss: 0.7383969
Validation loss decreased (0.720062 --> 0.633561).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6014885
	speed: 0.6274s/iter; left time: 2309.5275s
Epoch: 3 cost time: 39.754531145095825
Epoch: 3, Steps: 135 | Train Loss: 0.6221580 Vali Loss: 0.5706171 Test Loss: 0.6674466
Validation loss decreased (0.633561 --> 0.570617).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5295357
	speed: 0.6134s/iter; left time: 2175.0671s
Epoch: 4 cost time: 38.20365881919861
Epoch: 4, Steps: 135 | Train Loss: 0.5451371 Vali Loss: 0.5136360 Test Loss: 0.6028654
Validation loss decreased (0.570617 --> 0.513636).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4550974
	speed: 0.6511s/iter; left time: 2220.7616s
Epoch: 5 cost time: 40.50276279449463
Epoch: 5, Steps: 135 | Train Loss: 0.4811106 Vali Loss: 0.4674017 Test Loss: 0.5506656
Validation loss decreased (0.513636 --> 0.467402).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4169649
	speed: 0.6155s/iter; left time: 2016.3711s
Epoch: 6 cost time: 38.328813314437866
Epoch: 6, Steps: 135 | Train Loss: 0.4271300 Vali Loss: 0.4260387 Test Loss: 0.5041128
Validation loss decreased (0.467402 --> 0.426039).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3660354
	speed: 0.6245s/iter; left time: 1961.5257s
Epoch: 7 cost time: 38.20401620864868
Epoch: 7, Steps: 135 | Train Loss: 0.3811743 Vali Loss: 0.3911221 Test Loss: 0.4642569
Validation loss decreased (0.426039 --> 0.391122).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3335457
	speed: 0.6062s/iter; left time: 1822.3219s
Epoch: 8 cost time: 37.67539858818054
Epoch: 8, Steps: 135 | Train Loss: 0.3418693 Vali Loss: 0.3623507 Test Loss: 0.4317394
Validation loss decreased (0.391122 --> 0.362351).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3045941
	speed: 0.6267s/iter; left time: 1799.2302s
Epoch: 9 cost time: 38.79499053955078
Epoch: 9, Steps: 135 | Train Loss: 0.3079844 Vali Loss: 0.3343762 Test Loss: 0.3996884
Validation loss decreased (0.362351 --> 0.334376).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2697123
	speed: 0.5986s/iter; left time: 1637.6671s
Epoch: 10 cost time: 36.774083614349365
Epoch: 10, Steps: 135 | Train Loss: 0.2787716 Vali Loss: 0.3111383 Test Loss: 0.3728986
Validation loss decreased (0.334376 --> 0.311138).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2484013
	speed: 0.6107s/iter; left time: 1588.5577s
Epoch: 11 cost time: 37.03312087059021
Epoch: 11, Steps: 135 | Train Loss: 0.2534193 Vali Loss: 0.2933332 Test Loss: 0.3525009
Validation loss decreased (0.311138 --> 0.293333).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2300840
	speed: 0.5947s/iter; left time: 1466.4749s
Epoch: 12 cost time: 37.88480520248413
Epoch: 12, Steps: 135 | Train Loss: 0.2312918 Vali Loss: 0.2757195 Test Loss: 0.3321809
Validation loss decreased (0.293333 --> 0.275719).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2114458
	speed: 0.6131s/iter; left time: 1429.0627s
Epoch: 13 cost time: 36.935948848724365
Epoch: 13, Steps: 135 | Train Loss: 0.2120555 Vali Loss: 0.2589236 Test Loss: 0.3126763
Validation loss decreased (0.275719 --> 0.258924).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1880737
	speed: 0.6084s/iter; left time: 1336.0581s
Epoch: 14 cost time: 37.29149651527405
Epoch: 14, Steps: 135 | Train Loss: 0.1952008 Vali Loss: 0.2458280 Test Loss: 0.2976071
Validation loss decreased (0.258924 --> 0.245828).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1752534
	speed: 0.6214s/iter; left time: 1280.6968s
Epoch: 15 cost time: 38.25200605392456
Epoch: 15, Steps: 135 | Train Loss: 0.1804436 Vali Loss: 0.2346724 Test Loss: 0.2842648
Validation loss decreased (0.245828 --> 0.234672).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1630438
	speed: 0.6372s/iter; left time: 1227.2513s
Epoch: 16 cost time: 39.1680748462677
Epoch: 16, Steps: 135 | Train Loss: 0.1674581 Vali Loss: 0.2241260 Test Loss: 0.2716217
Validation loss decreased (0.234672 --> 0.224126).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1515758
	speed: 0.5977s/iter; left time: 1070.4756s
Epoch: 17 cost time: 37.06178569793701
Epoch: 17, Steps: 135 | Train Loss: 0.1560277 Vali Loss: 0.2148195 Test Loss: 0.2609658
Validation loss decreased (0.224126 --> 0.214820).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1351482
	speed: 0.6171s/iter; left time: 1021.8836s
Epoch: 18 cost time: 38.42319178581238
Epoch: 18, Steps: 135 | Train Loss: 0.1459736 Vali Loss: 0.2075535 Test Loss: 0.2519257
Validation loss decreased (0.214820 --> 0.207554).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1338791
	speed: 0.5982s/iter; left time: 909.8707s
Epoch: 19 cost time: 36.95394539833069
Epoch: 19, Steps: 135 | Train Loss: 0.1370281 Vali Loss: 0.2000279 Test Loss: 0.2434340
Validation loss decreased (0.207554 --> 0.200028).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1250821
	speed: 0.6111s/iter; left time: 846.9610s
Epoch: 20 cost time: 38.254661321640015
Epoch: 20, Steps: 135 | Train Loss: 0.1291920 Vali Loss: 0.1943760 Test Loss: 0.2361903
Validation loss decreased (0.200028 --> 0.194376).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1172778
	speed: 0.6293s/iter; left time: 787.2502s
Epoch: 21 cost time: 39.454649925231934
Epoch: 21, Steps: 135 | Train Loss: 0.1221893 Vali Loss: 0.1887118 Test Loss: 0.2297525
Validation loss decreased (0.194376 --> 0.188712).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1119600
	speed: 0.6503s/iter; left time: 725.7039s
Epoch: 22 cost time: 39.605042457580566
Epoch: 22, Steps: 135 | Train Loss: 0.1159794 Vali Loss: 0.1833876 Test Loss: 0.2232334
Validation loss decreased (0.188712 --> 0.183388).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1135929
	speed: 0.6038s/iter; left time: 592.3509s
Epoch: 23 cost time: 37.12632632255554
Epoch: 23, Steps: 135 | Train Loss: 0.1104681 Vali Loss: 0.1795942 Test Loss: 0.2183919
Validation loss decreased (0.183388 --> 0.179594).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.0998031
	speed: 0.6226s/iter; left time: 526.7215s
Epoch: 24 cost time: 37.45975637435913
Epoch: 24, Steps: 135 | Train Loss: 0.1055557 Vali Loss: 0.1760898 Test Loss: 0.2140872
Validation loss decreased (0.179594 --> 0.176090).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.0954170
	speed: 0.5930s/iter; left time: 421.6526s
Epoch: 25 cost time: 36.96847891807556
Epoch: 25, Steps: 135 | Train Loss: 0.1011894 Vali Loss: 0.1724377 Test Loss: 0.2098471
Validation loss decreased (0.176090 --> 0.172438).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1029134
	speed: 0.6122s/iter; left time: 352.6433s
Epoch: 26 cost time: 37.72309374809265
Epoch: 26, Steps: 135 | Train Loss: 0.0972684 Vali Loss: 0.1721231 Test Loss: 0.2056173
Validation loss decreased (0.172438 --> 0.172123).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.0912122
	speed: 0.6565s/iter; left time: 289.5221s
Epoch: 27 cost time: 41.489073038101196
Epoch: 27, Steps: 135 | Train Loss: 0.0938288 Vali Loss: 0.1666285 Test Loss: 0.2021361
Validation loss decreased (0.172123 --> 0.166628).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.0904498
	speed: 0.6224s/iter; left time: 190.4547s
Epoch: 28 cost time: 37.434261322021484
Epoch: 28, Steps: 135 | Train Loss: 0.0907027 Vali Loss: 0.1640372 Test Loss: 0.1994026
Validation loss decreased (0.166628 --> 0.164037).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.0908263
	speed: 0.6233s/iter; left time: 106.5820s
Epoch: 29 cost time: 39.302165269851685
Epoch: 29, Steps: 135 | Train Loss: 0.0879109 Vali Loss: 0.1622563 Test Loss: 0.1967907
Validation loss decreased (0.164037 --> 0.162256).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.0862130
	speed: 0.6145s/iter; left time: 22.1212s
Epoch: 30 cost time: 37.542245626449585
Epoch: 30, Steps: 135 | Train Loss: 0.0854204 Vali Loss: 0.1598222 Test Loss: 0.1939667
Validation loss decreased (0.162256 --> 0.159822).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=258, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4007066112.0
params:  97902.0
Trainable parameters:  97902
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.1680911
	speed: 0.2863s/iter; left time: 1131.0709s
Epoch: 1 cost time: 37.699639558792114
Epoch: 1, Steps: 135 | Train Loss: 0.1783198 Vali Loss: 0.1435364 Test Loss: 0.1729942
Validation loss decreased (inf --> 0.143536).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1752811
	speed: 0.5954s/iter; left time: 2271.9955s
Epoch: 2 cost time: 37.188183546066284
Epoch: 2, Steps: 135 | Train Loss: 0.1747573 Vali Loss: 0.1435055 Test Loss: 0.1727269
Validation loss decreased (0.143536 --> 0.143506).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1657912
	speed: 0.6516s/iter; left time: 2398.5870s
Epoch: 3 cost time: 39.853962421417236
Epoch: 3, Steps: 135 | Train Loss: 0.1744634 Vali Loss: 0.1433178 Test Loss: 0.1726001
Validation loss decreased (0.143506 --> 0.143318).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1877647
	speed: 0.6303s/iter; left time: 2234.9216s
Epoch: 4 cost time: 38.981303215026855
Epoch: 4, Steps: 135 | Train Loss: 0.1744596 Vali Loss: 0.1431689 Test Loss: 0.1725694
Validation loss decreased (0.143318 --> 0.143169).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1677763
	speed: 0.6194s/iter; left time: 2112.8620s
Epoch: 5 cost time: 36.70465564727783
Epoch: 5, Steps: 135 | Train Loss: 0.1743522 Vali Loss: 0.1430486 Test Loss: 0.1725348
Validation loss decreased (0.143169 --> 0.143049).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1748490
	speed: 0.6161s/iter; left time: 2018.4778s
Epoch: 6 cost time: 39.74448251724243
Epoch: 6, Steps: 135 | Train Loss: 0.1743037 Vali Loss: 0.1429796 Test Loss: 0.1725037
Validation loss decreased (0.143049 --> 0.142980).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1725613
	speed: 0.6086s/iter; left time: 1911.5430s
Epoch: 7 cost time: 37.042702198028564
Epoch: 7, Steps: 135 | Train Loss: 0.1742413 Vali Loss: 0.1429049 Test Loss: 0.1723501
Validation loss decreased (0.142980 --> 0.142905).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1705673
	speed: 0.6255s/iter; left time: 1880.3284s
Epoch: 8 cost time: 38.13727426528931
Epoch: 8, Steps: 135 | Train Loss: 0.1743146 Vali Loss: 0.1430980 Test Loss: 0.1724538
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1793041
	speed: 0.5938s/iter; left time: 1704.9167s
Epoch: 9 cost time: 36.6745331287384
Epoch: 9, Steps: 135 | Train Loss: 0.1742593 Vali Loss: 0.1430641 Test Loss: 0.1723777
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1769371
	speed: 0.5777s/iter; left time: 1580.6566s
Epoch: 10 cost time: 36.55862355232239
Epoch: 10, Steps: 135 | Train Loss: 0.1742095 Vali Loss: 0.1428416 Test Loss: 0.1723489
Validation loss decreased (0.142905 --> 0.142842).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1757832
	speed: 0.5542s/iter; left time: 1441.5863s
Epoch: 11 cost time: 34.11160111427307
Epoch: 11, Steps: 135 | Train Loss: 0.1741302 Vali Loss: 0.1430670 Test Loss: 0.1723688
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1672384
	speed: 0.5670s/iter; left time: 1398.2926s
Epoch: 12 cost time: 34.63837289810181
Epoch: 12, Steps: 135 | Train Loss: 0.1741604 Vali Loss: 0.1430927 Test Loss: 0.1723570
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1754690
	speed: 0.5251s/iter; left time: 1224.0073s
Epoch: 13 cost time: 34.954018354415894
Epoch: 13, Steps: 135 | Train Loss: 0.1741445 Vali Loss: 0.1433085 Test Loss: 0.1723129
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1806098
	speed: 0.5858s/iter; left time: 1286.3120s
Epoch: 14 cost time: 35.56212401390076
Epoch: 14, Steps: 135 | Train Loss: 0.1741307 Vali Loss: 0.1429737 Test Loss: 0.1723183
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1738571
	speed: 0.6023s/iter; left time: 1241.3065s
Epoch: 15 cost time: 38.777793169021606
Epoch: 15, Steps: 135 | Train Loss: 0.1741026 Vali Loss: 0.1428156 Test Loss: 0.1723378
Validation loss decreased (0.142842 --> 0.142816).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1730009
	speed: 0.5625s/iter; left time: 1083.3178s
Epoch: 16 cost time: 33.105082273483276
Epoch: 16, Steps: 135 | Train Loss: 0.1741277 Vali Loss: 0.1431115 Test Loss: 0.1723131
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1763323
	speed: 0.5613s/iter; left time: 1005.2609s
Epoch: 17 cost time: 35.674553632736206
Epoch: 17, Steps: 135 | Train Loss: 0.1741036 Vali Loss: 0.1432794 Test Loss: 0.1723155
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1687059
	speed: 0.5484s/iter; left time: 908.2247s
Epoch: 18 cost time: 34.09826374053955
Epoch: 18, Steps: 135 | Train Loss: 0.1740736 Vali Loss: 0.1429562 Test Loss: 0.1723101
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1729538
	speed: 0.5594s/iter; left time: 850.8321s
Epoch: 19 cost time: 36.524367332458496
Epoch: 19, Steps: 135 | Train Loss: 0.1740954 Vali Loss: 0.1431364 Test Loss: 0.1722864
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1817574
	speed: 0.5669s/iter; left time: 785.6982s
Epoch: 20 cost time: 35.435606479644775
Epoch: 20, Steps: 135 | Train Loss: 0.1740873 Vali Loss: 0.1429753 Test Loss: 0.1723213
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : electricity_720_336_FITS_custom_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.16913693737983704, mae:0.269738751411438, rse:0.4068904519081116, corr:[0.4585453  0.4615653  0.46293932 0.46311718 0.4636589  0.46374542
 0.46365958 0.46369705 0.46344477 0.46341807 0.46343154 0.4633818
 0.46332967 0.463193   0.46320048 0.46328983 0.46327558 0.46324763
 0.4631231  0.46291414 0.4628591  0.46287912 0.46283016 0.46294028
 0.46315497 0.4634137  0.46350822 0.46354574 0.46358302 0.46350008
 0.46331796 0.46315116 0.46301016 0.4629531  0.46283835 0.4627911
 0.46291536 0.4628889  0.46279064 0.46289614 0.46297348 0.4628764
 0.4627708  0.46265256 0.46258178 0.46257102 0.4625136  0.46252513
 0.46265975 0.4628679  0.46299663 0.46294418 0.4627819  0.4627092
 0.46270335 0.46263227 0.46247363 0.4623328  0.46225417 0.46225664
 0.462311   0.46225563 0.46211582 0.46215618 0.46233168 0.46232796
 0.46211755 0.46197978 0.46198112 0.46186137 0.46179274 0.4619715
 0.4621054  0.4621216  0.46214354 0.46213207 0.46200746 0.46194166
 0.46191368 0.46183163 0.46177158 0.46168154 0.46159187 0.46159247
 0.46155566 0.4614746  0.46148235 0.46150503 0.46148893 0.4614855
 0.4615552  0.46163177 0.46167332 0.46172434 0.4617302  0.46172404
 0.46176496 0.46184638 0.46191967 0.46199256 0.46187225 0.46168095
 0.4616934  0.46165973 0.46142977 0.46139762 0.4614494  0.46133345
 0.46125707 0.4613166  0.46135923 0.46139175 0.46146768 0.46151245
 0.4614885  0.46136993 0.46130362 0.46133035 0.4613848  0.46153235
 0.4617036  0.46180582 0.46187773 0.46185932 0.46177694 0.46171373
 0.46158254 0.4614858  0.46146175 0.46138355 0.46134052 0.46136788
 0.46134654 0.4614037  0.46152037 0.46143085 0.4613006  0.46132565
 0.46131292 0.46113682 0.46104017 0.46096686 0.46083122 0.46087492
 0.4609671  0.46094212 0.4609493  0.46105236 0.46104598 0.46099874
 0.46101478 0.4609129  0.46069807 0.46063432 0.4606546  0.4605991
 0.46059725 0.4606829  0.46072382 0.46074253 0.46082395 0.4608213
 0.46069053 0.46048918 0.46040905 0.4603886  0.46029422 0.4602661
 0.460297   0.4603255  0.46032518 0.46034327 0.46024737 0.46005902
 0.45993483 0.4598576  0.4597071  0.45959362 0.45954487 0.45942947
 0.4592469  0.45919582 0.45922592 0.4591799  0.4591197  0.45902246
 0.4588825  0.4587885  0.45871666 0.45861396 0.45859084 0.4586644
 0.45873728 0.4588813  0.4589825  0.45892817 0.4587193  0.4586276
 0.45860144 0.45839548 0.4581668  0.45814177 0.45816836 0.4581412
 0.45807528 0.45799902 0.4580065  0.45801017 0.45801207 0.45808208
 0.4580603  0.4578485  0.4577795  0.45788744 0.45790404 0.45794964
 0.45810553 0.45828238 0.45838985 0.4584299  0.45833144 0.4582087
 0.4582014  0.45816427 0.45804054 0.4579852  0.45800444 0.45792675
 0.4577946  0.45769957 0.45764214 0.45765156 0.45775428 0.457729
 0.45751938 0.4573606  0.45732066 0.45729005 0.457335   0.4574455
 0.4575025  0.45758137 0.45764956 0.45765498 0.45758352 0.45755267
 0.45756668 0.45752117 0.45742625 0.45732203 0.45725825 0.45723006
 0.45721462 0.45729166 0.45735088 0.45723146 0.45712566 0.45714065
 0.45713153 0.45709237 0.45715755 0.45726973 0.45735052 0.45744127
 0.45744765 0.45744875 0.45756552 0.4576796  0.45755568 0.4574
 0.4574258  0.45742998 0.4573032  0.45723662 0.45715648 0.45702216
 0.45701715 0.45705143 0.45709357 0.45714128 0.45713672 0.45715448
 0.4572216  0.45717552 0.45718536 0.45726436 0.45727584 0.4573901
 0.45753723 0.4575678  0.45766342 0.45766306 0.4574773  0.45742348
 0.45742854 0.45731172 0.4572197  0.45719925 0.4571817  0.4571491
 0.45712566 0.45711496 0.4569873  0.4568764  0.456992   0.45700672
 0.45683658 0.4566985  0.45654723 0.45640668 0.45644534 0.45653388
 0.45648846 0.4564764  0.45661274 0.4567442  0.45658225 0.4564057
 0.4564047  0.45626318 0.45611015 0.45603806 0.4559936  0.4560794
 0.45605367 0.4561141  0.4563773  0.45623702 0.45621723 0.45637035
 0.45618647 0.4562451  0.45619124 0.45634192 0.456443   0.4564203 ]
