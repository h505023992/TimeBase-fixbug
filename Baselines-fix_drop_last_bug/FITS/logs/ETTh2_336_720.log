Args in experiment:
Namespace(is_training=1, model_id='ETTh2_336_720', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=100, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_336_720_FITS_ETTh2_ftM_sl336_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=100, out_features=314, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  28134400.0
params:  31714.0
Trainable parameters:  31714
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.246023654937744
Epoch: 1, Steps: 59 | Train Loss: 0.9513234 Vali Loss: 0.8124067 Test Loss: 0.5254532
Validation loss decreased (inf --> 0.812407).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.1812753677368164
Epoch: 2, Steps: 59 | Train Loss: 0.7733454 Vali Loss: 0.7477013 Test Loss: 0.4705045
Validation loss decreased (0.812407 --> 0.747701).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.246934413909912
Epoch: 3, Steps: 59 | Train Loss: 0.6877948 Vali Loss: 0.7185009 Test Loss: 0.4449048
Validation loss decreased (0.747701 --> 0.718501).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.0117883682250977
Epoch: 4, Steps: 59 | Train Loss: 0.6477996 Vali Loss: 0.7010723 Test Loss: 0.4322712
Validation loss decreased (0.718501 --> 0.701072).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.345523834228516
Epoch: 5, Steps: 59 | Train Loss: 0.6242364 Vali Loss: 0.6961014 Test Loss: 0.4255045
Validation loss decreased (0.701072 --> 0.696101).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.563397169113159
Epoch: 6, Steps: 59 | Train Loss: 0.6102644 Vali Loss: 0.6875255 Test Loss: 0.4214075
Validation loss decreased (0.696101 --> 0.687526).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.6723103523254395
Epoch: 7, Steps: 59 | Train Loss: 0.6013533 Vali Loss: 0.6828009 Test Loss: 0.4185399
Validation loss decreased (0.687526 --> 0.682801).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.4664177894592285
Epoch: 8, Steps: 59 | Train Loss: 0.5957103 Vali Loss: 0.6734046 Test Loss: 0.4163222
Validation loss decreased (0.682801 --> 0.673405).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.739231824874878
Epoch: 9, Steps: 59 | Train Loss: 0.5906517 Vali Loss: 0.6745521 Test Loss: 0.4144542
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.7414872646331787
Epoch: 10, Steps: 59 | Train Loss: 0.5861785 Vali Loss: 0.6685758 Test Loss: 0.4128946
Validation loss decreased (0.673405 --> 0.668576).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.7415037155151367
Epoch: 11, Steps: 59 | Train Loss: 0.5820271 Vali Loss: 0.6673597 Test Loss: 0.4115146
Validation loss decreased (0.668576 --> 0.667360).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.730362892150879
Epoch: 12, Steps: 59 | Train Loss: 0.5800088 Vali Loss: 0.6656944 Test Loss: 0.4102098
Validation loss decreased (0.667360 --> 0.665694).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.6813247203826904
Epoch: 13, Steps: 59 | Train Loss: 0.5774151 Vali Loss: 0.6656417 Test Loss: 0.4091086
Validation loss decreased (0.665694 --> 0.665642).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.367426872253418
Epoch: 14, Steps: 59 | Train Loss: 0.5753226 Vali Loss: 0.6647896 Test Loss: 0.4080456
Validation loss decreased (0.665642 --> 0.664790).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.591452121734619
Epoch: 15, Steps: 59 | Train Loss: 0.5742447 Vali Loss: 0.6602380 Test Loss: 0.4071265
Validation loss decreased (0.664790 --> 0.660238).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.4862072467803955
Epoch: 16, Steps: 59 | Train Loss: 0.5712781 Vali Loss: 0.6603019 Test Loss: 0.4062029
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.6511547565460205
Epoch: 17, Steps: 59 | Train Loss: 0.5690597 Vali Loss: 0.6584060 Test Loss: 0.4054751
Validation loss decreased (0.660238 --> 0.658406).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.372530460357666
Epoch: 18, Steps: 59 | Train Loss: 0.5682474 Vali Loss: 0.6593406 Test Loss: 0.4047339
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.161902189254761
Epoch: 19, Steps: 59 | Train Loss: 0.5683213 Vali Loss: 0.6567547 Test Loss: 0.4040358
Validation loss decreased (0.658406 --> 0.656755).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.8276195526123047
Epoch: 20, Steps: 59 | Train Loss: 0.5673187 Vali Loss: 0.6594887 Test Loss: 0.4034379
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.7514052391052246
Epoch: 21, Steps: 59 | Train Loss: 0.5661249 Vali Loss: 0.6563718 Test Loss: 0.4028828
Validation loss decreased (0.656755 --> 0.656372).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.719433069229126
Epoch: 22, Steps: 59 | Train Loss: 0.5655562 Vali Loss: 0.6538121 Test Loss: 0.4023552
Validation loss decreased (0.656372 --> 0.653812).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.6757736206054688
Epoch: 23, Steps: 59 | Train Loss: 0.5635579 Vali Loss: 0.6518368 Test Loss: 0.4018410
Validation loss decreased (0.653812 --> 0.651837).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.303743839263916
Epoch: 24, Steps: 59 | Train Loss: 0.5648374 Vali Loss: 0.6571236 Test Loss: 0.4013776
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.066659212112427
Epoch: 25, Steps: 59 | Train Loss: 0.5643378 Vali Loss: 0.6527380 Test Loss: 0.4009691
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.527488946914673
Epoch: 26, Steps: 59 | Train Loss: 0.5636086 Vali Loss: 0.6539611 Test Loss: 0.4006048
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.486630916595459
Epoch: 27, Steps: 59 | Train Loss: 0.5635978 Vali Loss: 0.6560000 Test Loss: 0.4001845
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.850693702697754
Epoch: 28, Steps: 59 | Train Loss: 0.5624241 Vali Loss: 0.6499860 Test Loss: 0.3998929
Validation loss decreased (0.651837 --> 0.649986).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.4340500831604004
Epoch: 29, Steps: 59 | Train Loss: 0.5621109 Vali Loss: 0.6512358 Test Loss: 0.3995801
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.0808308124542236
Epoch: 30, Steps: 59 | Train Loss: 0.5616896 Vali Loss: 0.6510048 Test Loss: 0.3992660
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00011296777049628277
train 7585
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=100, out_features=314, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  28134400.0
params:  31714.0
Trainable parameters:  31714
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.438612461090088
Epoch: 1, Steps: 59 | Train Loss: 0.8075621 Vali Loss: 0.6503345 Test Loss: 0.3955756
Validation loss decreased (inf --> 0.650334).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.3924450874328613
Epoch: 2, Steps: 59 | Train Loss: 0.8050844 Vali Loss: 0.6459923 Test Loss: 0.3929897
Validation loss decreased (0.650334 --> 0.645992).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.192770481109619
Epoch: 3, Steps: 59 | Train Loss: 0.8017351 Vali Loss: 0.6394761 Test Loss: 0.3913893
Validation loss decreased (0.645992 --> 0.639476).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.202702760696411
Epoch: 4, Steps: 59 | Train Loss: 0.7993878 Vali Loss: 0.6427755 Test Loss: 0.3900990
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.1070234775543213
Epoch: 5, Steps: 59 | Train Loss: 0.7978470 Vali Loss: 0.6383903 Test Loss: 0.3892805
Validation loss decreased (0.639476 --> 0.638390).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.6887614727020264
Epoch: 6, Steps: 59 | Train Loss: 0.7962715 Vali Loss: 0.6383933 Test Loss: 0.3886578
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1145541667938232
Epoch: 7, Steps: 59 | Train Loss: 0.7965574 Vali Loss: 0.6378840 Test Loss: 0.3882518
Validation loss decreased (0.638390 --> 0.637884).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.7999937534332275
Epoch: 8, Steps: 59 | Train Loss: 0.7968802 Vali Loss: 0.6414350 Test Loss: 0.3880145
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.7709126472473145
Epoch: 9, Steps: 59 | Train Loss: 0.7944130 Vali Loss: 0.6338658 Test Loss: 0.3878538
Validation loss decreased (0.637884 --> 0.633866).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.8152077198028564
Epoch: 10, Steps: 59 | Train Loss: 0.7951172 Vali Loss: 0.6355851 Test Loss: 0.3876669
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.798332929611206
Epoch: 11, Steps: 59 | Train Loss: 0.7960750 Vali Loss: 0.6381280 Test Loss: 0.3876529
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.93636417388916
Epoch: 12, Steps: 59 | Train Loss: 0.7952527 Vali Loss: 0.6400537 Test Loss: 0.3875909
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.975282907485962
Epoch: 13, Steps: 59 | Train Loss: 0.7947225 Vali Loss: 0.6383411 Test Loss: 0.3874990
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.842242956161499
Epoch: 14, Steps: 59 | Train Loss: 0.7955435 Vali Loss: 0.6346309 Test Loss: 0.3874123
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh2_336_720_FITS_ETTh2_ftM_sl336_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.38589176535606384, mae:0.4229590892791748, rse:0.4965220093727112, corr:[ 2.20687762e-01  2.21929252e-01  2.20424488e-01  2.20763579e-01
  2.19625816e-01  2.18170315e-01  2.18183577e-01  2.17637300e-01
  2.16230616e-01  2.15273619e-01  2.14406252e-01  2.12906912e-01
  2.11600542e-01  2.10763350e-01  2.10129425e-01  2.09636241e-01
  2.08921805e-01  2.07867011e-01  2.06988007e-01  2.06229135e-01
  2.05066130e-01  2.04267293e-01  2.03706324e-01  2.02240705e-01
  2.00016469e-01  1.98557451e-01  1.97967693e-01  1.97163075e-01
  1.96215689e-01  1.95700273e-01  1.95374295e-01  1.94507286e-01
  1.93136379e-01  1.91747740e-01  1.90819427e-01  1.90084696e-01
  1.89070448e-01  1.88166812e-01  1.87515423e-01  1.86663032e-01
  1.85678899e-01  1.85154021e-01  1.84702128e-01  1.83878779e-01
  1.82959720e-01  1.82466909e-01  1.81770265e-01  1.79927483e-01
  1.77865848e-01  1.76419497e-01  1.75180018e-01  1.73925132e-01
  1.73158124e-01  1.72588691e-01  1.71306834e-01  1.70293257e-01
  1.70131564e-01  1.69896141e-01  1.68714583e-01  1.68079033e-01
  1.68546334e-01  1.68564409e-01  1.68074220e-01  1.67992681e-01
  1.68226033e-01  1.68064609e-01  1.67782918e-01  1.68023586e-01
  1.68107197e-01  1.67580470e-01  1.67205185e-01  1.67080134e-01
  1.66384697e-01  1.65837914e-01  1.66059375e-01  1.66220039e-01
  1.65606216e-01  1.65143386e-01  1.65165216e-01  1.65099964e-01
  1.64824337e-01  1.64660305e-01  1.64555877e-01  1.64448336e-01
  1.64446369e-01  1.64410666e-01  1.64128974e-01  1.63951933e-01
  1.64098710e-01  1.64125234e-01  1.63877711e-01  1.63845301e-01
  1.63988665e-01  1.63771659e-01  1.63664103e-01  1.63739055e-01
  1.63545564e-01  1.63255692e-01  1.63210467e-01  1.63294509e-01
  1.63021460e-01  1.62235364e-01  1.61915258e-01  1.62143111e-01
  1.62574053e-01  1.62231162e-01  1.61634505e-01  1.61564723e-01
  1.61714584e-01  1.61122873e-01  1.60154983e-01  1.60087451e-01
  1.60517991e-01  1.60754025e-01  1.60544008e-01  1.59925222e-01
  1.59036621e-01  1.58327579e-01  1.58273414e-01  1.57947227e-01
  1.56827167e-01  1.55931517e-01  1.55674145e-01  1.55129895e-01
  1.53940067e-01  1.53149262e-01  1.52946770e-01  1.52567968e-01
  1.51888177e-01  1.51357114e-01  1.51575774e-01  1.51722178e-01
  1.50714561e-01  1.49220645e-01  1.48613334e-01  1.48807347e-01
  1.48347333e-01  1.47444382e-01  1.46967143e-01  1.46853551e-01
  1.46949932e-01  1.46985441e-01  1.46810099e-01  1.45710796e-01
  1.44165084e-01  1.43605202e-01  1.43610746e-01  1.43138900e-01
  1.42645702e-01  1.42853722e-01  1.42917305e-01  1.42037034e-01
  1.41387150e-01  1.41591534e-01  1.41746297e-01  1.40769809e-01
  1.39679849e-01  1.39794916e-01  1.40521795e-01  1.40213713e-01
  1.39182702e-01  1.39189929e-01  1.39747143e-01  1.39528707e-01
  1.39126405e-01  1.39677033e-01  1.40277147e-01  1.39301538e-01
  1.37952283e-01  1.37665734e-01  1.37878820e-01  1.37883618e-01
  1.37742206e-01  1.37553081e-01  1.37350932e-01  1.36918649e-01
  1.36072144e-01  1.35033697e-01  1.34791330e-01  1.35281667e-01
  1.34881362e-01  1.33859321e-01  1.33565500e-01  1.33803785e-01
  1.33385316e-01  1.33455217e-01  1.34419352e-01  1.34669572e-01
  1.34274676e-01  1.35071963e-01  1.36427775e-01  1.36589482e-01
  1.36389375e-01  1.36460945e-01  1.36216581e-01  1.36013597e-01
  1.36601210e-01  1.37301341e-01  1.37534469e-01  1.37649253e-01
  1.37701124e-01  1.37570858e-01  1.37670830e-01  1.37573630e-01
  1.37200654e-01  1.37254581e-01  1.37909219e-01  1.38102382e-01
  1.37837157e-01  1.38536140e-01  1.39529154e-01  1.39658555e-01
  1.39957026e-01  1.40701920e-01  1.40197113e-01  1.38688281e-01
  1.38278350e-01  1.38369352e-01  1.37854263e-01  1.37715235e-01
  1.38377786e-01  1.38114855e-01  1.36931285e-01  1.37238443e-01
  1.38661057e-01  1.39446601e-01  1.39690533e-01  1.39742896e-01
  1.39197111e-01  1.38399631e-01  1.38701230e-01  1.39442191e-01
  1.39527783e-01  1.40146241e-01  1.41295776e-01  1.41460165e-01
  1.41455919e-01  1.42977580e-01  1.44365892e-01  1.43668652e-01
  1.42836094e-01  1.43563122e-01  1.44541666e-01  1.44446209e-01
  1.44152910e-01  1.44713745e-01  1.45341158e-01  1.45406872e-01
  1.45481274e-01  1.46013573e-01  1.46609589e-01  1.46498457e-01
  1.46190435e-01  1.46730289e-01  1.47783726e-01  1.48796991e-01
  1.49153709e-01  1.49456680e-01  1.49971455e-01  1.50847092e-01
  1.51702553e-01  1.52134016e-01  1.52657598e-01  1.53405532e-01
  1.53972998e-01  1.54199898e-01  1.54942930e-01  1.55992523e-01
  1.56333908e-01  1.55861989e-01  1.55841008e-01  1.56693518e-01
  1.57414109e-01  1.57697797e-01  1.58143342e-01  1.58563778e-01
  1.58792257e-01  1.59224302e-01  1.59953728e-01  1.60675853e-01
  1.61261320e-01  1.61742225e-01  1.62096098e-01  1.62346095e-01
  1.62516147e-01  1.62513688e-01  1.62862584e-01  1.63390845e-01
  1.63768023e-01  1.63713425e-01  1.63469136e-01  1.63431779e-01
  1.63477212e-01  1.63523823e-01  1.63571715e-01  1.64291531e-01
  1.65450096e-01  1.66279182e-01  1.66461274e-01  1.66018829e-01
  1.65534317e-01  1.65169850e-01  1.65302917e-01  1.66379184e-01
  1.67474270e-01  1.67975441e-01  1.67718917e-01  1.67683929e-01
  1.67431146e-01  1.67306170e-01  1.67958319e-01  1.68497160e-01
  1.68238446e-01  1.67862147e-01  1.68070659e-01  1.68266997e-01
  1.67945608e-01  1.67332321e-01  1.66785464e-01  1.66825265e-01
  1.67390242e-01  1.67415470e-01  1.67010546e-01  1.66706517e-01
  1.66616678e-01  1.66464329e-01  1.66814342e-01  1.67424276e-01
  1.67758122e-01  1.67872399e-01  1.67858154e-01  1.68058828e-01
  1.68400005e-01  1.68779954e-01  1.69129878e-01  1.69089764e-01
  1.69369802e-01  1.70019716e-01  1.70295432e-01  1.70108140e-01
  1.70290828e-01  1.70500547e-01  1.70325503e-01  1.70176446e-01
  1.70561671e-01  1.70925647e-01  1.70809001e-01  1.70570776e-01
  1.70236990e-01  1.70337424e-01  1.71037048e-01  1.71353281e-01
  1.71163052e-01  1.71714306e-01  1.72993883e-01  1.73657954e-01
  1.73320517e-01  1.73837483e-01  1.74992770e-01  1.75301760e-01
  1.74932227e-01  1.74740344e-01  1.74810126e-01  1.74649954e-01
  1.74927115e-01  1.75789014e-01  1.76549703e-01  1.77194327e-01
  1.77463844e-01  1.77560329e-01  1.77428469e-01  1.77439094e-01
  1.77676037e-01  1.77887306e-01  1.77938864e-01  1.77996904e-01
  1.78047866e-01  1.78057939e-01  1.77935198e-01  1.77838877e-01
  1.77774802e-01  1.77913532e-01  1.77822694e-01  1.77499264e-01
  1.77246779e-01  1.77114755e-01  1.77027196e-01  1.76916674e-01
  1.76847592e-01  1.76759198e-01  1.76815599e-01  1.77288547e-01
  1.77826703e-01  1.77784368e-01  1.77617162e-01  1.77745059e-01
  1.77809104e-01  1.77039206e-01  1.76154628e-01  1.75798148e-01
  1.75629750e-01  1.75318643e-01  1.75107837e-01  1.75190404e-01
  1.75083041e-01  1.74581334e-01  1.73935577e-01  1.73437685e-01
  1.73176572e-01  1.72877535e-01  1.72557116e-01  1.72582000e-01
  1.72597364e-01  1.71930462e-01  1.71335176e-01  1.71789065e-01
  1.72133654e-01  1.71240479e-01  1.70296118e-01  1.70579240e-01
  1.70738310e-01  1.69635400e-01  1.68280229e-01  1.67632058e-01
  1.67107746e-01  1.66069984e-01  1.64581254e-01  1.63155898e-01
  1.62524447e-01  1.62678614e-01  1.62377939e-01  1.61248282e-01
  1.60072207e-01  1.59342796e-01  1.58810809e-01  1.58478573e-01
  1.58219591e-01  1.57413036e-01  1.56450331e-01  1.56290129e-01
  1.55990511e-01  1.54799461e-01  1.53871402e-01  1.53575659e-01
  1.52488261e-01  1.50988013e-01  1.50804579e-01  1.51381120e-01
  1.50635600e-01  1.49110183e-01  1.48080856e-01  1.48103088e-01
  1.48735255e-01  1.49391234e-01  1.48914486e-01  1.47687450e-01
  1.47036374e-01  1.46687850e-01  1.45957127e-01  1.45911545e-01
  1.46578059e-01  1.46429524e-01  1.45591840e-01  1.45249054e-01
  1.44742563e-01  1.43363848e-01  1.42640889e-01  1.43124610e-01
  1.43239364e-01  1.42317042e-01  1.41951174e-01  1.42336875e-01
  1.42048106e-01  1.40703380e-01  1.39769644e-01  1.39947683e-01
  1.40570357e-01  1.40661478e-01  1.39719531e-01  1.38173267e-01
  1.36444017e-01  1.35048434e-01  1.34521067e-01  1.34381935e-01
  1.33492976e-01  1.32288754e-01  1.32260591e-01  1.32269681e-01
  1.30877540e-01  1.29557401e-01  1.29759744e-01  1.29567131e-01
  1.28388181e-01  1.28305331e-01  1.28779948e-01  1.27622321e-01
  1.25724226e-01  1.24964274e-01  1.24470867e-01  1.23839922e-01
  1.23870589e-01  1.23451404e-01  1.21583790e-01  1.19997039e-01
  1.19561747e-01  1.18928783e-01  1.17848642e-01  1.17645957e-01
  1.17664658e-01  1.16636053e-01  1.15329079e-01  1.14861064e-01
  1.14736125e-01  1.14594243e-01  1.13868132e-01  1.11899585e-01
  1.10531785e-01  1.10490292e-01  1.09985471e-01  1.08499251e-01
  1.08462058e-01  1.09196626e-01  1.07415400e-01  1.04197584e-01
  1.03053451e-01  1.03740126e-01  1.03242122e-01  1.01332702e-01
  9.90613848e-02  9.72007662e-02  9.62671936e-02  9.57923234e-02
  9.51574817e-02  9.44542959e-02  9.34761539e-02  9.15941894e-02
  9.02029201e-02  9.02780816e-02  8.95829871e-02  8.66989866e-02
  8.46311077e-02  8.50474611e-02  8.49174857e-02  8.28070790e-02
  8.14433917e-02  8.16354603e-02  8.10074434e-02  8.00697505e-02
  8.00570175e-02  8.02835748e-02  7.97818974e-02  7.81813860e-02
  7.57532269e-02  7.41555467e-02  7.40849227e-02  7.39841014e-02
  7.27455765e-02  7.16190785e-02  7.10175335e-02  6.97053447e-02
  6.87746331e-02  6.89105839e-02  6.83864877e-02  6.70377985e-02
  6.69787377e-02  6.78292587e-02  6.69905096e-02  6.47744238e-02
  6.42582551e-02  6.49105757e-02  6.45922497e-02  6.37688190e-02
  6.29629940e-02  6.18205406e-02  6.09270781e-02  5.99793568e-02
  5.79429865e-02  5.58606796e-02  5.52327968e-02  5.49626052e-02
  5.35975732e-02  5.21205999e-02  5.18456101e-02  5.14960214e-02
  4.99948449e-02  4.80347723e-02  4.67561223e-02  4.65640351e-02
  4.62259129e-02  4.51852009e-02  4.44061570e-02  4.48194593e-02
  4.48271222e-02  4.31618541e-02  4.17934582e-02  4.22279723e-02
  4.23884057e-02  4.17323709e-02  4.14569005e-02  4.10245545e-02
  3.88123468e-02  3.63695845e-02  3.56663540e-02  3.57969813e-02
  3.46935838e-02  3.33132371e-02  3.24976593e-02  3.19829769e-02
  3.11581921e-02  3.03125232e-02  2.96829976e-02  2.93511711e-02
  2.87023224e-02  2.74207629e-02  2.66750306e-02  2.74488237e-02
  2.80935355e-02  2.79660486e-02  2.76788492e-02  2.74824332e-02
  2.71449536e-02  2.72799898e-02  2.73236502e-02  2.64883991e-02
  2.48992797e-02  2.40541659e-02  2.42919866e-02  2.48026922e-02
  2.46831197e-02  2.39736512e-02  2.34522019e-02  2.33768784e-02
  2.39037890e-02  2.39446759e-02  2.33728979e-02  2.33504865e-02
  2.34259237e-02  2.32382622e-02  2.34012101e-02  2.37816274e-02
  2.32533049e-02  2.28252262e-02  2.31571719e-02  2.30678711e-02
  2.26099361e-02  2.28274669e-02  2.33829282e-02  2.21801158e-02
  1.99403428e-02  1.84123255e-02  1.81178376e-02  1.82043351e-02
  1.82677936e-02  1.82522684e-02  1.81526449e-02  1.76752843e-02
  1.68737378e-02  1.61495619e-02  1.64782535e-02  1.66535657e-02
  1.58126652e-02  1.51892258e-02  1.57475881e-02  1.59020200e-02
  1.50690144e-02  1.49326595e-02  1.58347823e-02  1.55020738e-02
  1.37087926e-02  1.34499557e-02  1.43620512e-02  1.39395865e-02
  1.21154673e-02  1.12265013e-02  1.14538381e-02  1.12252124e-02
  1.01723857e-02  9.29212570e-03  9.77643859e-03  9.91383661e-03
  8.93959869e-03  8.49707983e-03  9.35589336e-03  9.76037513e-03
  9.20380559e-03  9.06474795e-03  9.27563198e-03  8.94388091e-03
  9.05499514e-03  1.00993719e-02  9.49287228e-03  7.44251534e-03
  7.41959456e-03  9.06318985e-03  7.83976633e-03  4.98583075e-03
  3.96720087e-03  3.68722365e-03  2.45237350e-03  2.03965581e-03
  3.29941325e-03  3.49103916e-03  2.45834352e-03  1.60252163e-03
  1.10265729e-03  7.03296624e-04 -1.44985743e-05 -5.71242766e-04
 -1.17718556e-03 -1.68285915e-03 -2.40292936e-03 -3.35726584e-03
 -3.57619952e-03 -4.56944853e-03 -6.75391080e-03 -7.18245376e-03
 -8.08557495e-03 -1.16229458e-02 -9.07686260e-03 -2.26108544e-03]
