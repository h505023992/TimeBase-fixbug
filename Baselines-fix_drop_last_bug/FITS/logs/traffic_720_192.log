Args in experiment:
Namespace(is_training=1, model_id='traffic_720_192', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=258, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_720_192_FITS_custom_ftM_sl720_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=258, out_features=326, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9280140288.0
params:  84434.0
Trainable parameters:  84434
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 51.04021072387695
Epoch: 1, Steps: 88 | Train Loss: 1.1063381 Vali Loss: 1.1677326 Test Loss: 1.3424429
Validation loss decreased (inf --> 1.167733).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 52.305124044418335
Epoch: 2, Steps: 88 | Train Loss: 0.8319772 Vali Loss: 1.0340986 Test Loss: 1.1880279
Validation loss decreased (1.167733 --> 1.034099).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 51.77005052566528
Epoch: 3, Steps: 88 | Train Loss: 0.7299884 Vali Loss: 0.9700942 Test Loss: 1.1144239
Validation loss decreased (1.034099 --> 0.970094).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 53.2793710231781
Epoch: 4, Steps: 88 | Train Loss: 0.6586301 Vali Loss: 0.9176695 Test Loss: 1.0545579
Validation loss decreased (0.970094 --> 0.917669).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 53.234344720840454
Epoch: 5, Steps: 88 | Train Loss: 0.6006320 Vali Loss: 0.8721861 Test Loss: 1.0034744
Validation loss decreased (0.917669 --> 0.872186).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 52.49765491485596
Epoch: 6, Steps: 88 | Train Loss: 0.5515769 Vali Loss: 0.8314211 Test Loss: 0.9575555
Validation loss decreased (0.872186 --> 0.831421).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 50.8347430229187
Epoch: 7, Steps: 88 | Train Loss: 0.5094092 Vali Loss: 0.7970090 Test Loss: 0.9183255
Validation loss decreased (0.831421 --> 0.797009).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 51.88345146179199
Epoch: 8, Steps: 88 | Train Loss: 0.4727001 Vali Loss: 0.7628711 Test Loss: 0.8790244
Validation loss decreased (0.797009 --> 0.762871).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 52.602949380874634
Epoch: 9, Steps: 88 | Train Loss: 0.4403411 Vali Loss: 0.7337580 Test Loss: 0.8463086
Validation loss decreased (0.762871 --> 0.733758).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 52.366589069366455
Epoch: 10, Steps: 88 | Train Loss: 0.4117421 Vali Loss: 0.7084811 Test Loss: 0.8171986
Validation loss decreased (0.733758 --> 0.708481).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 51.56847405433655
Epoch: 11, Steps: 88 | Train Loss: 0.3863405 Vali Loss: 0.6848667 Test Loss: 0.7903597
Validation loss decreased (0.708481 --> 0.684867).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 48.49430322647095
Epoch: 12, Steps: 88 | Train Loss: 0.3635421 Vali Loss: 0.6623441 Test Loss: 0.7640822
Validation loss decreased (0.684867 --> 0.662344).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 53.47237205505371
Epoch: 13, Steps: 88 | Train Loss: 0.3430632 Vali Loss: 0.6417282 Test Loss: 0.7404490
Validation loss decreased (0.662344 --> 0.641728).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 52.01715111732483
Epoch: 14, Steps: 88 | Train Loss: 0.3247130 Vali Loss: 0.6217179 Test Loss: 0.7177175
Validation loss decreased (0.641728 --> 0.621718).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 51.13030815124512
Epoch: 15, Steps: 88 | Train Loss: 0.3080296 Vali Loss: 0.6055521 Test Loss: 0.6993614
Validation loss decreased (0.621718 --> 0.605552).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 51.38779091835022
Epoch: 16, Steps: 88 | Train Loss: 0.2930062 Vali Loss: 0.5910864 Test Loss: 0.6833584
Validation loss decreased (0.605552 --> 0.591086).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 51.015305042266846
Epoch: 17, Steps: 88 | Train Loss: 0.2792924 Vali Loss: 0.5757324 Test Loss: 0.6658774
Validation loss decreased (0.591086 --> 0.575732).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 52.95505499839783
Epoch: 18, Steps: 88 | Train Loss: 0.2667970 Vali Loss: 0.5620680 Test Loss: 0.6509284
Validation loss decreased (0.575732 --> 0.562068).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 52.717116594314575
Epoch: 19, Steps: 88 | Train Loss: 0.2553935 Vali Loss: 0.5511857 Test Loss: 0.6383435
Validation loss decreased (0.562068 --> 0.551186).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 52.25135922431946
Epoch: 20, Steps: 88 | Train Loss: 0.2449249 Vali Loss: 0.5401299 Test Loss: 0.6250734
Validation loss decreased (0.551186 --> 0.540130).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 51.045912742614746
Epoch: 21, Steps: 88 | Train Loss: 0.2354240 Vali Loss: 0.5292289 Test Loss: 0.6133986
Validation loss decreased (0.540130 --> 0.529229).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 52.48276114463806
Epoch: 22, Steps: 88 | Train Loss: 0.2266929 Vali Loss: 0.5195308 Test Loss: 0.6022004
Validation loss decreased (0.529229 --> 0.519531).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 53.32761216163635
Epoch: 23, Steps: 88 | Train Loss: 0.2185707 Vali Loss: 0.5114281 Test Loss: 0.5933991
Validation loss decreased (0.519531 --> 0.511428).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 52.577513456344604
Epoch: 24, Steps: 88 | Train Loss: 0.2111263 Vali Loss: 0.5030447 Test Loss: 0.5832914
Validation loss decreased (0.511428 --> 0.503045).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 51.24720239639282
Epoch: 25, Steps: 88 | Train Loss: 0.2043017 Vali Loss: 0.4965388 Test Loss: 0.5760962
Validation loss decreased (0.503045 --> 0.496539).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 52.69466733932495
Epoch: 26, Steps: 88 | Train Loss: 0.1979610 Vali Loss: 0.4896554 Test Loss: 0.5679812
Validation loss decreased (0.496539 --> 0.489655).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 52.3067524433136
Epoch: 27, Steps: 88 | Train Loss: 0.1920406 Vali Loss: 0.4828398 Test Loss: 0.5601923
Validation loss decreased (0.489655 --> 0.482840).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 53.479095697402954
Epoch: 28, Steps: 88 | Train Loss: 0.1866135 Vali Loss: 0.4762808 Test Loss: 0.5532851
Validation loss decreased (0.482840 --> 0.476281).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 52.57705545425415
Epoch: 29, Steps: 88 | Train Loss: 0.1815550 Vali Loss: 0.4716123 Test Loss: 0.5475045
Validation loss decreased (0.476281 --> 0.471612).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 50.884172201156616
Epoch: 30, Steps: 88 | Train Loss: 0.1768479 Vali Loss: 0.4652320 Test Loss: 0.5412874
Validation loss decreased (0.471612 --> 0.465232).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=258, out_features=326, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9280140288.0
params:  84434.0
Trainable parameters:  84434
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 52.19579291343689
Epoch: 1, Steps: 88 | Train Loss: 0.2774216 Vali Loss: 0.3369728 Test Loss: 0.4151852
Validation loss decreased (inf --> 0.336973).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 52.73263740539551
Epoch: 2, Steps: 88 | Train Loss: 0.2402108 Vali Loss: 0.3303857 Test Loss: 0.4109757
Validation loss decreased (0.336973 --> 0.330386).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 52.60233449935913
Epoch: 3, Steps: 88 | Train Loss: 0.2389093 Vali Loss: 0.3297226 Test Loss: 0.4108029
Validation loss decreased (0.330386 --> 0.329723).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 52.02128028869629
Epoch: 4, Steps: 88 | Train Loss: 0.2386965 Vali Loss: 0.3292891 Test Loss: 0.4106685
Validation loss decreased (0.329723 --> 0.329289).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 49.58650016784668
Epoch: 5, Steps: 88 | Train Loss: 0.2385190 Vali Loss: 0.3295691 Test Loss: 0.4106886
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 50.935513496398926
Epoch: 6, Steps: 88 | Train Loss: 0.2384713 Vali Loss: 0.3288622 Test Loss: 0.4102431
Validation loss decreased (0.329289 --> 0.328862).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 52.56575679779053
Epoch: 7, Steps: 88 | Train Loss: 0.2384450 Vali Loss: 0.3283758 Test Loss: 0.4101755
Validation loss decreased (0.328862 --> 0.328376).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 50.74066948890686
Epoch: 8, Steps: 88 | Train Loss: 0.2383502 Vali Loss: 0.3285691 Test Loss: 0.4197982
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 50.145947217941284
Epoch: 9, Steps: 88 | Train Loss: 0.2383101 Vali Loss: 0.3289733 Test Loss: 0.4100087
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 51.66224694252014
Epoch: 10, Steps: 88 | Train Loss: 0.2382313 Vali Loss: 0.3287924 Test Loss: 0.4198615
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 51.89870524406433
Epoch: 11, Steps: 88 | Train Loss: 0.2381690 Vali Loss: 0.3288925 Test Loss: 0.4101662
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 49.40512990951538
Epoch: 12, Steps: 88 | Train Loss: 0.2382091 Vali Loss: 0.3288266 Test Loss: 0.4198804
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_720_192_FITS_custom_ftM_sl720_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4192312550544739, mae:0.2858321762084961, rse:0.521484375, corr:[0.27957216 0.2890477  0.2903937  0.28952605 0.29027003 0.2908137
 0.2907476  0.2914828  0.2911195  0.29011175 0.29028565 0.29017338
 0.28965273 0.28985977 0.28985283 0.28942114 0.28951132 0.2899421
 0.29030597 0.29044497 0.29012632 0.28968626 0.2894941  0.28960517
 0.290624   0.29089418 0.29079083 0.29074568 0.29107693 0.29107308
 0.29025826 0.2894101  0.28907093 0.28935847 0.2900768  0.2902394
 0.28970027 0.2896595  0.2901863  0.29033476 0.29008606 0.28986692
 0.28986347 0.28990397 0.28980342 0.28984058 0.28982136 0.28953487
 0.28948444 0.2897136  0.28999907 0.28978428 0.28963277 0.28986114
 0.28983256 0.2893366  0.28890494 0.28872672 0.28882998 0.2892492
 0.28953773 0.2896696  0.29007515 0.29012296 0.28943878 0.28871873
 0.28828993 0.28830937 0.28901938 0.2897669  0.28977057 0.289524
 0.28931385 0.28901148 0.2886187  0.28833407 0.28810015 0.2878173
 0.2877616  0.28793067 0.28801855 0.28793168 0.28767842 0.28725287
 0.2869994  0.28734088 0.28819937 0.288883   0.28902644 0.28894037
 0.2888139  0.28850037 0.28826618 0.28829145 0.2880795  0.2878494
 0.28793755 0.2881924  0.28809375 0.28785723 0.2877503  0.28774282
 0.28788787 0.28768307 0.2869954  0.28684187 0.2873397  0.28724444
 0.28674906 0.28712827 0.2881013  0.28857508 0.28853253 0.2883496
 0.28805014 0.28778172 0.2875981  0.2875201  0.2876597  0.28741023
 0.28685364 0.2871893  0.2878871  0.28809842 0.2882417  0.28829804
 0.28798008 0.28789064 0.28812557 0.28797776 0.28785214 0.28821036
 0.2883987  0.28839076 0.2886708  0.28872752 0.2884239  0.28850764
 0.2886458  0.288199   0.28797236 0.2883208  0.28833196 0.28813046
 0.28850242 0.28890347 0.28907347 0.2890565  0.2886417  0.28815785
 0.28829765 0.28879717 0.28887236 0.28847498 0.28824362 0.28841242
 0.28851575 0.28845713 0.28859878 0.28886306 0.28909656 0.28932858
 0.28917855 0.28859463 0.28837633 0.28862107 0.28854743 0.288572
 0.28965876 0.28981555 0.28980646 0.29010534 0.29030392 0.29021868
 0.29009148 0.28981015 0.28962165 0.29024935 0.29095942 0.29095536
 0.2908004  0.29038346 0.2900626  0.29065126 0.29058    0.28971997
 0.28970706 0.28868026 0.28743526 0.28884113 0.28836745 0.29147366]
