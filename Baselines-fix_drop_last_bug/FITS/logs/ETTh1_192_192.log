Args in experiment:
Namespace(is_training=1, model_id='ETTh1_192_192', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=64, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_192_192_FITS_ETTh1_ftM_sl192_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8257
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=64, out_features=128, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7340032.0
params:  8320.0
Trainable parameters:  8320
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.7229843139648438
Epoch: 1, Steps: 64 | Train Loss: 0.6978038 Vali Loss: 1.6657090 Test Loss: 0.9054146
Validation loss decreased (inf --> 1.665709).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.6487720012664795
Epoch: 2, Steps: 64 | Train Loss: 0.5576051 Vali Loss: 1.4907670 Test Loss: 0.7816353
Validation loss decreased (1.665709 --> 1.490767).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.126467227935791
Epoch: 3, Steps: 64 | Train Loss: 0.4811049 Vali Loss: 1.3959367 Test Loss: 0.7154422
Validation loss decreased (1.490767 --> 1.395937).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.2162094116210938
Epoch: 4, Steps: 64 | Train Loss: 0.4372702 Vali Loss: 1.3404101 Test Loss: 0.6780508
Validation loss decreased (1.395937 --> 1.340410).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.231201648712158
Epoch: 5, Steps: 64 | Train Loss: 0.4089742 Vali Loss: 1.3034784 Test Loss: 0.6535679
Validation loss decreased (1.340410 --> 1.303478).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.796400547027588
Epoch: 6, Steps: 64 | Train Loss: 0.3888381 Vali Loss: 1.2759511 Test Loss: 0.6356016
Validation loss decreased (1.303478 --> 1.275951).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.5402286052703857
Epoch: 7, Steps: 64 | Train Loss: 0.3732858 Vali Loss: 1.2536299 Test Loss: 0.6207473
Validation loss decreased (1.275951 --> 1.253630).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.43001389503479
Epoch: 8, Steps: 64 | Train Loss: 0.3603721 Vali Loss: 1.2348045 Test Loss: 0.6085502
Validation loss decreased (1.253630 --> 1.234805).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.9420154094696045
Epoch: 9, Steps: 64 | Train Loss: 0.3494821 Vali Loss: 1.2187322 Test Loss: 0.5970461
Validation loss decreased (1.234805 --> 1.218732).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2726707458496094
Epoch: 10, Steps: 64 | Train Loss: 0.3396920 Vali Loss: 1.2034019 Test Loss: 0.5865167
Validation loss decreased (1.218732 --> 1.203402).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.746450662612915
Epoch: 11, Steps: 64 | Train Loss: 0.3311568 Vali Loss: 1.1905371 Test Loss: 0.5774648
Validation loss decreased (1.203402 --> 1.190537).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.510985851287842
Epoch: 12, Steps: 64 | Train Loss: 0.3235127 Vali Loss: 1.1790110 Test Loss: 0.5693285
Validation loss decreased (1.190537 --> 1.179011).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.198467254638672
Epoch: 13, Steps: 64 | Train Loss: 0.3166438 Vali Loss: 1.1675948 Test Loss: 0.5614854
Validation loss decreased (1.179011 --> 1.167595).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.157033920288086
Epoch: 14, Steps: 64 | Train Loss: 0.3108211 Vali Loss: 1.1577805 Test Loss: 0.5541668
Validation loss decreased (1.167595 --> 1.157781).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.7133266925811768
Epoch: 15, Steps: 64 | Train Loss: 0.3051076 Vali Loss: 1.1486073 Test Loss: 0.5478339
Validation loss decreased (1.157781 --> 1.148607).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.427527904510498
Epoch: 16, Steps: 64 | Train Loss: 0.3001556 Vali Loss: 1.1401031 Test Loss: 0.5417966
Validation loss decreased (1.148607 --> 1.140103).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.2288715839385986
Epoch: 17, Steps: 64 | Train Loss: 0.2957305 Vali Loss: 1.1327713 Test Loss: 0.5364487
Validation loss decreased (1.140103 --> 1.132771).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.1678547859191895
Epoch: 18, Steps: 64 | Train Loss: 0.2916083 Vali Loss: 1.1247311 Test Loss: 0.5311032
Validation loss decreased (1.132771 --> 1.124731).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.289415121078491
Epoch: 19, Steps: 64 | Train Loss: 0.2879095 Vali Loss: 1.1184576 Test Loss: 0.5266199
Validation loss decreased (1.124731 --> 1.118458).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.6188414096832275
Epoch: 20, Steps: 64 | Train Loss: 0.2846563 Vali Loss: 1.1131495 Test Loss: 0.5223052
Validation loss decreased (1.118458 --> 1.113150).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.7022552490234375
Epoch: 21, Steps: 64 | Train Loss: 0.2815044 Vali Loss: 1.1072793 Test Loss: 0.5185133
Validation loss decreased (1.113150 --> 1.107279).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.3771779537200928
Epoch: 22, Steps: 64 | Train Loss: 0.2783532 Vali Loss: 1.1020725 Test Loss: 0.5146415
Validation loss decreased (1.107279 --> 1.102072).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.035348892211914
Epoch: 23, Steps: 64 | Train Loss: 0.2757984 Vali Loss: 1.0968390 Test Loss: 0.5112684
Validation loss decreased (1.102072 --> 1.096839).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.0521836280822754
Epoch: 24, Steps: 64 | Train Loss: 0.2732820 Vali Loss: 1.0921335 Test Loss: 0.5079171
Validation loss decreased (1.096839 --> 1.092134).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.748441457748413
Epoch: 25, Steps: 64 | Train Loss: 0.2709768 Vali Loss: 1.0884943 Test Loss: 0.5049405
Validation loss decreased (1.092134 --> 1.088494).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.9902091026306152
Epoch: 26, Steps: 64 | Train Loss: 0.2691930 Vali Loss: 1.0846884 Test Loss: 0.5022709
Validation loss decreased (1.088494 --> 1.084688).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.7819414138793945
Epoch: 27, Steps: 64 | Train Loss: 0.2671510 Vali Loss: 1.0806921 Test Loss: 0.4998699
Validation loss decreased (1.084688 --> 1.080692).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.7322981357574463
Epoch: 28, Steps: 64 | Train Loss: 0.2653802 Vali Loss: 1.0774571 Test Loss: 0.4973110
Validation loss decreased (1.080692 --> 1.077457).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.7122652530670166
Epoch: 29, Steps: 64 | Train Loss: 0.2635584 Vali Loss: 1.0737177 Test Loss: 0.4948918
Validation loss decreased (1.077457 --> 1.073718).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.7325828075408936
Epoch: 30, Steps: 64 | Train Loss: 0.2619366 Vali Loss: 1.0708750 Test Loss: 0.4929266
Validation loss decreased (1.073718 --> 1.070875).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 8257
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=64, out_features=128, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7340032.0
params:  8320.0
Trainable parameters:  8320
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.728036642074585
Epoch: 1, Steps: 64 | Train Loss: 0.4429326 Vali Loss: 1.0266680 Test Loss: 0.4594225
Validation loss decreased (inf --> 1.026668).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.6820449829101562
Epoch: 2, Steps: 64 | Train Loss: 0.4255670 Vali Loss: 1.0002933 Test Loss: 0.4412357
Validation loss decreased (1.026668 --> 1.000293).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.7430267333984375
Epoch: 3, Steps: 64 | Train Loss: 0.4155319 Vali Loss: 0.9846053 Test Loss: 0.4321426
Validation loss decreased (1.000293 --> 0.984605).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.7774765491485596
Epoch: 4, Steps: 64 | Train Loss: 0.4097578 Vali Loss: 0.9753636 Test Loss: 0.4280598
Validation loss decreased (0.984605 --> 0.975364).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.762108325958252
Epoch: 5, Steps: 64 | Train Loss: 0.4067960 Vali Loss: 0.9706875 Test Loss: 0.4265451
Validation loss decreased (0.975364 --> 0.970688).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.793301820755005
Epoch: 6, Steps: 64 | Train Loss: 0.4045665 Vali Loss: 0.9678763 Test Loss: 0.4267896
Validation loss decreased (0.970688 --> 0.967876).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.691105842590332
Epoch: 7, Steps: 64 | Train Loss: 0.4037012 Vali Loss: 0.9662349 Test Loss: 0.4272369
Validation loss decreased (0.967876 --> 0.966235).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.606951951980591
Epoch: 8, Steps: 64 | Train Loss: 0.4028974 Vali Loss: 0.9651826 Test Loss: 0.4277945
Validation loss decreased (0.966235 --> 0.965183).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.5669233798980713
Epoch: 9, Steps: 64 | Train Loss: 0.4025267 Vali Loss: 0.9647286 Test Loss: 0.4282779
Validation loss decreased (0.965183 --> 0.964729).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.1596262454986572
Epoch: 10, Steps: 64 | Train Loss: 0.4026263 Vali Loss: 0.9640173 Test Loss: 0.4289157
Validation loss decreased (0.964729 --> 0.964017).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.1934244632720947
Epoch: 11, Steps: 64 | Train Loss: 0.4022451 Vali Loss: 0.9636222 Test Loss: 0.4291834
Validation loss decreased (0.964017 --> 0.963622).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3815765380859375
Epoch: 12, Steps: 64 | Train Loss: 0.4024212 Vali Loss: 0.9637107 Test Loss: 0.4296343
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.5654079914093018
Epoch: 13, Steps: 64 | Train Loss: 0.4021920 Vali Loss: 0.9634048 Test Loss: 0.4297217
Validation loss decreased (0.963622 --> 0.963405).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.5269970893859863
Epoch: 14, Steps: 64 | Train Loss: 0.4021371 Vali Loss: 0.9630752 Test Loss: 0.4297452
Validation loss decreased (0.963405 --> 0.963075).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.5516164302825928
Epoch: 15, Steps: 64 | Train Loss: 0.4020654 Vali Loss: 0.9628657 Test Loss: 0.4298653
Validation loss decreased (0.963075 --> 0.962866).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.0617051124572754
Epoch: 16, Steps: 64 | Train Loss: 0.4021255 Vali Loss: 0.9627768 Test Loss: 0.4300657
Validation loss decreased (0.962866 --> 0.962777).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.064598798751831
Epoch: 17, Steps: 64 | Train Loss: 0.4024483 Vali Loss: 0.9632583 Test Loss: 0.4300846
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.0393545627593994
Epoch: 18, Steps: 64 | Train Loss: 0.4017872 Vali Loss: 0.9627432 Test Loss: 0.4300700
Validation loss decreased (0.962777 --> 0.962743).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.103738784790039
Epoch: 19, Steps: 64 | Train Loss: 0.4016849 Vali Loss: 0.9624559 Test Loss: 0.4300402
Validation loss decreased (0.962743 --> 0.962456).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.386986255645752
Epoch: 20, Steps: 64 | Train Loss: 0.4015822 Vali Loss: 0.9626513 Test Loss: 0.4301592
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.4709389209747314
Epoch: 21, Steps: 64 | Train Loss: 0.4019011 Vali Loss: 0.9625346 Test Loss: 0.4302089
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.8831326961517334
Epoch: 22, Steps: 64 | Train Loss: 0.4019416 Vali Loss: 0.9613127 Test Loss: 0.4302821
Validation loss decreased (0.962456 --> 0.961313).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.0701777935028076
Epoch: 23, Steps: 64 | Train Loss: 0.4016983 Vali Loss: 0.9624155 Test Loss: 0.4302187
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.201603889465332
Epoch: 24, Steps: 64 | Train Loss: 0.4017060 Vali Loss: 0.9624742 Test Loss: 0.4300918
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2520596981048584
Epoch: 25, Steps: 64 | Train Loss: 0.4015946 Vali Loss: 0.9621155 Test Loss: 0.4301567
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.4155144691467285
Epoch: 26, Steps: 64 | Train Loss: 0.4016652 Vali Loss: 0.9621150 Test Loss: 0.4300905
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.427530527114868
Epoch: 27, Steps: 64 | Train Loss: 0.4016570 Vali Loss: 0.9622010 Test Loss: 0.4301592
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_192_192_FITS_ETTh1_ftM_sl192_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.42409372329711914, mae:0.4191186726093292, rse:0.6184267401695251, corr:[0.26425782 0.26731533 0.26745388 0.26923424 0.266452   0.26432893
 0.2638843  0.26304865 0.26270625 0.26306176 0.26254916 0.26210925
 0.2623636  0.26182324 0.26130894 0.26174134 0.26181993 0.26148885
 0.26110634 0.26095194 0.26070908 0.260405   0.26055995 0.26117125
 0.26072407 0.26006943 0.25968537 0.2592328  0.2587909  0.25859365
 0.25809306 0.25744253 0.2571524  0.2567897  0.2566905  0.25693798
 0.25702927 0.25706175 0.2571887  0.25739285 0.25773218 0.25814387
 0.2582266  0.25813612 0.25811434 0.2580302  0.25832275 0.25895995
 0.25866643 0.25744098 0.25611857 0.2550504  0.2541648  0.2529728
 0.25194785 0.2513982  0.2511491  0.25102207 0.25076342 0.2507826
 0.25092894 0.25078523 0.25053498 0.25067464 0.25073773 0.2507258
 0.2511647  0.2513374  0.25093395 0.25082785 0.25111517 0.2509538
 0.24993542 0.248771   0.2479246  0.24731949 0.24688587 0.24646829
 0.24605131 0.24552213 0.24516605 0.24482717 0.24468242 0.24469988
 0.24450898 0.24441256 0.24472459 0.24481373 0.24473977 0.24481189
 0.24489896 0.24492411 0.24480122 0.24477309 0.2449595  0.24522352
 0.2451115  0.24468179 0.24410051 0.24344823 0.24304356 0.24263403
 0.24227087 0.24223545 0.24220012 0.242041   0.24193154 0.24193512
 0.24171716 0.24146831 0.2414058  0.24178268 0.24209833 0.24232784
 0.24255459 0.24270844 0.24262418 0.24261075 0.24267778 0.2427067
 0.2424412  0.24185674 0.24097408 0.23994307 0.23921362 0.23880374
 0.23829311 0.23820724 0.2383102  0.23798709 0.23758794 0.23751071
 0.23752254 0.23736712 0.2374302  0.23742849 0.23755209 0.23778711
 0.23804958 0.23821467 0.23814508 0.2379374  0.23787771 0.23775677
 0.23724853 0.23651825 0.23591268 0.23499966 0.23416601 0.23338789
 0.23292181 0.23275766 0.23272307 0.23261166 0.23269086 0.23293284
 0.23284633 0.23281011 0.23315454 0.23317702 0.23293588 0.23296672
 0.23328154 0.23344675 0.23342332 0.23349226 0.23367985 0.23393217
 0.23381408 0.23305379 0.2324291  0.23230784 0.23180829 0.23120598
 0.23127714 0.23178513 0.2315866  0.23138921 0.23206194 0.23252279
 0.23206115 0.23194689 0.23247188 0.23198986 0.23097225 0.23145516
 0.23170346 0.23042089 0.23117544 0.23184587 0.23067471 0.23452412]
