Args in experiment:
Namespace(is_training=1, model_id='traffic_720_192', model='SparseTSF', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=192, period_len=24, basis_num=6, fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=7, individual=0, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='learned', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.03, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=0, devices='0,1', test_flop=False)
Use GPU: cuda:0
model的参数数量: 265
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
MACs: 20481120.0
Params: 265.0
20.48M MACs
>>>>>>>start training : traffic_720_192_SparseTSF_custom_ftM_sl720_pl192_test_0_seed2023>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Max Memory (MB): 1887.9267578125
Epoch: 1 cost time: 43.112359046936035
Epoch: 1, Steps: 88 | Train Loss: 0.5646381 Vali Loss: 0.4992228 Test Loss: 0.6055498
Validation loss decreased (inf --> 0.499223).  Saving model ...
Updating learning rate to 0.03
Max Memory (MB): 2071.6298828125
Epoch: 2 cost time: 43.27959704399109
Epoch: 2, Steps: 88 | Train Loss: 0.2814082 Vali Loss: 0.3341213 Test Loss: 0.3934215
Validation loss decreased (0.499223 --> 0.334121).  Saving model ...
Updating learning rate to 0.03
Max Memory (MB): 2071.6298828125
Epoch: 3 cost time: 43.04413294792175
Epoch: 3, Steps: 88 | Train Loss: 0.2404730 Vali Loss: 0.3296798 Test Loss: 0.3896457
Validation loss decreased (0.334121 --> 0.329680).  Saving model ...
Updating learning rate to 0.03
Max Memory (MB): 2071.6298828125
Epoch: 4 cost time: 43.08770418167114
Epoch: 4, Steps: 88 | Train Loss: 0.2391225 Vali Loss: 0.3292477 Test Loss: 0.3889062
Validation loss decreased (0.329680 --> 0.329248).  Saving model ...
Updating learning rate to 0.024
Max Memory (MB): 2071.6298828125
Epoch: 5 cost time: 43.50005292892456
Epoch: 5, Steps: 88 | Train Loss: 0.2386894 Vali Loss: 0.3296358 Test Loss: 0.3882830
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.019200000000000002
Max Memory (MB): 2071.6298828125
Epoch: 6 cost time: 42.94705772399902
Epoch: 6, Steps: 88 | Train Loss: 0.2386077 Vali Loss: 0.3316160 Test Loss: 0.3920449
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.015360000000000004
Max Memory (MB): 2071.6298828125
Epoch: 7 cost time: 42.872727155685425
Epoch: 7, Steps: 88 | Train Loss: 0.2386293 Vali Loss: 0.3293109 Test Loss: 0.3885165
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.012288000000000002
Max Memory (MB): 2071.6298828125
Epoch: 8 cost time: 43.397762060165405
Epoch: 8, Steps: 88 | Train Loss: 0.2383936 Vali Loss: 0.3290447 Test Loss: 0.3891208
Validation loss decreased (0.329248 --> 0.329045).  Saving model ...
Updating learning rate to 0.009830400000000001
Max Memory (MB): 2071.6298828125
Epoch: 9 cost time: 43.97712206840515
Epoch: 9, Steps: 88 | Train Loss: 0.2385422 Vali Loss: 0.3294249 Test Loss: 0.3882729
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.007864320000000003
Max Memory (MB): 2071.6298828125
Epoch: 10 cost time: 42.46967911720276
Epoch: 10, Steps: 88 | Train Loss: 0.2382175 Vali Loss: 0.3291226 Test Loss: 0.3878569
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.006291456000000002
Max Memory (MB): 2071.6298828125
Epoch: 11 cost time: 42.99237132072449
Epoch: 11, Steps: 88 | Train Loss: 0.2382843 Vali Loss: 0.3296991 Test Loss: 0.3894532
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.005033164800000003
Max Memory (MB): 2071.6298828125
Epoch: 12 cost time: 42.82528305053711
Epoch: 12, Steps: 88 | Train Loss: 0.2383012 Vali Loss: 0.3288957 Test Loss: 0.3878505
Validation loss decreased (0.329045 --> 0.328896).  Saving model ...
Updating learning rate to 0.004026531840000002
Max Memory (MB): 2071.6298828125
Epoch: 13 cost time: 43.575350522994995
Epoch: 13, Steps: 88 | Train Loss: 0.2383581 Vali Loss: 0.3285872 Test Loss: 0.3879470
Validation loss decreased (0.328896 --> 0.328587).  Saving model ...
Updating learning rate to 0.0032212254720000015
Max Memory (MB): 2071.6298828125
Epoch: 14 cost time: 42.83887457847595
Epoch: 14, Steps: 88 | Train Loss: 0.2382767 Vali Loss: 0.3289796 Test Loss: 0.3882251
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0025769803776000016
Max Memory (MB): 2071.6298828125
Epoch: 15 cost time: 43.60341024398804
Epoch: 15, Steps: 88 | Train Loss: 0.2382864 Vali Loss: 0.3290510 Test Loss: 0.3884113
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.002061584302080001
Max Memory (MB): 2071.6298828125
Epoch: 16 cost time: 43.001055002212524
Epoch: 16, Steps: 88 | Train Loss: 0.2383266 Vali Loss: 0.3290170 Test Loss: 0.3874601
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.001649267441664001
Max Memory (MB): 2071.6298828125
Epoch: 17 cost time: 42.99765872955322
Epoch: 17, Steps: 88 | Train Loss: 0.2383108 Vali Loss: 0.3286099 Test Loss: 0.3883035
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.001319413953331201
Max Memory (MB): 2071.6298828125
Epoch: 18 cost time: 43.10586500167847
Epoch: 18, Steps: 88 | Train Loss: 0.2381810 Vali Loss: 0.3295414 Test Loss: 0.3878878
EarlyStopping counter: 5 out of 5
Early stopping
Final Max Memory (MB): 2071.6298828125
>>>>>>>testing : traffic_720_192_SparseTSF_custom_ftM_sl720_pl192_test_0_seed2023<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.38794851303100586, mae:0.2642371654510498, rse:0.5108056664466858
