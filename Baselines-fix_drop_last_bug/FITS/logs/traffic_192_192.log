Args in experiment:
Namespace(is_training=1, model_id='traffic_192_192', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=82, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_192_192_FITS_custom_ftM_sl192_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11897
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=82, out_features=164, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1483798528.0
params:  13612.0
Trainable parameters:  13612
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 30.684195280075073
Epoch: 1, Steps: 92 | Train Loss: 1.3113544 Vali Loss: 1.3507010 Test Loss: 1.5784507
Validation loss decreased (inf --> 1.350701).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 28.29690718650818
Epoch: 2, Steps: 92 | Train Loss: 0.9303083 Vali Loss: 1.1036510 Test Loss: 1.2883337
Validation loss decreased (1.350701 --> 1.103651).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 32.78156781196594
Epoch: 3, Steps: 92 | Train Loss: 0.7762877 Vali Loss: 0.9985128 Test Loss: 1.1661326
Validation loss decreased (1.103651 --> 0.998513).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 32.014938831329346
Epoch: 4, Steps: 92 | Train Loss: 0.6945591 Vali Loss: 0.9343565 Test Loss: 1.0912334
Validation loss decreased (0.998513 --> 0.934357).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 31.73405122756958
Epoch: 5, Steps: 92 | Train Loss: 0.6360806 Vali Loss: 0.8822219 Test Loss: 1.0318123
Validation loss decreased (0.934357 --> 0.882222).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 32.1386296749115
Epoch: 6, Steps: 92 | Train Loss: 0.5882516 Vali Loss: 0.8377312 Test Loss: 0.9807246
Validation loss decreased (0.882222 --> 0.837731).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 31.2037672996521
Epoch: 7, Steps: 92 | Train Loss: 0.5473059 Vali Loss: 0.7985637 Test Loss: 0.9363245
Validation loss decreased (0.837731 --> 0.798564).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 32.633883476257324
Epoch: 8, Steps: 92 | Train Loss: 0.5116847 Vali Loss: 0.7648160 Test Loss: 0.8967258
Validation loss decreased (0.798564 --> 0.764816).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 32.23985958099365
Epoch: 9, Steps: 92 | Train Loss: 0.4803729 Vali Loss: 0.7330846 Test Loss: 0.8607000
Validation loss decreased (0.764816 --> 0.733085).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 33.59871315956116
Epoch: 10, Steps: 92 | Train Loss: 0.4527718 Vali Loss: 0.7043682 Test Loss: 0.8280166
Validation loss decreased (0.733085 --> 0.704368).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 32.67424941062927
Epoch: 11, Steps: 92 | Train Loss: 0.4282393 Vali Loss: 0.6783584 Test Loss: 0.7984684
Validation loss decreased (0.704368 --> 0.678358).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 32.646809101104736
Epoch: 12, Steps: 92 | Train Loss: 0.4062739 Vali Loss: 0.6555243 Test Loss: 0.7721803
Validation loss decreased (0.678358 --> 0.655524).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 31.742737531661987
Epoch: 13, Steps: 92 | Train Loss: 0.3866732 Vali Loss: 0.6373655 Test Loss: 0.7510849
Validation loss decreased (0.655524 --> 0.637366).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 31.603673458099365
Epoch: 14, Steps: 92 | Train Loss: 0.3691107 Vali Loss: 0.6170778 Test Loss: 0.7277424
Validation loss decreased (0.637366 --> 0.617078).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 32.1175320148468
Epoch: 15, Steps: 92 | Train Loss: 0.3533204 Vali Loss: 0.6000641 Test Loss: 0.7089078
Validation loss decreased (0.617078 --> 0.600064).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 30.859753131866455
Epoch: 16, Steps: 92 | Train Loss: 0.3390303 Vali Loss: 0.5854702 Test Loss: 0.6920395
Validation loss decreased (0.600064 --> 0.585470).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 31.746854543685913
Epoch: 17, Steps: 92 | Train Loss: 0.3261185 Vali Loss: 0.5707906 Test Loss: 0.6757840
Validation loss decreased (0.585470 --> 0.570791).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 31.697871923446655
Epoch: 18, Steps: 92 | Train Loss: 0.3143630 Vali Loss: 0.5584944 Test Loss: 0.6618049
Validation loss decreased (0.570791 --> 0.558494).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 32.37310862541199
Epoch: 19, Steps: 92 | Train Loss: 0.3036928 Vali Loss: 0.5477926 Test Loss: 0.6494368
Validation loss decreased (0.558494 --> 0.547793).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 31.005381107330322
Epoch: 20, Steps: 92 | Train Loss: 0.2940091 Vali Loss: 0.5365525 Test Loss: 0.6367789
Validation loss decreased (0.547793 --> 0.536552).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 31.812721490859985
Epoch: 21, Steps: 92 | Train Loss: 0.2851210 Vali Loss: 0.5273507 Test Loss: 0.6264153
Validation loss decreased (0.536552 --> 0.527351).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 31.975250720977783
Epoch: 22, Steps: 92 | Train Loss: 0.2769454 Vali Loss: 0.5184348 Test Loss: 0.6163615
Validation loss decreased (0.527351 --> 0.518435).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 32.05484414100647
Epoch: 23, Steps: 92 | Train Loss: 0.2695350 Vali Loss: 0.5100322 Test Loss: 0.6069974
Validation loss decreased (0.518435 --> 0.510032).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 32.341564416885376
Epoch: 24, Steps: 92 | Train Loss: 0.2626983 Vali Loss: 0.5028742 Test Loss: 0.5989730
Validation loss decreased (0.510032 --> 0.502874).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 33.02151799201965
Epoch: 25, Steps: 92 | Train Loss: 0.2564096 Vali Loss: 0.4954658 Test Loss: 0.5910762
Validation loss decreased (0.502874 --> 0.495466).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 32.1548388004303
Epoch: 26, Steps: 92 | Train Loss: 0.2505970 Vali Loss: 0.4897482 Test Loss: 0.5840020
Validation loss decreased (0.495466 --> 0.489748).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 33.15593719482422
Epoch: 27, Steps: 92 | Train Loss: 0.2452236 Vali Loss: 0.4838752 Test Loss: 0.5776963
Validation loss decreased (0.489748 --> 0.483875).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 32.8779513835907
Epoch: 28, Steps: 92 | Train Loss: 0.2402877 Vali Loss: 0.4780782 Test Loss: 0.5717269
Validation loss decreased (0.483875 --> 0.478078).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 30.043914318084717
Epoch: 29, Steps: 92 | Train Loss: 0.2357582 Vali Loss: 0.4738055 Test Loss: 0.5666189
Validation loss decreased (0.478078 --> 0.473806).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 30.154556035995483
Epoch: 30, Steps: 92 | Train Loss: 0.2315458 Vali Loss: 0.4688442 Test Loss: 0.5613371
Validation loss decreased (0.473806 --> 0.468844).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11897
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=82, out_features=164, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1483798528.0
params:  13612.0
Trainable parameters:  13612
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 29.228721857070923
Epoch: 1, Steps: 92 | Train Loss: 0.3291734 Vali Loss: 0.4020246 Test Loss: 0.4885873
Validation loss decreased (inf --> 0.402025).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 29.49657964706421
Epoch: 2, Steps: 92 | Train Loss: 0.2920182 Vali Loss: 0.3790383 Test Loss: 0.4660268
Validation loss decreased (0.402025 --> 0.379038).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 30.302130222320557
Epoch: 3, Steps: 92 | Train Loss: 0.2803107 Vali Loss: 0.3719878 Test Loss: 0.4610961
Validation loss decreased (0.379038 --> 0.371988).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 30.187291145324707
Epoch: 4, Steps: 92 | Train Loss: 0.2773662 Vali Loss: 0.3709883 Test Loss: 0.4606166
Validation loss decreased (0.371988 --> 0.370988).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 30.67484450340271
Epoch: 5, Steps: 92 | Train Loss: 0.2766319 Vali Loss: 0.3701239 Test Loss: 0.4606455
Validation loss decreased (0.370988 --> 0.370124).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 30.67693567276001
Epoch: 6, Steps: 92 | Train Loss: 0.2764840 Vali Loss: 0.3702351 Test Loss: 0.4606560
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 30.801934003829956
Epoch: 7, Steps: 92 | Train Loss: 0.2763432 Vali Loss: 0.3702426 Test Loss: 0.4606862
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 30.971477270126343
Epoch: 8, Steps: 92 | Train Loss: 0.2762156 Vali Loss: 0.3705452 Test Loss: 0.4605057
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 31.041398763656616
Epoch: 9, Steps: 92 | Train Loss: 0.2761573 Vali Loss: 0.3699234 Test Loss: 0.4605277
Validation loss decreased (0.370124 --> 0.369923).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 31.31603217124939
Epoch: 10, Steps: 92 | Train Loss: 0.2760095 Vali Loss: 0.3700891 Test Loss: 0.4605662
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 31.167110681533813
Epoch: 11, Steps: 92 | Train Loss: 0.2760326 Vali Loss: 0.3699370 Test Loss: 0.4603611
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 31.165655851364136
Epoch: 12, Steps: 92 | Train Loss: 0.2761251 Vali Loss: 0.3699752 Test Loss: 0.4603574
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 31.68173909187317
Epoch: 13, Steps: 92 | Train Loss: 0.2759360 Vali Loss: 0.3700863 Test Loss: 0.4603090
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 30.72699761390686
Epoch: 14, Steps: 92 | Train Loss: 0.2759264 Vali Loss: 0.3697533 Test Loss: 0.4603448
Validation loss decreased (0.369923 --> 0.369753).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 30.44896960258484
Epoch: 15, Steps: 92 | Train Loss: 0.2760168 Vali Loss: 0.3702883 Test Loss: 0.4602807
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 32.70680499076843
Epoch: 16, Steps: 92 | Train Loss: 0.2758889 Vali Loss: 0.3697648 Test Loss: 0.4602855
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 28.869602918624878
Epoch: 17, Steps: 92 | Train Loss: 0.2758923 Vali Loss: 0.3701295 Test Loss: 0.4602533
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 31.518025159835815
Epoch: 18, Steps: 92 | Train Loss: 0.2759002 Vali Loss: 0.3696421 Test Loss: 0.4603041
Validation loss decreased (0.369753 --> 0.369642).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 31.02863597869873
Epoch: 19, Steps: 92 | Train Loss: 0.2759799 Vali Loss: 0.3702935 Test Loss: 0.4602581
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 31.729721307754517
Epoch: 20, Steps: 92 | Train Loss: 0.2757878 Vali Loss: 0.3698428 Test Loss: 0.4602350
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 30.437357187271118
Epoch: 21, Steps: 92 | Train Loss: 0.2758418 Vali Loss: 0.3698107 Test Loss: 0.4602145
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 31.19817352294922
Epoch: 22, Steps: 92 | Train Loss: 0.2758055 Vali Loss: 0.3695589 Test Loss: 0.4601892
Validation loss decreased (0.369642 --> 0.369559).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 31.142457246780396
Epoch: 23, Steps: 92 | Train Loss: 0.2758445 Vali Loss: 0.3692581 Test Loss: 0.4601706
Validation loss decreased (0.369559 --> 0.369258).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 31.57291555404663
Epoch: 24, Steps: 92 | Train Loss: 0.2757763 Vali Loss: 0.3697207 Test Loss: 0.4601928
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 31.636479377746582
Epoch: 25, Steps: 92 | Train Loss: 0.2758327 Vali Loss: 0.3698066 Test Loss: 0.4601653
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 31.334417581558228
Epoch: 26, Steps: 92 | Train Loss: 0.2757432 Vali Loss: 0.3703785 Test Loss: 0.4601249
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 30.88345170021057
Epoch: 27, Steps: 92 | Train Loss: 0.2758439 Vali Loss: 0.3701045 Test Loss: 0.4601905
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 29.24496054649353
Epoch: 28, Steps: 92 | Train Loss: 0.2758159 Vali Loss: 0.3701193 Test Loss: 0.4601514
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_192_192_FITS_custom_ftM_sl192_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4593034088611603, mae:0.30014291405677795, rse:0.5593438744544983, corr:[0.27280506 0.28801283 0.286501   0.28659108 0.28670642 0.2864875
 0.28771126 0.28695855 0.28736097 0.28704444 0.28688198 0.28713673
 0.28632763 0.28682905 0.28653878 0.28600377 0.286207   0.28572437
 0.28600466 0.2860409  0.28608024 0.28611156 0.2851136  0.28555766
 0.28691643 0.28718662 0.28759646 0.28709945 0.28707102 0.28706574
 0.28672865 0.28683227 0.28621694 0.2861608  0.28637376 0.28604493
 0.2861253  0.2856186  0.28542638 0.28579986 0.2856185  0.28555837
 0.2853063  0.28524724 0.28552058 0.28522354 0.28508392 0.28502905
 0.2856244  0.28626117 0.28620574 0.2860934  0.28609833 0.28589323
 0.2857256  0.28514418 0.2851259  0.28518057 0.2846783  0.2847092
 0.2844348  0.2843475  0.28457102 0.28432265 0.28447357 0.2842341
 0.28422695 0.28465167 0.28426257 0.2844653  0.28456068 0.28423393
 0.2845656  0.2844402  0.28437862 0.2843329  0.28388917 0.28434655
 0.2843182  0.2838493  0.28400946 0.2837107  0.28380162 0.28383455
 0.28327715 0.28348947 0.28358772 0.28366312 0.28380024 0.28363982
 0.28399402 0.2838952  0.2835945  0.283574   0.28332937 0.28348997
 0.28314114 0.28310025 0.2834542  0.2830178  0.28288984 0.28301466
 0.28315303 0.283387   0.28301528 0.28312922 0.28315344 0.28247336
 0.2826746  0.28298485 0.28306034 0.28307903 0.28263348 0.2829552
 0.28339767 0.28350133 0.28341925 0.28275153 0.28313732 0.2837485
 0.28342712 0.28381544 0.28364977 0.28325775 0.28324315 0.2830508
 0.28331885 0.28331137 0.28333157 0.28349033 0.2830577  0.28305754
 0.28301376 0.28322065 0.2837996  0.2834288  0.2835184  0.28385055
 0.28359053 0.2840762  0.283856   0.2835274  0.284477   0.28536397
 0.2857105  0.28555208 0.28551894 0.28584188 0.28558725 0.28599092
 0.28616938 0.28573906 0.2859288  0.2856759  0.28554484 0.28548765
 0.28526068 0.28602543 0.28613153 0.28607744 0.28643247 0.28608677
 0.2863778  0.28643194 0.28615588 0.28622177 0.2855327  0.28635725
 0.28803763 0.28714508 0.2866828  0.2859646  0.28596735 0.2862943
 0.28600615 0.28628314 0.28547135 0.28581202 0.28651658 0.28603262
 0.286734   0.28642386 0.28671715 0.2867986  0.286297   0.2869822
 0.28521794 0.28559643 0.28456512 0.2838313  0.28481692 0.28601018]
