Args in experiment:
Namespace(is_training=1, model_id='traffic_336_192', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=130, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_336_192_FITS_custom_ftM_sl336_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11753
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=130, out_features=204, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2926110720.0
params:  26724.0
Trainable parameters:  26724
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 43.35107731819153
Epoch: 1, Steps: 91 | Train Loss: 1.1910522 Vali Loss: 1.2551298 Test Loss: 1.4602120
Validation loss decreased (inf --> 1.255130).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 40.728753089904785
Epoch: 2, Steps: 91 | Train Loss: 0.8762301 Vali Loss: 1.0942079 Test Loss: 1.2720335
Validation loss decreased (1.255130 --> 1.094208).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 42.486655950546265
Epoch: 3, Steps: 91 | Train Loss: 0.7583927 Vali Loss: 1.0168997 Test Loss: 1.1841290
Validation loss decreased (1.094208 --> 1.016900).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 43.48866558074951
Epoch: 4, Steps: 91 | Train Loss: 0.6860104 Vali Loss: 0.9575604 Test Loss: 1.1157639
Validation loss decreased (1.016900 --> 0.957560).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 42.51608324050903
Epoch: 5, Steps: 91 | Train Loss: 0.6276553 Vali Loss: 0.9019551 Test Loss: 1.0525861
Validation loss decreased (0.957560 --> 0.901955).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 40.84722542762756
Epoch: 6, Steps: 91 | Train Loss: 0.5774075 Vali Loss: 0.8525910 Test Loss: 0.9961169
Validation loss decreased (0.901955 --> 0.852591).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 46.339380502700806
Epoch: 7, Steps: 91 | Train Loss: 0.5337267 Vali Loss: 0.8093501 Test Loss: 0.9461925
Validation loss decreased (0.852591 --> 0.809350).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 46.358309507369995
Epoch: 8, Steps: 91 | Train Loss: 0.4954682 Vali Loss: 0.7703736 Test Loss: 0.9013156
Validation loss decreased (0.809350 --> 0.770374).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 46.31523656845093
Epoch: 9, Steps: 91 | Train Loss: 0.4617208 Vali Loss: 0.7352965 Test Loss: 0.8614215
Validation loss decreased (0.770374 --> 0.735296).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 44.961684226989746
Epoch: 10, Steps: 91 | Train Loss: 0.4319051 Vali Loss: 0.7027099 Test Loss: 0.8242729
Validation loss decreased (0.735296 --> 0.702710).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 44.433157444000244
Epoch: 11, Steps: 91 | Train Loss: 0.4055539 Vali Loss: 0.6774769 Test Loss: 0.7946250
Validation loss decreased (0.702710 --> 0.677477).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 47.73757982254028
Epoch: 12, Steps: 91 | Train Loss: 0.3819209 Vali Loss: 0.6518697 Test Loss: 0.7651380
Validation loss decreased (0.677477 --> 0.651870).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 45.096052408218384
Epoch: 13, Steps: 91 | Train Loss: 0.3608703 Vali Loss: 0.6302242 Test Loss: 0.7402530
Validation loss decreased (0.651870 --> 0.630224).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 45.948071002960205
Epoch: 14, Steps: 91 | Train Loss: 0.3420187 Vali Loss: 0.6105835 Test Loss: 0.7177547
Validation loss decreased (0.630224 --> 0.610583).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 46.345877170562744
Epoch: 15, Steps: 91 | Train Loss: 0.3250549 Vali Loss: 0.5934125 Test Loss: 0.6977780
Validation loss decreased (0.610583 --> 0.593412).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 43.991551876068115
Epoch: 16, Steps: 91 | Train Loss: 0.3097149 Vali Loss: 0.5755338 Test Loss: 0.6785616
Validation loss decreased (0.593412 --> 0.575534).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 45.07865285873413
Epoch: 17, Steps: 91 | Train Loss: 0.2959415 Vali Loss: 0.5607622 Test Loss: 0.6618162
Validation loss decreased (0.575534 --> 0.560762).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 41.886330366134644
Epoch: 18, Steps: 91 | Train Loss: 0.2833844 Vali Loss: 0.5469683 Test Loss: 0.6459538
Validation loss decreased (0.560762 --> 0.546968).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 43.31151247024536
Epoch: 19, Steps: 91 | Train Loss: 0.2719689 Vali Loss: 0.5339592 Test Loss: 0.6310572
Validation loss decreased (0.546968 --> 0.533959).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 41.6505925655365
Epoch: 20, Steps: 91 | Train Loss: 0.2616001 Vali Loss: 0.5228586 Test Loss: 0.6188713
Validation loss decreased (0.533959 --> 0.522859).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 44.94376564025879
Epoch: 21, Steps: 91 | Train Loss: 0.2521157 Vali Loss: 0.5129505 Test Loss: 0.6073382
Validation loss decreased (0.522859 --> 0.512951).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 44.325032234191895
Epoch: 22, Steps: 91 | Train Loss: 0.2434951 Vali Loss: 0.5035753 Test Loss: 0.5962907
Validation loss decreased (0.512951 --> 0.503575).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 43.32602596282959
Epoch: 23, Steps: 91 | Train Loss: 0.2355653 Vali Loss: 0.4943656 Test Loss: 0.5865279
Validation loss decreased (0.503575 --> 0.494366).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 41.962035179138184
Epoch: 24, Steps: 91 | Train Loss: 0.2283082 Vali Loss: 0.4861876 Test Loss: 0.5767211
Validation loss decreased (0.494366 --> 0.486188).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 44.9401171207428
Epoch: 25, Steps: 91 | Train Loss: 0.2215949 Vali Loss: 0.4798094 Test Loss: 0.5695889
Validation loss decreased (0.486188 --> 0.479809).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 44.20373749732971
Epoch: 26, Steps: 91 | Train Loss: 0.2154726 Vali Loss: 0.4721052 Test Loss: 0.5609604
Validation loss decreased (0.479809 --> 0.472105).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 43.430787801742554
Epoch: 27, Steps: 91 | Train Loss: 0.2098269 Vali Loss: 0.4658875 Test Loss: 0.5545462
Validation loss decreased (0.472105 --> 0.465888).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 44.99792146682739
Epoch: 28, Steps: 91 | Train Loss: 0.2046014 Vali Loss: 0.4597340 Test Loss: 0.5479286
Validation loss decreased (0.465888 --> 0.459734).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 45.917163372039795
Epoch: 29, Steps: 91 | Train Loss: 0.1997449 Vali Loss: 0.4545449 Test Loss: 0.5420805
Validation loss decreased (0.459734 --> 0.454545).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 43.34250044822693
Epoch: 30, Steps: 91 | Train Loss: 0.1952528 Vali Loss: 0.4499567 Test Loss: 0.5364479
Validation loss decreased (0.454545 --> 0.449957).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11753
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=130, out_features=204, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2926110720.0
params:  26724.0
Trainable parameters:  26724
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 43.376152992248535
Epoch: 1, Steps: 91 | Train Loss: 0.2973291 Vali Loss: 0.3580585 Test Loss: 0.4381047
Validation loss decreased (inf --> 0.358058).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 42.751604318618774
Epoch: 2, Steps: 91 | Train Loss: 0.2597114 Vali Loss: 0.3435808 Test Loss: 0.4260125
Validation loss decreased (0.358058 --> 0.343581).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 44.17383408546448
Epoch: 3, Steps: 91 | Train Loss: 0.2551403 Vali Loss: 0.3425079 Test Loss: 0.4256267
Validation loss decreased (0.343581 --> 0.342508).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 39.11131811141968
Epoch: 4, Steps: 91 | Train Loss: 0.2547611 Vali Loss: 0.3415109 Test Loss: 0.4256556
Validation loss decreased (0.342508 --> 0.341511).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 40.12626528739929
Epoch: 5, Steps: 91 | Train Loss: 0.2545192 Vali Loss: 0.3420294 Test Loss: 0.4255948
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 43.451826333999634
Epoch: 6, Steps: 91 | Train Loss: 0.2545041 Vali Loss: 0.3417783 Test Loss: 0.4255368
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 44.31907057762146
Epoch: 7, Steps: 91 | Train Loss: 0.2544736 Vali Loss: 0.3412295 Test Loss: 0.4255165
Validation loss decreased (0.341511 --> 0.341229).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 44.205164194107056
Epoch: 8, Steps: 91 | Train Loss: 0.2544384 Vali Loss: 0.3420954 Test Loss: 0.4254507
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 44.041640520095825
Epoch: 9, Steps: 91 | Train Loss: 0.2543886 Vali Loss: 0.3412788 Test Loss: 0.4255075
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 45.13770055770874
Epoch: 10, Steps: 91 | Train Loss: 0.2543766 Vali Loss: 0.3415236 Test Loss: 0.4253220
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 43.704917669296265
Epoch: 11, Steps: 91 | Train Loss: 0.2543783 Vali Loss: 0.3418283 Test Loss: 0.4253049
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 43.69884157180786
Epoch: 12, Steps: 91 | Train Loss: 0.2543530 Vali Loss: 0.3412003 Test Loss: 0.4253845
Validation loss decreased (0.341229 --> 0.341200).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 41.04332160949707
Epoch: 13, Steps: 91 | Train Loss: 0.2543326 Vali Loss: 0.3417611 Test Loss: 0.4254939
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 43.062769174575806
Epoch: 14, Steps: 91 | Train Loss: 0.2541965 Vali Loss: 0.3412757 Test Loss: 0.4250889
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 41.50129294395447
Epoch: 15, Steps: 91 | Train Loss: 0.2543283 Vali Loss: 0.3418919 Test Loss: 0.4253241
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 41.32790493965149
Epoch: 16, Steps: 91 | Train Loss: 0.2541495 Vali Loss: 0.3422015 Test Loss: 0.4252470
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 39.21584749221802
Epoch: 17, Steps: 91 | Train Loss: 0.2541707 Vali Loss: 0.3414509 Test Loss: 0.4250416
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_336_192_FITS_custom_ftM_sl336_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.42438384890556335, mae:0.28556936979293823, rse:0.5376608967781067, corr:[0.27605337 0.28881675 0.2892579  0.28792885 0.2894889  0.28925177
 0.28948864 0.2899067  0.28900355 0.2892083  0.2893083  0.28865266
 0.28883797 0.28851333 0.2880986  0.2883672  0.28814614 0.28826413
 0.28838217 0.28790557 0.28836614 0.2885805  0.28806186 0.28900227
 0.2899188  0.2891007  0.28923577 0.28935862 0.28916597 0.2895633
 0.28949255 0.28915164 0.28934523 0.2890777  0.28875107 0.2888556
 0.28864032 0.2886439  0.28874955 0.2881498  0.2879654  0.28824252
 0.28816128 0.28844556 0.28883296 0.28872254 0.28862494 0.28849664
 0.2884415  0.28848416 0.28818122 0.28800395 0.28837088 0.288278
 0.2879086  0.2877886  0.28761986 0.2876857  0.2877034  0.2873313
 0.28763273 0.28801966 0.28734854 0.28691536 0.28720874 0.28719085
 0.28711277 0.2872989  0.28748298 0.2877182  0.28764206 0.2875253
 0.28767583 0.2874219  0.28704265 0.2871886  0.28735697 0.28727856
 0.2869207  0.28636906 0.2864767  0.28701934 0.28696936 0.286753
 0.28683448 0.28683776 0.2868483  0.28675002 0.2864828  0.28662208
 0.28686416 0.2866717  0.28654927 0.2865412  0.2864087  0.28651604
 0.28649935 0.28639328 0.28658676 0.28658742 0.2864499  0.28642544
 0.2861012  0.28600228 0.28632143 0.28629345 0.28614834 0.2859087
 0.28543976 0.28554687 0.28596726 0.28606522 0.28632498 0.28644085
 0.28617945 0.28651044 0.28708404 0.28683633 0.28617522 0.28572777
 0.2858977  0.28638214 0.28650466 0.28642327 0.2863807  0.28589
 0.2855496  0.28582716 0.28592372 0.28593233 0.28618485 0.28616276
 0.28593704 0.28571966 0.285614   0.2858248  0.28593606 0.28606346
 0.28664097 0.28670672 0.28621796 0.2861732  0.28608528 0.28596976
 0.28629035 0.2865052  0.28702965 0.28742296 0.2869184  0.2865779
 0.28666946 0.28613383 0.28603578 0.28662047 0.28644955 0.2864336
 0.28693792 0.28664786 0.28650278 0.28695077 0.28681237 0.28659937
 0.28687957 0.28718862 0.28721142 0.2868987  0.28709635 0.28865787
 0.28951353 0.2889158  0.28912026 0.28859666 0.28828642 0.28873858
 0.28846434 0.2880005  0.2878853  0.28777027 0.28824303 0.28814933
 0.28765938 0.2881132  0.28790462 0.2879203  0.28888223 0.2880729
 0.28758368 0.2879255  0.28683916 0.28777498 0.2869929  0.28901243]
