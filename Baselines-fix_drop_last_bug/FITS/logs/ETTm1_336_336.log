Args in experiment:
Namespace(is_training=1, model_id='ETTm1_336_336', model='FITS', data='ETTm1', root_path='./dataset/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=100, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_336_336_FITS_ETTm1_ftM_sl336_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33889
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=100, out_features=200, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17920000.0
params:  20200.0
Trainable parameters:  20200
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4433960
	speed: 0.0923s/iter; left time: 722.2258s
	iters: 200, epoch: 1 | loss: 0.3722502
	speed: 0.0773s/iter; left time: 597.0046s
Epoch: 1 cost time: 22.764360189437866
Epoch: 1, Steps: 264 | Train Loss: 0.4406129 Vali Loss: 0.8881570 Test Loss: 0.5716308
Validation loss decreased (inf --> 0.888157).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3024008
	speed: 0.3423s/iter; left time: 2586.4354s
	iters: 200, epoch: 2 | loss: 0.2673446
	speed: 0.0807s/iter; left time: 601.8021s
Epoch: 2 cost time: 21.840462923049927
Epoch: 2, Steps: 264 | Train Loss: 0.2825355 Vali Loss: 0.7803802 Test Loss: 0.4834691
Validation loss decreased (0.888157 --> 0.780380).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2284516
	speed: 0.3687s/iter; left time: 2689.2005s
	iters: 200, epoch: 3 | loss: 0.2257444
	speed: 0.0688s/iter; left time: 495.0763s
Epoch: 3 cost time: 21.252169847488403
Epoch: 3, Steps: 264 | Train Loss: 0.2374296 Vali Loss: 0.7328014 Test Loss: 0.4425999
Validation loss decreased (0.780380 --> 0.732801).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2313096
	speed: 0.3521s/iter; left time: 2474.5617s
	iters: 200, epoch: 4 | loss: 0.1974262
	speed: 0.0880s/iter; left time: 609.4607s
Epoch: 4 cost time: 24.54584503173828
Epoch: 4, Steps: 264 | Train Loss: 0.2158182 Vali Loss: 0.7068732 Test Loss: 0.4187848
Validation loss decreased (0.732801 --> 0.706873).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1767344
	speed: 0.3980s/iter; left time: 2692.6034s
	iters: 200, epoch: 5 | loss: 0.2123537
	speed: 0.0837s/iter; left time: 557.6125s
Epoch: 5 cost time: 22.932724237442017
Epoch: 5, Steps: 264 | Train Loss: 0.2036051 Vali Loss: 0.6907416 Test Loss: 0.4042885
Validation loss decreased (0.706873 --> 0.690742).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1980166
	speed: 0.3593s/iter; left time: 2335.8801s
	iters: 200, epoch: 6 | loss: 0.1934395
	speed: 0.0862s/iter; left time: 551.8034s
Epoch: 6 cost time: 23.907310009002686
Epoch: 6, Steps: 264 | Train Loss: 0.1961424 Vali Loss: 0.6815614 Test Loss: 0.3934431
Validation loss decreased (0.690742 --> 0.681561).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1980129
	speed: 0.3436s/iter; left time: 2142.9203s
	iters: 200, epoch: 7 | loss: 0.1950368
	speed: 0.0679s/iter; left time: 416.7142s
Epoch: 7 cost time: 18.967466831207275
Epoch: 7, Steps: 264 | Train Loss: 0.1913737 Vali Loss: 0.6763725 Test Loss: 0.3874336
Validation loss decreased (0.681561 --> 0.676373).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1883725
	speed: 0.3288s/iter; left time: 1963.6582s
	iters: 200, epoch: 8 | loss: 0.1851575
	speed: 0.0674s/iter; left time: 395.7560s
Epoch: 8 cost time: 18.445352792739868
Epoch: 8, Steps: 264 | Train Loss: 0.1881265 Vali Loss: 0.6713711 Test Loss: 0.3827565
Validation loss decreased (0.676373 --> 0.671371).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1888609
	speed: 0.2740s/iter; left time: 1564.3419s
	iters: 200, epoch: 9 | loss: 0.2032177
	speed: 0.0529s/iter; left time: 296.9692s
Epoch: 9 cost time: 15.565308332443237
Epoch: 9, Steps: 264 | Train Loss: 0.1859838 Vali Loss: 0.6689596 Test Loss: 0.3803563
Validation loss decreased (0.671371 --> 0.668960).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1752925
	speed: 0.3099s/iter; left time: 1687.2433s
	iters: 200, epoch: 10 | loss: 0.1756389
	speed: 0.0660s/iter; left time: 352.5312s
Epoch: 10 cost time: 18.823050498962402
Epoch: 10, Steps: 264 | Train Loss: 0.1845465 Vali Loss: 0.6664506 Test Loss: 0.3781996
Validation loss decreased (0.668960 --> 0.666451).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1978836
	speed: 0.3124s/iter; left time: 1618.3469s
	iters: 200, epoch: 11 | loss: 0.1913971
	speed: 0.0616s/iter; left time: 312.9230s
Epoch: 11 cost time: 17.111567735671997
Epoch: 11, Steps: 264 | Train Loss: 0.1835063 Vali Loss: 0.6652938 Test Loss: 0.3768375
Validation loss decreased (0.666451 --> 0.665294).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1828030
	speed: 0.2941s/iter; left time: 1446.1203s
	iters: 200, epoch: 12 | loss: 0.2037977
	speed: 0.0710s/iter; left time: 342.2286s
Epoch: 12 cost time: 19.965861320495605
Epoch: 12, Steps: 264 | Train Loss: 0.1828969 Vali Loss: 0.6644692 Test Loss: 0.3764046
Validation loss decreased (0.665294 --> 0.664469).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1749082
	speed: 0.3107s/iter; left time: 1445.8278s
	iters: 200, epoch: 13 | loss: 0.1841208
	speed: 0.0651s/iter; left time: 296.3064s
Epoch: 13 cost time: 18.419119358062744
Epoch: 13, Steps: 264 | Train Loss: 0.1824615 Vali Loss: 0.6645720 Test Loss: 0.3757106
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1863407
	speed: 0.2993s/iter; left time: 1313.5942s
	iters: 200, epoch: 14 | loss: 0.1891829
	speed: 0.0574s/iter; left time: 246.3764s
Epoch: 14 cost time: 18.126256704330444
Epoch: 14, Steps: 264 | Train Loss: 0.1822762 Vali Loss: 0.6640030 Test Loss: 0.3754582
Validation loss decreased (0.664469 --> 0.664003).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1847313
	speed: 0.3034s/iter; left time: 1251.5094s
	iters: 200, epoch: 15 | loss: 0.1974740
	speed: 0.0820s/iter; left time: 330.0909s
Epoch: 15 cost time: 21.08952236175537
Epoch: 15, Steps: 264 | Train Loss: 0.1821031 Vali Loss: 0.6641635 Test Loss: 0.3754476
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1921805
	speed: 0.3076s/iter; left time: 1187.7942s
	iters: 200, epoch: 16 | loss: 0.1859872
	speed: 0.0710s/iter; left time: 267.1298s
Epoch: 16 cost time: 19.29523539543152
Epoch: 16, Steps: 264 | Train Loss: 0.1819701 Vali Loss: 0.6638107 Test Loss: 0.3754146
Validation loss decreased (0.664003 --> 0.663811).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1814663
	speed: 0.3089s/iter; left time: 1110.9575s
	iters: 200, epoch: 17 | loss: 0.1821193
	speed: 0.0609s/iter; left time: 213.0483s
Epoch: 17 cost time: 17.794392824172974
Epoch: 17, Steps: 264 | Train Loss: 0.1819634 Vali Loss: 0.6636208 Test Loss: 0.3755960
Validation loss decreased (0.663811 --> 0.663621).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1861258
	speed: 0.3091s/iter; left time: 1030.3042s
	iters: 200, epoch: 18 | loss: 0.1663581
	speed: 0.0731s/iter; left time: 236.4280s
Epoch: 18 cost time: 19.956782341003418
Epoch: 18, Steps: 264 | Train Loss: 0.1818720 Vali Loss: 0.6647601 Test Loss: 0.3754547
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1985992
	speed: 0.3017s/iter; left time: 925.9365s
	iters: 200, epoch: 19 | loss: 0.1829526
	speed: 0.0666s/iter; left time: 197.8727s
Epoch: 19 cost time: 18.873525619506836
Epoch: 19, Steps: 264 | Train Loss: 0.1818727 Vali Loss: 0.6638599 Test Loss: 0.3755221
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1901011
	speed: 0.3090s/iter; left time: 866.7377s
	iters: 200, epoch: 20 | loss: 0.1902215
	speed: 0.0599s/iter; left time: 162.0721s
Epoch: 20 cost time: 17.12462091445923
Epoch: 20, Steps: 264 | Train Loss: 0.1818336 Vali Loss: 0.6641117 Test Loss: 0.3755455
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1671691
	speed: 0.3171s/iter; left time: 805.7684s
	iters: 200, epoch: 21 | loss: 0.1771068
	speed: 0.0745s/iter; left time: 181.7539s
Epoch: 21 cost time: 20.32347345352173
Epoch: 21, Steps: 264 | Train Loss: 0.1818761 Vali Loss: 0.6647913 Test Loss: 0.3751873
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1816963
	speed: 0.2967s/iter; left time: 675.6046s
	iters: 200, epoch: 22 | loss: 0.1747143
	speed: 0.0671s/iter; left time: 146.0011s
Epoch: 22 cost time: 18.243517637252808
Epoch: 22, Steps: 264 | Train Loss: 0.1818016 Vali Loss: 0.6640732 Test Loss: 0.3754442
EarlyStopping counter: 5 out of 5
Early stopping
train 33889
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=100, out_features=200, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17920000.0
params:  20200.0
Trainable parameters:  20200
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3542006
	speed: 0.0682s/iter; left time: 533.7480s
	iters: 200, epoch: 1 | loss: 0.3487830
	speed: 0.0606s/iter; left time: 467.9213s
Epoch: 1 cost time: 16.454701900482178
Epoch: 1, Steps: 264 | Train Loss: 0.3515116 Vali Loss: 0.6568205 Test Loss: 0.3726593
Validation loss decreased (inf --> 0.656820).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3976904
	speed: 0.2905s/iter; left time: 2195.6130s
	iters: 200, epoch: 2 | loss: 0.3545329
	speed: 0.0637s/iter; left time: 474.7092s
Epoch: 2 cost time: 17.098254203796387
Epoch: 2, Steps: 264 | Train Loss: 0.3506650 Vali Loss: 0.6559975 Test Loss: 0.3724949
Validation loss decreased (0.656820 --> 0.655997).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3663326
	speed: 0.2462s/iter; left time: 1795.7536s
	iters: 200, epoch: 3 | loss: 0.3540433
	speed: 0.0606s/iter; left time: 435.5426s
Epoch: 3 cost time: 16.181653022766113
Epoch: 3, Steps: 264 | Train Loss: 0.3503292 Vali Loss: 0.6552586 Test Loss: 0.3725145
Validation loss decreased (0.655997 --> 0.655259).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3405260
	speed: 0.2476s/iter; left time: 1740.4293s
	iters: 200, epoch: 4 | loss: 0.4067780
	speed: 0.0521s/iter; left time: 361.2921s
Epoch: 4 cost time: 15.109835147857666
Epoch: 4, Steps: 264 | Train Loss: 0.3500486 Vali Loss: 0.6539090 Test Loss: 0.3727704
Validation loss decreased (0.655259 --> 0.653909).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3564390
	speed: 0.2598s/iter; left time: 1757.3213s
	iters: 200, epoch: 5 | loss: 0.3498111
	speed: 0.0589s/iter; left time: 392.2538s
Epoch: 5 cost time: 16.216700077056885
Epoch: 5, Steps: 264 | Train Loss: 0.3498729 Vali Loss: 0.6540112 Test Loss: 0.3723256
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3701123
	speed: 0.2436s/iter; left time: 1583.6156s
	iters: 200, epoch: 6 | loss: 0.3853552
	speed: 0.0541s/iter; left time: 346.2345s
Epoch: 6 cost time: 15.820674657821655
Epoch: 6, Steps: 264 | Train Loss: 0.3498258 Vali Loss: 0.6541523 Test Loss: 0.3721045
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3232290
	speed: 0.2746s/iter; left time: 1712.4597s
	iters: 200, epoch: 7 | loss: 0.3890260
	speed: 0.0553s/iter; left time: 339.5165s
Epoch: 7 cost time: 15.33428168296814
Epoch: 7, Steps: 264 | Train Loss: 0.3498192 Vali Loss: 0.6528040 Test Loss: 0.3721772
Validation loss decreased (0.653909 --> 0.652804).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3667006
	speed: 0.2530s/iter; left time: 1510.9265s
	iters: 200, epoch: 8 | loss: 0.3577182
	speed: 0.0613s/iter; left time: 360.1138s
Epoch: 8 cost time: 17.045591115951538
Epoch: 8, Steps: 264 | Train Loss: 0.3497691 Vali Loss: 0.6538803 Test Loss: 0.3722917
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3815207
	speed: 0.2372s/iter; left time: 1354.0013s
	iters: 200, epoch: 9 | loss: 0.3122272
	speed: 0.0496s/iter; left time: 278.4266s
Epoch: 9 cost time: 13.963996171951294
Epoch: 9, Steps: 264 | Train Loss: 0.3496459 Vali Loss: 0.6532102 Test Loss: 0.3719499
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3321407
	speed: 0.2893s/iter; left time: 1575.2986s
	iters: 200, epoch: 10 | loss: 0.3263524
	speed: 0.0698s/iter; left time: 373.2355s
Epoch: 10 cost time: 18.882203817367554
Epoch: 10, Steps: 264 | Train Loss: 0.3497535 Vali Loss: 0.6537088 Test Loss: 0.3718939
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3791665
	speed: 0.3279s/iter; left time: 1699.0152s
	iters: 200, epoch: 11 | loss: 0.2994368
	speed: 0.0633s/iter; left time: 321.6981s
Epoch: 11 cost time: 18.283292531967163
Epoch: 11, Steps: 264 | Train Loss: 0.3497366 Vali Loss: 0.6534540 Test Loss: 0.3720569
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3645100
	speed: 0.3234s/iter; left time: 1590.2562s
	iters: 200, epoch: 12 | loss: 0.3329599
	speed: 0.0690s/iter; left time: 332.5516s
Epoch: 12 cost time: 19.65882921218872
Epoch: 12, Steps: 264 | Train Loss: 0.3495767 Vali Loss: 0.6535347 Test Loss: 0.3721800
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTm1_336_336_FITS_ETTm1_ftM_sl336_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3719773292541504, mae:0.38475778698921204, rse:0.580372154712677, corr:[0.53942734 0.54575783 0.5470613  0.55130696 0.5516311  0.5510814
 0.5520683  0.55193704 0.55138123 0.55193555 0.5516402  0.5508179
 0.55095524 0.5503699  0.54857105 0.5472947  0.5460097  0.54399914
 0.54236275 0.5411953  0.5396783  0.53748065 0.53507817 0.53268284
 0.53080124 0.52981126 0.52896994 0.52734107 0.52659017 0.527031
 0.5273704  0.5274673  0.5282187  0.52950376 0.5302192  0.53073066
 0.5313509  0.5318046  0.5319877  0.5318291  0.5319883  0.53228843
 0.5320719  0.5322921  0.5336199  0.53495073 0.5348803  0.53464776
 0.5354764  0.53603166 0.5359783  0.5362026  0.5363234  0.53531855
 0.5341769  0.5340432  0.5342054  0.5338442  0.5334531  0.53345364
 0.5335818  0.5332496  0.5326145  0.53225434 0.5324883  0.5328669
 0.5328528  0.53270876 0.5327384  0.53258455 0.53250265 0.5328582
 0.5331039  0.5328867  0.5328404  0.53331226 0.5336227  0.5331751
 0.53228056 0.53205633 0.5324896  0.532623   0.5324302  0.5322251
 0.5322073  0.5319528  0.53167415 0.53137153 0.5309817  0.53090477
 0.5313983  0.53191566 0.5317838  0.5313568  0.53132087 0.53127134
 0.5307354  0.5301173  0.5295431  0.5286569  0.52757806 0.52701116
 0.52693063 0.52658683 0.52599955 0.5259267  0.5262323  0.52620506
 0.52552277 0.52499527 0.5252508  0.5253267  0.52463186 0.523715
 0.52324253 0.52294254 0.5222673  0.52155954 0.5212561  0.52125984
 0.52142984 0.5216688  0.5214606  0.5210606  0.5213141  0.5217965
 0.521367   0.5207215  0.52154106 0.5227008  0.52253354 0.5221807
 0.5229555  0.5240669  0.5242086  0.52414435 0.5243695  0.52451974
 0.52489537 0.52584934 0.5266288  0.5269006  0.5270339  0.5273395
 0.52743113 0.5272327  0.5271183  0.5272763  0.5272235  0.5266159
 0.5263023  0.52675045 0.526929   0.52633184 0.52577794 0.52588147
 0.52607656 0.5259444  0.52577114 0.52537286 0.5249474  0.52505624
 0.52556795 0.5256276  0.52565163 0.52612144 0.52638066 0.52625585
 0.5263196  0.5265131  0.52663946 0.5270289  0.5274452  0.52734274
 0.5270118  0.52700835 0.5271936  0.52731055 0.52742255 0.5272968
 0.5267936  0.5266031  0.52705055 0.5275015  0.5275452  0.52766275
 0.5278876  0.52782744 0.5279666  0.52869755 0.529185   0.52868694
 0.52799743 0.52798253 0.52781075 0.52662873 0.5251891  0.5245316
 0.5241646  0.5233303  0.52206475 0.52084035 0.5198342  0.51924366
 0.5188161  0.5181284  0.5168987  0.5156304  0.51493466 0.51480937
 0.5145142  0.5137925  0.51304865 0.51278836 0.5123976  0.51138854
 0.5104693  0.51040035 0.5105898  0.5103779  0.5101177  0.5101711
 0.5103183  0.51057404 0.5111555  0.5118405  0.5123602  0.5127005
 0.513131   0.5134552  0.5135532  0.5135797  0.51406    0.51460576
 0.514739   0.5148569  0.51528084 0.51584405 0.5159778  0.5160673
 0.5165276  0.5168149  0.51656866 0.5160285  0.51551616 0.515389
 0.51560056 0.51561356 0.51538354 0.5153897  0.51560473 0.5156771
 0.5154114  0.51495075 0.51470226 0.51500404 0.5153597  0.515102
 0.5146141  0.5145071  0.5145611  0.51448494 0.5145982  0.51513004
 0.5151801  0.5147753  0.5148993  0.51545113 0.5155658  0.5155141
 0.5158066  0.5157847  0.515456   0.5154088  0.5154641  0.5151504
 0.5150308  0.515601   0.51560426 0.5148803  0.5146247  0.5152605
 0.515703   0.51583225 0.51602787 0.5158701  0.5152889  0.5150577
 0.5150007  0.51416546 0.5128668  0.51215816 0.51184434 0.51105857
 0.51010066 0.50943345 0.50897026 0.5083251  0.5076236  0.50723916
 0.50672257 0.5060025  0.5052848  0.5047585  0.50417244 0.50352925
 0.5032024  0.5030151  0.50279623 0.50252056 0.5026295  0.5029415
 0.5027557  0.50232    0.50220054 0.50265867 0.5028743  0.50255907
 0.5020983  0.50189    0.50206643 0.5023693  0.5025287  0.5022913
 0.5021413  0.5024332  0.5023038  0.5017762  0.50209177 0.50287163
 0.5026339  0.5025683  0.5033504  0.5020999  0.5011208  0.5040258 ]
