Args in experiment:
Namespace(is_training=1, model_id='ETTh1_720_336', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=196, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.8886263370513916
Epoch: 1, Steps: 59 | Train Loss: 0.6931720 Vali Loss: 1.7282454 Test Loss: 0.8207445
Validation loss decreased (inf --> 1.728245).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.393278121948242
Epoch: 2, Steps: 59 | Train Loss: 0.5389823 Vali Loss: 1.5712903 Test Loss: 0.7414850
Validation loss decreased (1.728245 --> 1.571290).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.94185733795166
Epoch: 3, Steps: 59 | Train Loss: 0.4707123 Vali Loss: 1.5037394 Test Loss: 0.7125761
Validation loss decreased (1.571290 --> 1.503739).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.987916946411133
Epoch: 4, Steps: 59 | Train Loss: 0.4317483 Vali Loss: 1.4762057 Test Loss: 0.6980525
Validation loss decreased (1.503739 --> 1.476206).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.99552845954895
Epoch: 5, Steps: 59 | Train Loss: 0.4040366 Vali Loss: 1.4482275 Test Loss: 0.6863087
Validation loss decreased (1.476206 --> 1.448228).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.836655616760254
Epoch: 6, Steps: 59 | Train Loss: 0.3819388 Vali Loss: 1.4363827 Test Loss: 0.6756136
Validation loss decreased (1.448228 --> 1.436383).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.069646835327148
Epoch: 7, Steps: 59 | Train Loss: 0.3634276 Vali Loss: 1.4188747 Test Loss: 0.6653219
Validation loss decreased (1.436383 --> 1.418875).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.047807455062866
Epoch: 8, Steps: 59 | Train Loss: 0.3476307 Vali Loss: 1.4023207 Test Loss: 0.6568344
Validation loss decreased (1.418875 --> 1.402321).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.062508583068848
Epoch: 9, Steps: 59 | Train Loss: 0.3335513 Vali Loss: 1.3920183 Test Loss: 0.6490765
Validation loss decreased (1.402321 --> 1.392018).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.045609474182129
Epoch: 10, Steps: 59 | Train Loss: 0.3213297 Vali Loss: 1.3818337 Test Loss: 0.6401805
Validation loss decreased (1.392018 --> 1.381834).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.006856441497803
Epoch: 11, Steps: 59 | Train Loss: 0.3105959 Vali Loss: 1.3735975 Test Loss: 0.6328604
Validation loss decreased (1.381834 --> 1.373598).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.098193168640137
Epoch: 12, Steps: 59 | Train Loss: 0.3009400 Vali Loss: 1.3644737 Test Loss: 0.6248861
Validation loss decreased (1.373598 --> 1.364474).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.158594131469727
Epoch: 13, Steps: 59 | Train Loss: 0.2923271 Vali Loss: 1.3555633 Test Loss: 0.6184195
Validation loss decreased (1.364474 --> 1.355563).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.103102684020996
Epoch: 14, Steps: 59 | Train Loss: 0.2844124 Vali Loss: 1.3521777 Test Loss: 0.6119329
Validation loss decreased (1.355563 --> 1.352178).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.086894273757935
Epoch: 15, Steps: 59 | Train Loss: 0.2775235 Vali Loss: 1.3476466 Test Loss: 0.6066176
Validation loss decreased (1.352178 --> 1.347647).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.053005933761597
Epoch: 16, Steps: 59 | Train Loss: 0.2712738 Vali Loss: 1.3368291 Test Loss: 0.6002544
Validation loss decreased (1.347647 --> 1.336829).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.9334824085235596
Epoch: 17, Steps: 59 | Train Loss: 0.2654302 Vali Loss: 1.3319045 Test Loss: 0.5954391
Validation loss decreased (1.336829 --> 1.331905).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.10719108581543
Epoch: 18, Steps: 59 | Train Loss: 0.2602778 Vali Loss: 1.3275083 Test Loss: 0.5896854
Validation loss decreased (1.331905 --> 1.327508).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.9000160694122314
Epoch: 19, Steps: 59 | Train Loss: 0.2554240 Vali Loss: 1.3191373 Test Loss: 0.5852984
Validation loss decreased (1.327508 --> 1.319137).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.2315328121185303
Epoch: 20, Steps: 59 | Train Loss: 0.2511125 Vali Loss: 1.3177816 Test Loss: 0.5812056
Validation loss decreased (1.319137 --> 1.317782).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.999171733856201
Epoch: 21, Steps: 59 | Train Loss: 0.2471156 Vali Loss: 1.3113928 Test Loss: 0.5771698
Validation loss decreased (1.317782 --> 1.311393).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.188877582550049
Epoch: 22, Steps: 59 | Train Loss: 0.2433000 Vali Loss: 1.3150923 Test Loss: 0.5736947
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.9846038818359375
Epoch: 23, Steps: 59 | Train Loss: 0.2400619 Vali Loss: 1.3044666 Test Loss: 0.5697837
Validation loss decreased (1.311393 --> 1.304467).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.9700124263763428
Epoch: 24, Steps: 59 | Train Loss: 0.2369718 Vali Loss: 1.3056719 Test Loss: 0.5662737
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.7127695083618164
Epoch: 25, Steps: 59 | Train Loss: 0.2339423 Vali Loss: 1.3009462 Test Loss: 0.5633572
Validation loss decreased (1.304467 --> 1.300946).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.8979668617248535
Epoch: 26, Steps: 59 | Train Loss: 0.2314371 Vali Loss: 1.2991552 Test Loss: 0.5607816
Validation loss decreased (1.300946 --> 1.299155).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.91180682182312
Epoch: 27, Steps: 59 | Train Loss: 0.2289211 Vali Loss: 1.2944132 Test Loss: 0.5574349
Validation loss decreased (1.299155 --> 1.294413).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.9072580337524414
Epoch: 28, Steps: 59 | Train Loss: 0.2266483 Vali Loss: 1.2909753 Test Loss: 0.5553160
Validation loss decreased (1.294413 --> 1.290975).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.840282678604126
Epoch: 29, Steps: 59 | Train Loss: 0.2245261 Vali Loss: 1.2946560 Test Loss: 0.5531241
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.763627290725708
Epoch: 30, Steps: 59 | Train Loss: 0.2226562 Vali Loss: 1.2927244 Test Loss: 0.5504590
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00011296777049628277
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.8458943367004395
Epoch: 1, Steps: 59 | Train Loss: 0.4828314 Vali Loss: 1.2271870 Test Loss: 0.4851413
Validation loss decreased (inf --> 1.227187).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.7918148040771484
Epoch: 2, Steps: 59 | Train Loss: 0.4546478 Vali Loss: 1.2028064 Test Loss: 0.4559044
Validation loss decreased (1.227187 --> 1.202806).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.4594388008117676
Epoch: 3, Steps: 59 | Train Loss: 0.4426630 Vali Loss: 1.1987447 Test Loss: 0.4446666
Validation loss decreased (1.202806 --> 1.198745).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.3157081604003906
Epoch: 4, Steps: 59 | Train Loss: 0.4373802 Vali Loss: 1.1945385 Test Loss: 0.4407424
Validation loss decreased (1.198745 --> 1.194538).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.414069652557373
Epoch: 5, Steps: 59 | Train Loss: 0.4354290 Vali Loss: 1.1927582 Test Loss: 0.4399929
Validation loss decreased (1.194538 --> 1.192758).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.5051722526550293
Epoch: 6, Steps: 59 | Train Loss: 0.4344298 Vali Loss: 1.2003772 Test Loss: 0.4401625
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.5122175216674805
Epoch: 7, Steps: 59 | Train Loss: 0.4336160 Vali Loss: 1.2060825 Test Loss: 0.4406389
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.569730043411255
Epoch: 8, Steps: 59 | Train Loss: 0.4334588 Vali Loss: 1.2055950 Test Loss: 0.4409544
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.4891412258148193
Epoch: 9, Steps: 59 | Train Loss: 0.4331068 Vali Loss: 1.2110862 Test Loss: 0.4413763
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.520639181137085
Epoch: 10, Steps: 59 | Train Loss: 0.4325066 Vali Loss: 1.2113507 Test Loss: 0.4414691
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43890225887298584, mae:0.4391862154006958, rse:0.630719006061554, corr:[0.25434634 0.26137897 0.25963297 0.25972632 0.25848714 0.2553325
 0.25372985 0.2542786  0.25418773 0.25311816 0.25264785 0.25292626
 0.2526888  0.25174943 0.25124875 0.25162503 0.2517909  0.25143433
 0.2508758  0.25052005 0.25041547 0.2503034  0.25014296 0.25021902
 0.2503732  0.25009313 0.24971063 0.24967352 0.24985477 0.24974103
 0.24917245 0.24848996 0.24819438 0.2481281  0.24790756 0.24772626
 0.24777095 0.24789241 0.24798141 0.24778417 0.24749193 0.24753155
 0.24803005 0.24844746 0.24845596 0.2486838  0.24959798 0.2503877
 0.25025636 0.24949789 0.24844436 0.24734865 0.24608669 0.2447865
 0.2440164  0.24364085 0.24327819 0.24307786 0.24284023 0.24255337
 0.24212064 0.24207471 0.24235079 0.24253018 0.24269989 0.24308147
 0.24343021 0.24324414 0.24298829 0.24296628 0.2428465  0.2425377
 0.24205618 0.24156982 0.24112888 0.24096204 0.24080446 0.24054936
 0.24024555 0.24006602 0.23989412 0.23952076 0.23888834 0.23838994
 0.2381845  0.23808    0.2380404  0.23793705 0.23751919 0.23682226
 0.23616597 0.2359755  0.23616049 0.2365581  0.23706636 0.23778844
 0.23854116 0.23906681 0.23937361 0.23958325 0.23954266 0.23919503
 0.2387808  0.23848523 0.23859298 0.23909481 0.2392686  0.23867977
 0.23807128 0.23803537 0.23843926 0.23869753 0.23886122 0.23915121
 0.23921663 0.23873143 0.2382916  0.23845018 0.23873751 0.23867023
 0.23831852 0.23779018 0.23728694 0.23678453 0.23603061 0.23519067
 0.23489363 0.23488887 0.23426759 0.23362575 0.23370889 0.23428649
 0.2345105  0.2342712  0.23419581 0.23443072 0.23466855 0.23485401
 0.23511356 0.23506255 0.23474886 0.23469423 0.23473372 0.23440821
 0.23378605 0.23332706 0.23299085 0.23180433 0.23017342 0.22951666
 0.23011038 0.23039846 0.22953252 0.22874278 0.22895183 0.2296916
 0.23007324 0.2298257  0.22921464 0.22863667 0.22846933 0.22895831
 0.22950548 0.22972757 0.22965717 0.22944921 0.22911333 0.22892849
 0.22907795 0.22911935 0.22892123 0.22896382 0.22911763 0.22873572
 0.22774324 0.22692926 0.22670126 0.22705938 0.22744432 0.22745548
 0.22701974 0.22662538 0.22663574 0.22666164 0.22642641 0.22638854
 0.22703584 0.22816542 0.22911474 0.22926696 0.22852935 0.22763795
 0.22735171 0.22751018 0.22709957 0.22572535 0.22445026 0.22424726
 0.2245436  0.22437593 0.22366868 0.22319809 0.22299238 0.22288966
 0.22261457 0.22263047 0.22293139 0.22320001 0.22315231 0.22256902
 0.22219682 0.22251582 0.22314876 0.22328608 0.22287934 0.22235198
 0.22187188 0.22183938 0.22242309 0.22264108 0.22157389 0.22021209
 0.22045837 0.22134823 0.2206486  0.21907175 0.21937133 0.2209143
 0.22073017 0.21900466 0.21839216 0.21899816 0.21910287 0.21898645
 0.2193101  0.21944416 0.21922877 0.21978895 0.22115695 0.22138432
 0.22034802 0.21964288 0.22015497 0.22034195 0.2195167  0.21886139
 0.21891923 0.21885401 0.21825357 0.21770029 0.2176283  0.21788736
 0.21794003 0.21734422 0.21682242 0.21705902 0.21768142 0.21776186
 0.21770337 0.21815388 0.21888882 0.21891096 0.2186067  0.21832769
 0.21776739 0.21699142 0.21688934 0.21750642 0.2173129  0.21617635
 0.21542287 0.21517807 0.21466324 0.21400289 0.21332961 0.21280515
 0.212654   0.21316572 0.21382824 0.21359952 0.21279962 0.21272108
 0.21339786 0.21367757 0.21358521 0.2132177  0.21271068 0.21272674
 0.21376708 0.21471189 0.21393356 0.21260339 0.21219212 0.21233828
 0.21180445 0.21140878 0.21153082 0.21102238 0.21002367 0.21072355
 0.21263637 0.21295932 0.21212071 0.21161853 0.21183154 0.21161453
 0.21144398 0.21110754 0.21056451 0.21036212 0.21177907 0.21295168
 0.21145429 0.20883963 0.20833117 0.20891824 0.20834486 0.20732181
 0.20744173 0.20684154 0.2054263  0.20560457 0.20638742 0.20547152
 0.20378023 0.20271254 0.20192483 0.20251605 0.2039052  0.20334604
 0.20078826 0.20118837 0.20260191 0.19706376 0.19551593 0.20860429]
