Args in experiment:
Namespace(is_training=1, model_id='ETTh1_192_720', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=64, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_192_720_FITS_ETTh1_ftM_sl192_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=64, out_features=304, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17432576.0
params:  19760.0
Trainable parameters:  19760
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.4713006019592285
Epoch: 1, Steps: 60 | Train Loss: 1.1152917 Vali Loss: 2.4468679 Test Loss: 1.1524210
Validation loss decreased (inf --> 2.446868).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.4320216178894043
Epoch: 2, Steps: 60 | Train Loss: 0.8423985 Vali Loss: 2.0918026 Test Loss: 0.8839622
Validation loss decreased (2.446868 --> 2.091803).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.4488582611083984
Epoch: 3, Steps: 60 | Train Loss: 0.7051430 Vali Loss: 1.9175704 Test Loss: 0.7512851
Validation loss decreased (2.091803 --> 1.917570).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.511514186859131
Epoch: 4, Steps: 60 | Train Loss: 0.6365779 Vali Loss: 1.8259376 Test Loss: 0.6830518
Validation loss decreased (1.917570 --> 1.825938).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.180668115615845
Epoch: 5, Steps: 60 | Train Loss: 0.6003631 Vali Loss: 1.7810251 Test Loss: 0.6457512
Validation loss decreased (1.825938 --> 1.781025).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2154016494750977
Epoch: 6, Steps: 60 | Train Loss: 0.5797810 Vali Loss: 1.7484889 Test Loss: 0.6233926
Validation loss decreased (1.781025 --> 1.748489).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.33471941947937
Epoch: 7, Steps: 60 | Train Loss: 0.5668634 Vali Loss: 1.7300751 Test Loss: 0.6076686
Validation loss decreased (1.748489 --> 1.730075).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.528960943222046
Epoch: 8, Steps: 60 | Train Loss: 0.5580078 Vali Loss: 1.7121356 Test Loss: 0.5957778
Validation loss decreased (1.730075 --> 1.712136).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.4897170066833496
Epoch: 9, Steps: 60 | Train Loss: 0.5511422 Vali Loss: 1.7025998 Test Loss: 0.5853994
Validation loss decreased (1.712136 --> 1.702600).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.0463664531707764
Epoch: 10, Steps: 60 | Train Loss: 0.5449256 Vali Loss: 1.6831782 Test Loss: 0.5764384
Validation loss decreased (1.702600 --> 1.683178).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.2817986011505127
Epoch: 11, Steps: 60 | Train Loss: 0.5403129 Vali Loss: 1.6788447 Test Loss: 0.5684329
Validation loss decreased (1.683178 --> 1.678845).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.4183318614959717
Epoch: 12, Steps: 60 | Train Loss: 0.5358640 Vali Loss: 1.6654587 Test Loss: 0.5614502
Validation loss decreased (1.678845 --> 1.665459).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.348132371902466
Epoch: 13, Steps: 60 | Train Loss: 0.5317434 Vali Loss: 1.6561546 Test Loss: 0.5550818
Validation loss decreased (1.665459 --> 1.656155).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.838301658630371
Epoch: 14, Steps: 60 | Train Loss: 0.5283775 Vali Loss: 1.6445255 Test Loss: 0.5489560
Validation loss decreased (1.656155 --> 1.644526).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.6297225952148438
Epoch: 15, Steps: 60 | Train Loss: 0.5246923 Vali Loss: 1.6318147 Test Loss: 0.5434260
Validation loss decreased (1.644526 --> 1.631815).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.130739688873291
Epoch: 16, Steps: 60 | Train Loss: 0.5224114 Vali Loss: 1.6312858 Test Loss: 0.5382119
Validation loss decreased (1.631815 --> 1.631286).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.2631328105926514
Epoch: 17, Steps: 60 | Train Loss: 0.5193570 Vali Loss: 1.6289337 Test Loss: 0.5337439
Validation loss decreased (1.631286 --> 1.628934).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.068939447402954
Epoch: 18, Steps: 60 | Train Loss: 0.5173080 Vali Loss: 1.6163391 Test Loss: 0.5293624
Validation loss decreased (1.628934 --> 1.616339).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.364851474761963
Epoch: 19, Steps: 60 | Train Loss: 0.5147876 Vali Loss: 1.6218522 Test Loss: 0.5253606
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.9415745735168457
Epoch: 20, Steps: 60 | Train Loss: 0.5125991 Vali Loss: 1.6132774 Test Loss: 0.5217946
Validation loss decreased (1.616339 --> 1.613277).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.619410514831543
Epoch: 21, Steps: 60 | Train Loss: 0.5106936 Vali Loss: 1.6060667 Test Loss: 0.5182768
Validation loss decreased (1.613277 --> 1.606067).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.7143850326538086
Epoch: 22, Steps: 60 | Train Loss: 0.5090684 Vali Loss: 1.6011517 Test Loss: 0.5151455
Validation loss decreased (1.606067 --> 1.601152).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.797969341278076
Epoch: 23, Steps: 60 | Train Loss: 0.5075091 Vali Loss: 1.6020339 Test Loss: 0.5123407
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.954423666000366
Epoch: 24, Steps: 60 | Train Loss: 0.5054237 Vali Loss: 1.5909193 Test Loss: 0.5095257
Validation loss decreased (1.601152 --> 1.590919).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.3471157550811768
Epoch: 25, Steps: 60 | Train Loss: 0.5043849 Vali Loss: 1.5864666 Test Loss: 0.5069998
Validation loss decreased (1.590919 --> 1.586467).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.952561616897583
Epoch: 26, Steps: 60 | Train Loss: 0.5028180 Vali Loss: 1.5847098 Test Loss: 0.5046703
Validation loss decreased (1.586467 --> 1.584710).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.9557600021362305
Epoch: 27, Steps: 60 | Train Loss: 0.5019784 Vali Loss: 1.5787165 Test Loss: 0.5024006
Validation loss decreased (1.584710 --> 1.578717).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.404017686843872
Epoch: 28, Steps: 60 | Train Loss: 0.5004823 Vali Loss: 1.5834172 Test Loss: 0.5004458
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.2237439155578613
Epoch: 29, Steps: 60 | Train Loss: 0.4996377 Vali Loss: 1.5786784 Test Loss: 0.4985110
Validation loss decreased (1.578717 --> 1.578678).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.3759889602661133
Epoch: 30, Steps: 60 | Train Loss: 0.4987913 Vali Loss: 1.5807116 Test Loss: 0.4967218
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011296777049628277
train 7729
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=64, out_features=304, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17432576.0
params:  19760.0
Trainable parameters:  19760
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.4953925609588623
Epoch: 1, Steps: 60 | Train Loss: 0.6116150 Vali Loss: 1.5433457 Test Loss: 0.4732553
Validation loss decreased (inf --> 1.543346).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.3737831115722656
Epoch: 2, Steps: 60 | Train Loss: 0.6005819 Vali Loss: 1.5239229 Test Loss: 0.4579825
Validation loss decreased (1.543346 --> 1.523923).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.160381317138672
Epoch: 3, Steps: 60 | Train Loss: 0.5938287 Vali Loss: 1.5072713 Test Loss: 0.4487908
Validation loss decreased (1.523923 --> 1.507271).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.1474242210388184
Epoch: 4, Steps: 60 | Train Loss: 0.5888725 Vali Loss: 1.5015056 Test Loss: 0.4432929
Validation loss decreased (1.507271 --> 1.501506).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.3040990829467773
Epoch: 5, Steps: 60 | Train Loss: 0.5865382 Vali Loss: 1.4983394 Test Loss: 0.4401513
Validation loss decreased (1.501506 --> 1.498339).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.3006601333618164
Epoch: 6, Steps: 60 | Train Loss: 0.5844686 Vali Loss: 1.4863510 Test Loss: 0.4384805
Validation loss decreased (1.498339 --> 1.486351).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.270939588546753
Epoch: 7, Steps: 60 | Train Loss: 0.5835229 Vali Loss: 1.4869485 Test Loss: 0.4377199
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.174128532409668
Epoch: 8, Steps: 60 | Train Loss: 0.5830182 Vali Loss: 1.4814389 Test Loss: 0.4373770
Validation loss decreased (1.486351 --> 1.481439).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.1864378452301025
Epoch: 9, Steps: 60 | Train Loss: 0.5821263 Vali Loss: 1.4807901 Test Loss: 0.4374646
Validation loss decreased (1.481439 --> 1.480790).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2177066802978516
Epoch: 10, Steps: 60 | Train Loss: 0.5820607 Vali Loss: 1.4869714 Test Loss: 0.4376056
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.2907333374023438
Epoch: 11, Steps: 60 | Train Loss: 0.5813278 Vali Loss: 1.4832035 Test Loss: 0.4378013
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.526263475418091
Epoch: 12, Steps: 60 | Train Loss: 0.5820172 Vali Loss: 1.4896737 Test Loss: 0.4379828
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.012002468109131
Epoch: 13, Steps: 60 | Train Loss: 0.5813383 Vali Loss: 1.4918860 Test Loss: 0.4381834
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.9418833255767822
Epoch: 14, Steps: 60 | Train Loss: 0.5810925 Vali Loss: 1.4845343 Test Loss: 0.4383595
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_192_720_FITS_ETTh1_ftM_sl192_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43629828095436096, mae:0.4507044553756714, rse:0.6323299407958984, corr:[0.22878525 0.2312864  0.23119701 0.2343596  0.231465   0.23006763
 0.23136245 0.23110843 0.23090741 0.2318192  0.23102777 0.23024118
 0.23083082 0.23054342 0.22960871 0.2292172  0.22896224 0.22906737
 0.22870788 0.22810481 0.22799213 0.22809331 0.2284331  0.22949736
 0.22935006 0.22893862 0.22931372 0.22895901 0.22820851 0.22834873
 0.22804269 0.22698396 0.2267229  0.22679016 0.22652893 0.22628431
 0.22629867 0.22630796 0.22633845 0.22600405 0.22546445 0.22561853
 0.22623399 0.22640638 0.22612056 0.22600332 0.22657044 0.22750132
 0.22781642 0.22753187 0.22712559 0.22638494 0.22524081 0.22407392
 0.22351591 0.22295871 0.22249345 0.22216424 0.221746   0.22132151
 0.22101296 0.22066732 0.22019348 0.22021061 0.2204662  0.22063464
 0.22097339 0.22119723 0.22098249 0.2207538  0.22068527 0.22075292
 0.22043245 0.21985921 0.21935934 0.21915579 0.21894886 0.21863873
 0.21863142 0.21845733 0.21789743 0.2173527  0.21724103 0.21698987
 0.2161813  0.21599145 0.2162589  0.21569735 0.21523836 0.21549425
 0.21532464 0.21484256 0.21474545 0.2152724  0.2161202  0.2173377
 0.21810228 0.21816425 0.21812306 0.21820733 0.21832773 0.21804577
 0.21755926 0.21751653 0.21750543 0.21714279 0.21674368 0.21660219
 0.21654873 0.21644452 0.21618375 0.21603285 0.2161043  0.21625273
 0.21626279 0.21625222 0.2163922  0.2164528  0.21641032 0.21651681
 0.2165477  0.21624254 0.21579069 0.21551992 0.21517526 0.2147232
 0.21435925 0.21442464 0.21443146 0.21421139 0.21405277 0.21384563
 0.21345045 0.21331109 0.21351549 0.21315277 0.21289846 0.21342921
 0.21389362 0.2135619  0.21317102 0.21323903 0.21333906 0.21324949
 0.21320295 0.21308635 0.21284665 0.21196127 0.21102712 0.21042255
 0.21019448 0.21013576 0.21010314 0.21000734 0.2098842  0.20983966
 0.20987505 0.20991224 0.20979661 0.2093348  0.209122   0.20927118
 0.20948806 0.20961893 0.2097108  0.20982088 0.21017626 0.21097729
 0.21128406 0.21113466 0.21129046 0.21168275 0.21150889 0.2110214
 0.21082292 0.21103498 0.21105668 0.21084458 0.2108798  0.21096444
 0.21085612 0.21094865 0.21096668 0.21050984 0.21041593 0.21107844
 0.21152437 0.21150821 0.21188608 0.21236497 0.21237797 0.21233954
 0.2120563  0.21149373 0.2110523  0.21069203 0.21032153 0.20989749
 0.20933636 0.20918895 0.20924926 0.20903055 0.2086989  0.20859554
 0.20840755 0.20846218 0.20864558 0.2085756  0.2083085  0.20812139
 0.20833507 0.20830081 0.20808828 0.20813707 0.20821472 0.20833616
 0.20836255 0.20820974 0.20812282 0.20785417 0.2073598  0.20692159
 0.20675625 0.20681967 0.20682037 0.20663878 0.20614171 0.20575811
 0.20558278 0.2055235  0.20536123 0.20505768 0.20491473 0.20509659
 0.20513293 0.20488726 0.20484908 0.20511216 0.2052945  0.2055565
 0.20603946 0.20611957 0.20604959 0.20621368 0.20637025 0.20612459
 0.2057605  0.20575404 0.2060976  0.20611022 0.20582078 0.20547056
 0.20511352 0.20508467 0.20542796 0.20565002 0.20577902 0.20597608
 0.2060547  0.20585443 0.20572239 0.20572399 0.2056825  0.20588046
 0.20616506 0.20591648 0.20560114 0.20559941 0.20547734 0.2050644
 0.2047222  0.20462841 0.20445769 0.20433883 0.2043086  0.20431301
 0.20431499 0.20423979 0.2040315  0.2038778  0.20372084 0.20370558
 0.20370097 0.20352371 0.20339666 0.20359762 0.20406662 0.2046924
 0.20516554 0.20551191 0.20615445 0.20670353 0.20662655 0.20626378
 0.20625824 0.20664231 0.20675686 0.20657487 0.20648234 0.2063415
 0.20625135 0.20651849 0.20666593 0.20650493 0.20647757 0.20648831
 0.20662321 0.20677379 0.20672244 0.20671152 0.20678808 0.2068811
 0.20711644 0.2075258  0.20767887 0.20734453 0.2070458  0.20674941
 0.20648527 0.2064985  0.20628984 0.20586483 0.20559771 0.20566534
 0.20570058 0.20565088 0.205631   0.20581023 0.20597805 0.20614456
 0.20615932 0.20600401 0.20601109 0.20629434 0.20659617 0.20675248
 0.2068713  0.20693782 0.206929   0.2065993  0.20613092 0.20575395
 0.20550752 0.20557949 0.2058196  0.20561506 0.20526662 0.20522542
 0.20525573 0.20519777 0.20519304 0.20502645 0.20479435 0.20503952
 0.20514695 0.2047712  0.20452036 0.20448762 0.2039625  0.20365657
 0.20387278 0.20415309 0.20451228 0.20476338 0.20445797 0.20392598
 0.2037659  0.20359476 0.20305951 0.20268843 0.20274454 0.20295772
 0.20290846 0.20223665 0.20157884 0.20134942 0.20133333 0.2014938
 0.20196576 0.20227167 0.20186915 0.20141664 0.20157802 0.2027923
 0.20444041 0.2057314  0.20649122 0.20642316 0.20608361 0.20570926
 0.20520142 0.20478058 0.20451167 0.20426996 0.20397344 0.20368825
 0.20341104 0.20344564 0.20356841 0.20336568 0.20329763 0.2036242
 0.20406003 0.2044494  0.20438701 0.2043193  0.20486791 0.20587948
 0.20678799 0.20732602 0.20749141 0.20744276 0.20750286 0.20722806
 0.20652255 0.20643075 0.2067103  0.2065607  0.2066334  0.20695657
 0.20683622 0.20675136 0.20685652 0.20646136 0.20619687 0.20688939
 0.20724986 0.20667036 0.20645435 0.20711711 0.20737147 0.20784926
 0.20896915 0.20964977 0.20962937 0.2095263  0.20954956 0.20950517
 0.20926912 0.20911098 0.2090115  0.20887262 0.20882583 0.20886533
 0.20881478 0.20858806 0.2086218  0.20869711 0.20829563 0.20783128
 0.20803802 0.20833145 0.208235   0.20855518 0.20958957 0.21068051
 0.21112539 0.21132614 0.2116092  0.21170346 0.2113068  0.21057607
 0.20998323 0.20990801 0.20992745 0.20984071 0.20993765 0.21019563
 0.2101889  0.21011484 0.2104307  0.21068335 0.21071526 0.21091025
 0.21116967 0.21103565 0.21078256 0.21077983 0.21127939 0.21182546
 0.21202484 0.21197054 0.21205324 0.2122288  0.21193254 0.21152544
 0.21135327 0.21112628 0.21075384 0.21079026 0.21080709 0.2106785
 0.21068215 0.21056797 0.21036804 0.21026401 0.21022013 0.21003467
 0.21006481 0.21026772 0.21012966 0.21017008 0.21091512 0.21164547
 0.21215244 0.21303742 0.21373983 0.21358122 0.21310517 0.21278065
 0.21241063 0.2119314  0.21159318 0.21119507 0.21096429 0.21109417
 0.21144693 0.21178201 0.21175405 0.21181758 0.2122758  0.21254444
 0.21262911 0.21288265 0.21280837 0.212569   0.21338062 0.21463169
 0.21459948 0.21422273 0.21428283 0.21400905 0.21324645 0.21273908
 0.2124747  0.21211547 0.21185438 0.21161899 0.21162465 0.21179144
 0.21185912 0.21191446 0.21202539 0.21200831 0.2120837  0.21244293
 0.21268246 0.21280596 0.2128502  0.21296945 0.21314478 0.21318485
 0.21287704 0.21250482 0.21254039 0.21204372 0.21097492 0.21017623
 0.209723   0.20899685 0.20848826 0.20811357 0.20748006 0.20733935
 0.20752965 0.20740777 0.20713733 0.20705366 0.20725785 0.20739214
 0.20742227 0.2072725  0.20715472 0.20717336 0.20741281 0.20750402
 0.20693843 0.2060489  0.20506185 0.2046379  0.20438668 0.20349775
 0.20256993 0.2021266  0.20190446 0.20187606 0.20184137 0.2017787
 0.20190868 0.20185637 0.20147684 0.20109007 0.20110019 0.20119876
 0.2012293  0.20130773 0.20135504 0.20102102 0.20055147 0.20022416
 0.1999381  0.19931003 0.19860943 0.19798043 0.19709295 0.19622341
 0.195545   0.19496079 0.19444945 0.19424844 0.19413099 0.19397546
 0.19400796 0.19385593 0.19371444 0.19385381 0.19395494 0.19392382
 0.19416368 0.1942844  0.1943371  0.19455607 0.19456817 0.19440134
 0.19438383 0.19410715 0.19338788 0.19256961 0.19190206 0.19123666
 0.19071229 0.1901133  0.18944795 0.18956812 0.18982    0.18963312
 0.18947066 0.18942343 0.18928072 0.18907672 0.18889512 0.18900667
 0.18921396 0.18910234 0.18900988 0.1892487  0.1894618  0.18965328
 0.18953857 0.18896341 0.18832153 0.18794881 0.1873253  0.18654662
 0.18643858 0.18639384 0.18566011 0.18524332 0.18559276 0.1860376
 0.1859995  0.18570901 0.18558131 0.1856274  0.18574306 0.18589582
 0.18619159 0.18612167 0.18596645 0.18632394 0.18638715 0.18575317
 0.18517317 0.18462278 0.18361637 0.18220073 0.1813119  0.18051842
 0.17979175 0.17930055 0.17897949 0.17883237 0.1788969  0.179044
 0.17946886 0.1796224  0.17937839 0.17910501 0.17888348 0.17897053
 0.17935927 0.17964433 0.1795911  0.17955066 0.17968649 0.17966516
 0.17940302 0.17897312 0.17833216 0.17797382 0.17735249 0.17601594
 0.17515948 0.17522152 0.17514004 0.17475414 0.17433612 0.17421027
 0.17405798 0.17378844 0.17353031 0.17284928 0.17243746 0.17301555
 0.173491   0.1730284  0.17457691 0.17519918 0.1740597  0.17544961]
