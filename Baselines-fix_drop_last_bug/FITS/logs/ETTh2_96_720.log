Args in experiment:
Namespace(is_training=1, model_id='ETTh2_96_720', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=40, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_96_720_FITS_ETTh2_ftM_sl96_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=40, out_features=340, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12185600.0
params:  13940.0
Trainable parameters:  13940
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.3273444175720215
Epoch: 1, Steps: 61 | Train Loss: 1.3068862 Vali Loss: 0.8705604 Test Loss: 0.7004333
Validation loss decreased (inf --> 0.870560).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.7513279914855957
Epoch: 2, Steps: 61 | Train Loss: 1.0720443 Vali Loss: 0.7825291 Test Loss: 0.5947024
Validation loss decreased (0.870560 --> 0.782529).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.634284734725952
Epoch: 3, Steps: 61 | Train Loss: 0.9500590 Vali Loss: 0.7278152 Test Loss: 0.5334679
Validation loss decreased (0.782529 --> 0.727815).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.3615543842315674
Epoch: 4, Steps: 61 | Train Loss: 0.8784762 Vali Loss: 0.6930120 Test Loss: 0.4964155
Validation loss decreased (0.727815 --> 0.693012).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.4500038623809814
Epoch: 5, Steps: 61 | Train Loss: 0.8366912 Vali Loss: 0.6779462 Test Loss: 0.4734805
Validation loss decreased (0.693012 --> 0.677946).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2600178718566895
Epoch: 6, Steps: 61 | Train Loss: 0.8113334 Vali Loss: 0.6692826 Test Loss: 0.4591365
Validation loss decreased (0.677946 --> 0.669283).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3312199115753174
Epoch: 7, Steps: 61 | Train Loss: 0.7946442 Vali Loss: 0.6633821 Test Loss: 0.4498825
Validation loss decreased (0.669283 --> 0.663382).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.2450625896453857
Epoch: 8, Steps: 61 | Train Loss: 0.7841119 Vali Loss: 0.6487827 Test Loss: 0.4438667
Validation loss decreased (0.663382 --> 0.648783).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.2568399906158447
Epoch: 9, Steps: 61 | Train Loss: 0.7769701 Vali Loss: 0.6494899 Test Loss: 0.4397696
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.986724376678467
Epoch: 10, Steps: 61 | Train Loss: 0.7725340 Vali Loss: 0.6489871 Test Loss: 0.4368703
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.2856385707855225
Epoch: 11, Steps: 61 | Train Loss: 0.7685441 Vali Loss: 0.6467060 Test Loss: 0.4347346
Validation loss decreased (0.648783 --> 0.646706).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.212916612625122
Epoch: 12, Steps: 61 | Train Loss: 0.7651423 Vali Loss: 0.6391270 Test Loss: 0.4330823
Validation loss decreased (0.646706 --> 0.639127).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.8489208221435547
Epoch: 13, Steps: 61 | Train Loss: 0.7638688 Vali Loss: 0.6403522 Test Loss: 0.4318433
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.2887156009674072
Epoch: 14, Steps: 61 | Train Loss: 0.7629917 Vali Loss: 0.6368306 Test Loss: 0.4308043
Validation loss decreased (0.639127 --> 0.636831).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.279879093170166
Epoch: 15, Steps: 61 | Train Loss: 0.7613165 Vali Loss: 0.6412575 Test Loss: 0.4299184
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.2492434978485107
Epoch: 16, Steps: 61 | Train Loss: 0.7603772 Vali Loss: 0.6392154 Test Loss: 0.4291595
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.444333553314209
Epoch: 17, Steps: 61 | Train Loss: 0.7594460 Vali Loss: 0.6364302 Test Loss: 0.4285221
Validation loss decreased (0.636831 --> 0.636430).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.955569267272949
Epoch: 18, Steps: 61 | Train Loss: 0.7586284 Vali Loss: 0.6361327 Test Loss: 0.4279518
Validation loss decreased (0.636430 --> 0.636133).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.379880666732788
Epoch: 19, Steps: 61 | Train Loss: 0.7580127 Vali Loss: 0.6342717 Test Loss: 0.4274359
Validation loss decreased (0.636133 --> 0.634272).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.2920620441436768
Epoch: 20, Steps: 61 | Train Loss: 0.7558455 Vali Loss: 0.6380997 Test Loss: 0.4269804
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.3503615856170654
Epoch: 21, Steps: 61 | Train Loss: 0.7567623 Vali Loss: 0.6342452 Test Loss: 0.4265724
Validation loss decreased (0.634272 --> 0.634245).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.3943493366241455
Epoch: 22, Steps: 61 | Train Loss: 0.7557333 Vali Loss: 0.6382881 Test Loss: 0.4261900
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.9616823196411133
Epoch: 23, Steps: 61 | Train Loss: 0.7551054 Vali Loss: 0.6358708 Test Loss: 0.4258362
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.3679351806640625
Epoch: 24, Steps: 61 | Train Loss: 0.7557161 Vali Loss: 0.6324172 Test Loss: 0.4255517
Validation loss decreased (0.634245 --> 0.632417).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.41817045211792
Epoch: 25, Steps: 61 | Train Loss: 0.7539711 Vali Loss: 0.6331658 Test Loss: 0.4252657
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.2320289611816406
Epoch: 26, Steps: 61 | Train Loss: 0.7544052 Vali Loss: 0.6283594 Test Loss: 0.4250023
Validation loss decreased (0.632417 --> 0.628359).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.370365619659424
Epoch: 27, Steps: 61 | Train Loss: 0.7523469 Vali Loss: 0.6309966 Test Loss: 0.4247315
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.5069432258605957
Epoch: 28, Steps: 61 | Train Loss: 0.7542671 Vali Loss: 0.6321571 Test Loss: 0.4244962
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.4283595085144043
Epoch: 29, Steps: 61 | Train Loss: 0.7537744 Vali Loss: 0.6359770 Test Loss: 0.4243002
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.087970018386841
Epoch: 30, Steps: 61 | Train Loss: 0.7532268 Vali Loss: 0.6361543 Test Loss: 0.4241056
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00011296777049628277
train 7825
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=40, out_features=340, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12185600.0
params:  13940.0
Trainable parameters:  13940
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.8201754093170166
Epoch: 1, Steps: 61 | Train Loss: 0.8524479 Vali Loss: 0.6299169 Test Loss: 0.4225654
Validation loss decreased (inf --> 0.629917).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.2501611709594727
Epoch: 2, Steps: 61 | Train Loss: 0.8493556 Vali Loss: 0.6235079 Test Loss: 0.4209606
Validation loss decreased (0.629917 --> 0.623508).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.0659403800964355
Epoch: 3, Steps: 61 | Train Loss: 0.8475269 Vali Loss: 0.6267321 Test Loss: 0.4200185
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.452792167663574
Epoch: 4, Steps: 61 | Train Loss: 0.8452820 Vali Loss: 0.6268731 Test Loss: 0.4193841
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.5768611431121826
Epoch: 5, Steps: 61 | Train Loss: 0.8454447 Vali Loss: 0.6280839 Test Loss: 0.4189383
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.253593683242798
Epoch: 6, Steps: 61 | Train Loss: 0.8442022 Vali Loss: 0.6216828 Test Loss: 0.4186797
Validation loss decreased (0.623508 --> 0.621683).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.5707905292510986
Epoch: 7, Steps: 61 | Train Loss: 0.8447922 Vali Loss: 0.6273317 Test Loss: 0.4184549
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.360956907272339
Epoch: 8, Steps: 61 | Train Loss: 0.8440461 Vali Loss: 0.6262583 Test Loss: 0.4183457
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.2441725730895996
Epoch: 9, Steps: 61 | Train Loss: 0.8432554 Vali Loss: 0.6276449 Test Loss: 0.4182279
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2917559146881104
Epoch: 10, Steps: 61 | Train Loss: 0.8440319 Vali Loss: 0.6242894 Test Loss: 0.4181371
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.3706657886505127
Epoch: 11, Steps: 61 | Train Loss: 0.8430763 Vali Loss: 0.6232970 Test Loss: 0.4180751
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh2_96_720_FITS_ETTh2_ftM_sl96_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.41706550121307373, mae:0.43623000383377075, rse:0.5161879658699036, corr:[ 0.22207747  0.2201976   0.22004162  0.2189596   0.21727361  0.21679473
  0.21561106  0.21546072  0.21398443  0.2127743   0.21174799  0.2106424
  0.20921499  0.20781982  0.20680067  0.20601623  0.20525396  0.20472959
  0.20407206  0.20302463  0.20277835  0.20152292  0.20062007  0.19967312
  0.1969653   0.19635007  0.19497354  0.19349243  0.1933904   0.19230242
  0.19150266  0.19107604  0.19029303  0.1889035   0.18799967  0.18694177
  0.18574701  0.18520062  0.18416609  0.18362442  0.1832304   0.18276699
  0.182495    0.18177587  0.18152831  0.18069662  0.17987652  0.17835708
  0.1755237   0.17360169  0.17260185  0.17152244  0.17048582  0.16991383
  0.16893832  0.16782258  0.16678232  0.1661822   0.165823    0.16519964
  0.1649913   0.16431859  0.16389619  0.16445509  0.16487548  0.16487075
  0.16452739  0.16428685  0.16439229  0.16422518  0.1642786   0.16375786
  0.16232654  0.16119817  0.16087566  0.16053377  0.15966687  0.1591926
  0.15890943  0.15863387  0.15798795  0.15734991  0.157166    0.15714547
  0.15740341  0.15697226  0.15682207  0.15736657  0.15690717  0.1568361
  0.15721457  0.15650736  0.15643363  0.15627752  0.15582892  0.15566176
  0.15464619  0.15312538  0.15262763  0.15246694  0.15157174  0.1505813
  0.15001805  0.14924273  0.14909281  0.14896308  0.14872624  0.14857522
  0.14808668  0.147375    0.1474567   0.1476358   0.14719462  0.14701591
  0.14701414  0.1468001   0.14631815  0.14640807  0.14601736  0.14425673
  0.14284116  0.14170656  0.14061311  0.14001735  0.13892889  0.1380245
  0.13788101  0.13735074  0.13642822  0.13588466  0.13577858  0.13499537
  0.13425854  0.13381708  0.13318327  0.13265395  0.13249321  0.13225433
  0.13121827  0.13047078  0.13024344  0.12954314  0.12932923  0.12823924
  0.12559924  0.12399896  0.12275436  0.12177719  0.12161559  0.12088841
  0.11980798  0.11912122  0.1187607   0.11868359  0.11879     0.11800551
  0.11730839  0.11728178  0.11722505  0.1169683   0.11672521  0.11639261
  0.11669456  0.11678748  0.11661173  0.11635481  0.1158944   0.1148733
  0.11327214  0.11270003  0.11236974  0.11152694  0.11098413  0.1105305
  0.11011785  0.10942587  0.10920573  0.1089991   0.10856483  0.10831443
  0.10818681  0.1081131   0.10783666  0.10776606  0.10820724  0.10869061
  0.10894446  0.10925974  0.10949823  0.11005641  0.11032002  0.10992292
  0.11045548  0.11117256  0.1106308   0.11048678  0.11070099  0.11045086
  0.11022699  0.11032287  0.11109094  0.11101577  0.1105666   0.1107205
  0.11087664  0.11111197  0.11052194  0.11015773  0.11101442  0.1115841
  0.1122943   0.11260382  0.1124071   0.11255708  0.11261195  0.11237714
  0.11235473  0.11171839  0.11147231  0.11176867  0.11074343  0.11044093
  0.11126454  0.11082501  0.11053283  0.11131775  0.11146585  0.11094731
  0.11119856  0.11162572  0.11177078  0.11165013  0.11182237  0.11316651
  0.11417022  0.11402346  0.11450166  0.11518629  0.11565629  0.11615186
  0.11641023  0.11637826  0.11575963  0.11550951  0.11604593  0.11612745
  0.11570631  0.11565012  0.11650469  0.11656857  0.11646713  0.11690263
  0.11716925  0.11739629  0.11735747  0.11780299  0.11815283  0.11880154
  0.11942459  0.11972092  0.12036689  0.12080856  0.12112103  0.12145636
  0.12157213  0.12192755  0.1223717   0.12226302  0.12170833  0.1217565
  0.12229532  0.12266161  0.12305444  0.12304997  0.12349818  0.12368131
  0.12409152  0.12421808  0.1241081   0.12441704  0.12483744  0.125028
  0.12534131  0.12548622  0.12566763  0.12603854  0.12633866  0.12617035
  0.12621848  0.12632306  0.12605554  0.12566707  0.1251664   0.12534972
  0.12542099  0.12480521  0.1245736   0.12499112  0.12524126  0.125059
  0.12516896  0.12568349  0.1260304   0.12584066  0.12630239  0.12699811
  0.12673259  0.12698303  0.12732781  0.1271055   0.1277603   0.12786776
  0.12698115  0.1273598   0.12763962  0.12719955  0.12696855  0.12770005
  0.1279232   0.12701283  0.12746994  0.12729476  0.12721688  0.12788208
  0.12769803  0.12854375  0.12864722  0.128548    0.12937286  0.12906209
  0.12936692  0.13012756  0.13063574  0.1313105   0.13135815  0.13225304
  0.13274314  0.13294266  0.13334005  0.13325492  0.13398759  0.13413396
  0.13398753  0.13470817  0.13524236  0.13460769  0.13467447  0.1359896
  0.1360861   0.13573168  0.1360634   0.13563487  0.13647062  0.13781975
  0.13814531  0.13937785  0.14016962  0.14049096  0.14178981  0.14242958
  0.14214599  0.14253858  0.1432194   0.14373896  0.14371389  0.14403383
  0.1448641   0.14579642  0.14747767  0.14866652  0.14950486  0.1506056
  0.15062828  0.15111937  0.1517324   0.15192607  0.15286377  0.15406907
  0.15489432  0.15573965  0.156839    0.15693852  0.15771589  0.15955834
  0.16052637  0.16133757  0.16212279  0.16223714  0.16210714  0.16242115
  0.16398072  0.16444731  0.16445118  0.165348    0.16575752  0.16605018
  0.16687374  0.16691677  0.16679657  0.16732442  0.16757196  0.16743197
  0.1673392   0.16792981  0.16868655  0.16855142  0.16903147  0.16950814
  0.16955695  0.17055908  0.17015988  0.16937743  0.1699386   0.16955668
  0.16925952  0.16954225  0.16977094  0.17003225  0.16985226  0.16997012
  0.17027134  0.17067602  0.1705589   0.1702474   0.17056659  0.1704495
  0.17042291  0.17093408  0.17127429  0.17130682  0.17121893  0.17134517
  0.1712433   0.17148021  0.17136246  0.17077006  0.17098074  0.17075415
  0.17063129  0.17078945  0.17031726  0.16989416  0.17007087  0.16995336
  0.16962284  0.16958997  0.16955923  0.16941601  0.16938421  0.16948605
  0.1695146   0.16938387  0.1693089   0.16962017  0.16998953  0.16983995
  0.16937198  0.16932358  0.16917565  0.16854501  0.16827808  0.16795747
  0.16777042  0.16783383  0.16705862  0.16674092  0.16691595  0.16637102
  0.16611017  0.16566503  0.16502857  0.16476102  0.16484325  0.16530018
  0.1648843   0.16470577  0.16466892  0.16349192  0.16327639  0.16316107
  0.16177145  0.1607716   0.1601829   0.15929255  0.15831259  0.15737261
  0.15676183  0.15627421  0.1555297   0.1547421   0.15423787  0.15372714
  0.15303247  0.1519286   0.15188573  0.15232727  0.1514171   0.15128517
  0.15092245  0.15025157  0.15055418  0.14984553  0.14918669  0.14829329
  0.14574122  0.14466111  0.14395224  0.14247842  0.14209686  0.14167789
  0.14060538  0.14057843  0.14018601  0.13946079  0.1400629   0.13888551
  0.13656077  0.13626336  0.13657053  0.13618046  0.13548073  0.13476515
  0.13462152  0.13408172  0.13261679  0.13252024  0.13211927  0.12995355
  0.12768559  0.12561701  0.12392576  0.12200744  0.12018753  0.12011327
  0.11912412  0.11724443  0.11676756  0.1163822   0.11580695  0.11526624
  0.11416178  0.11274271  0.11221059  0.11124285  0.11004769  0.10998932
  0.10850196  0.10827119  0.1077978   0.1053689   0.10450596  0.10278141
  0.10087679  0.10003683  0.09807581  0.09690722  0.09605746  0.0947258
  0.09317052  0.0919432   0.0909805   0.08973313  0.09033827  0.09000364
  0.08841658  0.0877233   0.08715843  0.08628104  0.08497172  0.08533522
  0.08528459  0.08329262  0.08232802  0.08171643  0.08139254  0.07938286
  0.07648633  0.07565229  0.0744482   0.07298911  0.07184133  0.0708644
  0.07013895  0.06843092  0.06725209  0.0670284   0.06703032  0.06636262
  0.06469239  0.06436129  0.06467306  0.06447131  0.0635627   0.06221429
  0.06207158  0.06176787  0.06101177  0.06120959  0.06052974  0.0584355
  0.05641875  0.05513667  0.05404004  0.0527654   0.05230111  0.05155076
  0.04975406  0.04865242  0.04773212  0.0471536   0.04713975  0.04650515
  0.04582298  0.04502183  0.04421311  0.04397517  0.04380782  0.0437295
  0.04294037  0.04215159  0.04141085  0.04055504  0.04036181  0.03925917
  0.03716746  0.03614558  0.03461443  0.03258314  0.0312583   0.02990515
  0.02922426  0.02902398  0.02742467  0.02574644  0.02527321  0.02517193
  0.02453124  0.02326445  0.02318191  0.02347734  0.02235842  0.02114027
  0.02064781  0.02017703  0.01867161  0.01832107  0.01856922  0.01655607
  0.0142904   0.01231456  0.01099504  0.01010884  0.00928475  0.00892639
  0.00724922  0.00625092  0.00555764  0.00533572  0.00675546  0.00542779
  0.00420318  0.00462625  0.00358734  0.00313817  0.00299465  0.00230619
  0.00289448  0.00305674  0.00166189  0.00129459  0.00105776 -0.00094309
 -0.00289951 -0.00347101 -0.00425497 -0.00646421 -0.00686655 -0.00718787
 -0.00929317 -0.00933499 -0.0097348  -0.00973972 -0.00870781 -0.01022379
 -0.01128717 -0.0117815  -0.011798   -0.01212521 -0.01162509 -0.01052285
 -0.01241482 -0.01269021 -0.01265286 -0.01361997 -0.01339559 -0.0149097
 -0.01676377 -0.01765424 -0.01856459 -0.02014004 -0.02065922 -0.02014932
 -0.02149523 -0.02205531 -0.02281533 -0.02405586 -0.02269153 -0.02290261
 -0.02387903 -0.02428781 -0.02499037 -0.02544628 -0.02529745 -0.02544175
 -0.02586374 -0.02664043 -0.02778948 -0.02624016 -0.02705108 -0.02166942]
