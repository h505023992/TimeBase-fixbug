Args in experiment:
Namespace(is_training=1, model_id='electricity_336_96', model='FITS', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, individual=False, embed_type=0, enc_in=321, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=130, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : electricity_336_96_FITS_custom_ftM_sl336_ll48_pl96_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17981
val 2537
test 5165
Model(
  (freq_upsampler): Linear(in_features=130, out_features=167, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  892020480.0
params:  21877.0
Trainable parameters:  21877
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7907128
	speed: 0.1879s/iter; left time: 770.5229s
Epoch: 1 cost time: 25.586087226867676
Epoch: 1, Steps: 140 | Train Loss: 0.9005507 Vali Loss: 0.6781850 Test Loss: 0.7769310
Validation loss decreased (inf --> 0.678185).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6327464
	speed: 0.4083s/iter; left time: 1617.2675s
Epoch: 2 cost time: 25.82070827484131
Epoch: 2, Steps: 140 | Train Loss: 0.6610915 Vali Loss: 0.5801154 Test Loss: 0.6647642
Validation loss decreased (0.678185 --> 0.580115).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5279656
	speed: 0.4223s/iter; left time: 1613.4421s
Epoch: 3 cost time: 26.56060290336609
Epoch: 3, Steps: 140 | Train Loss: 0.5612976 Vali Loss: 0.5147180 Test Loss: 0.5917027
Validation loss decreased (0.580115 --> 0.514718).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4611273
	speed: 0.4214s/iter; left time: 1551.0508s
Epoch: 4 cost time: 26.37100839614868
Epoch: 4, Steps: 140 | Train Loss: 0.4841075 Vali Loss: 0.4602268 Test Loss: 0.5303643
Validation loss decreased (0.514718 --> 0.460227).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4067360
	speed: 0.4421s/iter; left time: 1565.5526s
Epoch: 5 cost time: 27.63506007194519
Epoch: 5, Steps: 140 | Train Loss: 0.4199573 Vali Loss: 0.4156828 Test Loss: 0.4798824
Validation loss decreased (0.460227 --> 0.415683).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3609479
	speed: 0.4334s/iter; left time: 1473.8697s
Epoch: 6 cost time: 26.249887228012085
Epoch: 6, Steps: 140 | Train Loss: 0.3659747 Vali Loss: 0.3751306 Test Loss: 0.4343344
Validation loss decreased (0.415683 --> 0.375131).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3132993
	speed: 0.4286s/iter; left time: 1397.5096s
Epoch: 7 cost time: 25.451733589172363
Epoch: 7, Steps: 140 | Train Loss: 0.3204043 Vali Loss: 0.3418028 Test Loss: 0.3970729
Validation loss decreased (0.375131 --> 0.341803).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2794317
	speed: 0.4304s/iter; left time: 1343.1645s
Epoch: 8 cost time: 26.197449207305908
Epoch: 8, Steps: 140 | Train Loss: 0.2816560 Vali Loss: 0.3120331 Test Loss: 0.3629846
Validation loss decreased (0.341803 --> 0.312033).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2404356
	speed: 0.4425s/iter; left time: 1319.1758s
Epoch: 9 cost time: 26.38331627845764
Epoch: 9, Steps: 140 | Train Loss: 0.2487788 Vali Loss: 0.2877290 Test Loss: 0.3358397
Validation loss decreased (0.312033 --> 0.287729).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2210049
	speed: 0.4324s/iter; left time: 1228.3968s
Epoch: 10 cost time: 26.52314043045044
Epoch: 10, Steps: 140 | Train Loss: 0.2206835 Vali Loss: 0.2667061 Test Loss: 0.3119923
Validation loss decreased (0.287729 --> 0.266706).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1942102
	speed: 0.4345s/iter; left time: 1173.6667s
Epoch: 11 cost time: 26.613582372665405
Epoch: 11, Steps: 140 | Train Loss: 0.1965925 Vali Loss: 0.2475644 Test Loss: 0.2898915
Validation loss decreased (0.266706 --> 0.247564).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1730846
	speed: 0.4485s/iter; left time: 1148.5634s
Epoch: 12 cost time: 28.36690664291382
Epoch: 12, Steps: 140 | Train Loss: 0.1759207 Vali Loss: 0.2320488 Test Loss: 0.2727818
Validation loss decreased (0.247564 --> 0.232049).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1571706
	speed: 0.4432s/iter; left time: 1073.0349s
Epoch: 13 cost time: 27.70906901359558
Epoch: 13, Steps: 140 | Train Loss: 0.1581395 Vali Loss: 0.2187992 Test Loss: 0.2573813
Validation loss decreased (0.232049 --> 0.218799).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1377458
	speed: 0.4373s/iter; left time: 997.3717s
Epoch: 14 cost time: 27.089847087860107
Epoch: 14, Steps: 140 | Train Loss: 0.1427859 Vali Loss: 0.2061637 Test Loss: 0.2433848
Validation loss decreased (0.218799 --> 0.206164).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1287714
	speed: 0.4371s/iter; left time: 935.7876s
Epoch: 15 cost time: 26.810498237609863
Epoch: 15, Steps: 140 | Train Loss: 0.1295334 Vali Loss: 0.1954384 Test Loss: 0.2307138
Validation loss decreased (0.206164 --> 0.195438).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1131008
	speed: 0.4262s/iter; left time: 852.7510s
Epoch: 16 cost time: 24.81386923789978
Epoch: 16, Steps: 140 | Train Loss: 0.1180396 Vali Loss: 0.1862282 Test Loss: 0.2203027
Validation loss decreased (0.195438 --> 0.186228).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1024864
	speed: 0.4270s/iter; left time: 794.7343s
Epoch: 17 cost time: 27.004071950912476
Epoch: 17, Steps: 140 | Train Loss: 0.1080781 Vali Loss: 0.1790066 Test Loss: 0.2115729
Validation loss decreased (0.186228 --> 0.179007).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.0999984
	speed: 0.4386s/iter; left time: 754.7770s
Epoch: 18 cost time: 26.369248628616333
Epoch: 18, Steps: 140 | Train Loss: 0.0994256 Vali Loss: 0.1723612 Test Loss: 0.2041015
Validation loss decreased (0.179007 --> 0.172361).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.0924114
	speed: 0.4192s/iter; left time: 662.7134s
Epoch: 19 cost time: 25.1506085395813
Epoch: 19, Steps: 140 | Train Loss: 0.0919003 Vali Loss: 0.1666918 Test Loss: 0.1972352
Validation loss decreased (0.172361 --> 0.166692).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.0843322
	speed: 0.4272s/iter; left time: 615.5917s
Epoch: 20 cost time: 25.589836835861206
Epoch: 20, Steps: 140 | Train Loss: 0.0853099 Vali Loss: 0.1610933 Test Loss: 0.1908913
Validation loss decreased (0.166692 --> 0.161093).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.0788823
	speed: 0.4303s/iter; left time: 559.8030s
Epoch: 21 cost time: 26.66031289100647
Epoch: 21, Steps: 140 | Train Loss: 0.0795813 Vali Loss: 0.1565623 Test Loss: 0.1856184
Validation loss decreased (0.161093 --> 0.156562).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.0711402
	speed: 0.4412s/iter; left time: 512.2667s
Epoch: 22 cost time: 26.47425365447998
Epoch: 22, Steps: 140 | Train Loss: 0.0745735 Vali Loss: 0.1529064 Test Loss: 0.1809810
Validation loss decreased (0.156562 --> 0.152906).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.0687638
	speed: 0.4338s/iter; left time: 442.9358s
Epoch: 23 cost time: 27.319892644882202
Epoch: 23, Steps: 140 | Train Loss: 0.0702036 Vali Loss: 0.1492451 Test Loss: 0.1770328
Validation loss decreased (0.152906 --> 0.149245).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.0647557
	speed: 0.4368s/iter; left time: 384.8242s
Epoch: 24 cost time: 27.014570713043213
Epoch: 24, Steps: 140 | Train Loss: 0.0663520 Vali Loss: 0.1462175 Test Loss: 0.1732441
Validation loss decreased (0.149245 --> 0.146217).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.0631815
	speed: 0.4351s/iter; left time: 322.4320s
Epoch: 25 cost time: 26.29083251953125
Epoch: 25, Steps: 140 | Train Loss: 0.0629993 Vali Loss: 0.1431441 Test Loss: 0.1697360
Validation loss decreased (0.146217 --> 0.143144).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.0588467
	speed: 0.4251s/iter; left time: 255.5110s
Epoch: 26 cost time: 26.45844578742981
Epoch: 26, Steps: 140 | Train Loss: 0.0600452 Vali Loss: 0.1411728 Test Loss: 0.1672546
Validation loss decreased (0.143144 --> 0.141173).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.0579678
	speed: 0.4358s/iter; left time: 200.9108s
Epoch: 27 cost time: 27.326777696609497
Epoch: 27, Steps: 140 | Train Loss: 0.0574405 Vali Loss: 0.1395421 Test Loss: 0.1650928
Validation loss decreased (0.141173 --> 0.139542).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.0537494
	speed: 0.4449s/iter; left time: 142.8098s
Epoch: 28 cost time: 25.875247478485107
Epoch: 28, Steps: 140 | Train Loss: 0.0551834 Vali Loss: 0.1374986 Test Loss: 0.1630711
Validation loss decreased (0.139542 --> 0.137499).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.0527301
	speed: 0.4285s/iter; left time: 77.5600s
Epoch: 29 cost time: 25.583776712417603
Epoch: 29, Steps: 140 | Train Loss: 0.0531749 Vali Loss: 0.1359067 Test Loss: 0.1609221
Validation loss decreased (0.137499 --> 0.135907).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.0540505
	speed: 0.4260s/iter; left time: 17.4654s
Epoch: 30 cost time: 26.91724395751953
Epoch: 30, Steps: 140 | Train Loss: 0.0514194 Vali Loss: 0.1348892 Test Loss: 0.1596697
Validation loss decreased (0.135907 --> 0.134889).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 17981
val 2537
test 5165
Model(
  (freq_upsampler): Linear(in_features=130, out_features=167, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  892020480.0
params:  21877.0
Trainable parameters:  21877
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.1504255
	speed: 0.1934s/iter; left time: 793.2348s
Epoch: 1 cost time: 26.413331270217896
Epoch: 1, Steps: 140 | Train Loss: 0.1465313 Vali Loss: 0.1232153 Test Loss: 0.1438299
Validation loss decreased (inf --> 0.123215).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1552547
	speed: 0.4221s/iter; left time: 1671.9850s
Epoch: 2 cost time: 25.79935097694397
Epoch: 2, Steps: 140 | Train Loss: 0.1444739 Vali Loss: 0.1228922 Test Loss: 0.1434729
Validation loss decreased (0.123215 --> 0.122892).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1375691
	speed: 0.4359s/iter; left time: 1665.5850s
Epoch: 3 cost time: 25.639962673187256
Epoch: 3, Steps: 140 | Train Loss: 0.1441758 Vali Loss: 0.1226902 Test Loss: 0.1432795
Validation loss decreased (0.122892 --> 0.122690).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1382608
	speed: 0.4183s/iter; left time: 1539.9301s
Epoch: 4 cost time: 23.764880418777466
Epoch: 4, Steps: 140 | Train Loss: 0.1440686 Vali Loss: 0.1225094 Test Loss: 0.1432727
Validation loss decreased (0.122690 --> 0.122509).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1494589
	speed: 0.4376s/iter; left time: 1549.6634s
Epoch: 5 cost time: 26.54050898551941
Epoch: 5, Steps: 140 | Train Loss: 0.1438357 Vali Loss: 0.1224393 Test Loss: 0.1432211
Validation loss decreased (0.122509 --> 0.122439).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1438691
	speed: 0.4245s/iter; left time: 1443.6448s
Epoch: 6 cost time: 26.333385229110718
Epoch: 6, Steps: 140 | Train Loss: 0.1438738 Vali Loss: 0.1225170 Test Loss: 0.1431634
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1458384
	speed: 0.4289s/iter; left time: 1398.6154s
Epoch: 7 cost time: 26.38167667388916
Epoch: 7, Steps: 140 | Train Loss: 0.1438454 Vali Loss: 0.1221199 Test Loss: 0.1431030
Validation loss decreased (0.122439 --> 0.122120).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1374887
	speed: 0.4150s/iter; left time: 1295.0815s
Epoch: 8 cost time: 25.514076709747314
Epoch: 8, Steps: 140 | Train Loss: 0.1437423 Vali Loss: 0.1226802 Test Loss: 0.1430824
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1519631
	speed: 0.4086s/iter; left time: 1218.1668s
Epoch: 9 cost time: 24.605012893676758
Epoch: 9, Steps: 140 | Train Loss: 0.1437710 Vali Loss: 0.1225819 Test Loss: 0.1430059
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1585455
	speed: 0.4139s/iter; left time: 1175.9451s
Epoch: 10 cost time: 24.51971936225891
Epoch: 10, Steps: 140 | Train Loss: 0.1436790 Vali Loss: 0.1223877 Test Loss: 0.1429982
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1494974
	speed: 0.3971s/iter; left time: 1072.5334s
Epoch: 11 cost time: 23.672555923461914
Epoch: 11, Steps: 140 | Train Loss: 0.1437023 Vali Loss: 0.1221746 Test Loss: 0.1430100
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1445355
	speed: 0.3715s/iter; left time: 951.3748s
Epoch: 12 cost time: 23.162742376327515
Epoch: 12, Steps: 140 | Train Loss: 0.1436793 Vali Loss: 0.1223274 Test Loss: 0.1430055
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : electricity_336_96_FITS_custom_ftM_sl336_ll48_pl96_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5165
mse:0.14287960529327393, mae:0.24031008780002594, rse:0.37568944692611694, corr:[0.4649915  0.46868318 0.46983504 0.47015497 0.47045702 0.47042495
 0.47019488 0.47037873 0.4699603  0.46990457 0.46982184 0.4696383
 0.46963596 0.4694292  0.46937117 0.46929863 0.46914065 0.46905884
 0.46897078 0.4689348  0.46877515 0.46874326 0.46888912 0.46904212
 0.46917543 0.46911487 0.46915796 0.46933782 0.46921644 0.46909815
 0.46902508 0.46888182 0.46888316 0.4688479  0.46869415 0.4686246
 0.46862024 0.4685212  0.4684081  0.46841735 0.46839297 0.4682923
 0.46823475 0.46802348 0.46781287 0.46776304 0.4677152  0.46790183
 0.46807212 0.46805865 0.4680774  0.46809202 0.46803832 0.46787965
 0.4677455  0.46780235 0.46780822 0.46774977 0.4677541  0.46771124
 0.4676533  0.46762142 0.4676702  0.46766055 0.4675465  0.46756962
 0.46745786 0.46715036 0.46707788 0.4669732  0.46690708 0.46714863
 0.46716437 0.46708736 0.4671078  0.46703047 0.46695772 0.46695146
 0.46690112 0.46678463 0.46669734 0.46661922 0.46650013 0.46652904
 0.46646854 0.466454   0.46667752 0.46649474 0.46645278 0.46651444
 0.46622747 0.46639505 0.4662137  0.46655926 0.4666122  0.46682143]
