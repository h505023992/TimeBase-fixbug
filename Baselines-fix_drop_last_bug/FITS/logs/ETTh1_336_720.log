Args in experiment:
Namespace(is_training=1, model_id='ETTh1_336_720', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=100, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_336_720_FITS_ETTh1_ftM_sl336_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=100, out_features=314, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  28134400.0
params:  31714.0
Trainable parameters:  31714
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0760810375213623
Epoch: 1, Steps: 59 | Train Loss: 0.9808349 Vali Loss: 2.2906933 Test Loss: 1.0205183
Validation loss decreased (inf --> 2.290693).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.1083977222442627
Epoch: 2, Steps: 59 | Train Loss: 0.7500624 Vali Loss: 2.0069494 Test Loss: 0.8278031
Validation loss decreased (2.290693 --> 2.006949).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.14542293548584
Epoch: 3, Steps: 59 | Train Loss: 0.6436632 Vali Loss: 1.8795434 Test Loss: 0.7430388
Validation loss decreased (2.006949 --> 1.879543).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.0757298469543457
Epoch: 4, Steps: 59 | Train Loss: 0.5927396 Vali Loss: 1.8247247 Test Loss: 0.7010078
Validation loss decreased (1.879543 --> 1.824725).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.394500732421875
Epoch: 5, Steps: 59 | Train Loss: 0.5648488 Vali Loss: 1.7847016 Test Loss: 0.6773750
Validation loss decreased (1.824725 --> 1.784702).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.442533016204834
Epoch: 6, Steps: 59 | Train Loss: 0.5473412 Vali Loss: 1.7620858 Test Loss: 0.6598598
Validation loss decreased (1.784702 --> 1.762086).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1959733963012695
Epoch: 7, Steps: 59 | Train Loss: 0.5344862 Vali Loss: 1.7420201 Test Loss: 0.6463846
Validation loss decreased (1.762086 --> 1.742020).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.184349775314331
Epoch: 8, Steps: 59 | Train Loss: 0.5242328 Vali Loss: 1.7351036 Test Loss: 0.6343430
Validation loss decreased (1.742020 --> 1.735104).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.5640525817871094
Epoch: 9, Steps: 59 | Train Loss: 0.5155869 Vali Loss: 1.7093978 Test Loss: 0.6234446
Validation loss decreased (1.735104 --> 1.709398).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.5748684406280518
Epoch: 10, Steps: 59 | Train Loss: 0.5079826 Vali Loss: 1.7042496 Test Loss: 0.6141411
Validation loss decreased (1.709398 --> 1.704250).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.09566068649292
Epoch: 11, Steps: 59 | Train Loss: 0.5015905 Vali Loss: 1.6898072 Test Loss: 0.6052545
Validation loss decreased (1.704250 --> 1.689807).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.7181591987609863
Epoch: 12, Steps: 59 | Train Loss: 0.4958966 Vali Loss: 1.6816517 Test Loss: 0.5970578
Validation loss decreased (1.689807 --> 1.681652).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.7263808250427246
Epoch: 13, Steps: 59 | Train Loss: 0.4908002 Vali Loss: 1.6717818 Test Loss: 0.5894091
Validation loss decreased (1.681652 --> 1.671782).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.4625139236450195
Epoch: 14, Steps: 59 | Train Loss: 0.4862587 Vali Loss: 1.6546955 Test Loss: 0.5827592
Validation loss decreased (1.671782 --> 1.654696).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.3152785301208496
Epoch: 15, Steps: 59 | Train Loss: 0.4820409 Vali Loss: 1.6498613 Test Loss: 0.5765539
Validation loss decreased (1.654696 --> 1.649861).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.52949595451355
Epoch: 16, Steps: 59 | Train Loss: 0.4783397 Vali Loss: 1.6425294 Test Loss: 0.5703152
Validation loss decreased (1.649861 --> 1.642529).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.5692801475524902
Epoch: 17, Steps: 59 | Train Loss: 0.4747982 Vali Loss: 1.6422604 Test Loss: 0.5650814
Validation loss decreased (1.642529 --> 1.642260).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.2178566455841064
Epoch: 18, Steps: 59 | Train Loss: 0.4716240 Vali Loss: 1.6238000 Test Loss: 0.5601567
Validation loss decreased (1.642260 --> 1.623800).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.149038076400757
Epoch: 19, Steps: 59 | Train Loss: 0.4688857 Vali Loss: 1.6233821 Test Loss: 0.5552652
Validation loss decreased (1.623800 --> 1.623382).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.662290096282959
Epoch: 20, Steps: 59 | Train Loss: 0.4661496 Vali Loss: 1.6144190 Test Loss: 0.5510780
Validation loss decreased (1.623382 --> 1.614419).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.6358370780944824
Epoch: 21, Steps: 59 | Train Loss: 0.4637959 Vali Loss: 1.6110401 Test Loss: 0.5469043
Validation loss decreased (1.614419 --> 1.611040).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.4974300861358643
Epoch: 22, Steps: 59 | Train Loss: 0.4615151 Vali Loss: 1.6071515 Test Loss: 0.5432306
Validation loss decreased (1.611040 --> 1.607152).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.399178981781006
Epoch: 23, Steps: 59 | Train Loss: 0.4594567 Vali Loss: 1.6061739 Test Loss: 0.5397420
Validation loss decreased (1.607152 --> 1.606174).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.391974687576294
Epoch: 24, Steps: 59 | Train Loss: 0.4573686 Vali Loss: 1.5939481 Test Loss: 0.5365070
Validation loss decreased (1.606174 --> 1.593948).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.546091079711914
Epoch: 25, Steps: 59 | Train Loss: 0.4559084 Vali Loss: 1.5962262 Test Loss: 0.5333713
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.4638259410858154
Epoch: 26, Steps: 59 | Train Loss: 0.4538429 Vali Loss: 1.5872695 Test Loss: 0.5305198
Validation loss decreased (1.593948 --> 1.587270).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.479088306427002
Epoch: 27, Steps: 59 | Train Loss: 0.4528674 Vali Loss: 1.5879116 Test Loss: 0.5276976
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.2971508502960205
Epoch: 28, Steps: 59 | Train Loss: 0.4509951 Vali Loss: 1.5856986 Test Loss: 0.5253407
Validation loss decreased (1.587270 --> 1.585699).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.392587423324585
Epoch: 29, Steps: 59 | Train Loss: 0.4498407 Vali Loss: 1.5837904 Test Loss: 0.5229822
Validation loss decreased (1.585699 --> 1.583790).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.4499728679656982
Epoch: 30, Steps: 59 | Train Loss: 0.4484898 Vali Loss: 1.5765806 Test Loss: 0.5206985
Validation loss decreased (1.583790 --> 1.576581).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 7585
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=100, out_features=314, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  28134400.0
params:  31714.0
Trainable parameters:  31714
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.4638779163360596
Epoch: 1, Steps: 59 | Train Loss: 0.6147668 Vali Loss: 1.5274181 Test Loss: 0.4864601
Validation loss decreased (inf --> 1.527418).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.338122844696045
Epoch: 2, Steps: 59 | Train Loss: 0.5990479 Vali Loss: 1.4953595 Test Loss: 0.4652048
Validation loss decreased (1.527418 --> 1.495360).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.4187495708465576
Epoch: 3, Steps: 59 | Train Loss: 0.5894457 Vali Loss: 1.4811951 Test Loss: 0.4515308
Validation loss decreased (1.495360 --> 1.481195).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.5918209552764893
Epoch: 4, Steps: 59 | Train Loss: 0.5831335 Vali Loss: 1.4655278 Test Loss: 0.4422393
Validation loss decreased (1.481195 --> 1.465528).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.436697483062744
Epoch: 5, Steps: 59 | Train Loss: 0.5784893 Vali Loss: 1.4589627 Test Loss: 0.4362277
Validation loss decreased (1.465528 --> 1.458963).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.346006155014038
Epoch: 6, Steps: 59 | Train Loss: 0.5760067 Vali Loss: 1.4502933 Test Loss: 0.4321558
Validation loss decreased (1.458963 --> 1.450293).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3069005012512207
Epoch: 7, Steps: 59 | Train Loss: 0.5738190 Vali Loss: 1.4473660 Test Loss: 0.4397628
Validation loss decreased (1.450293 --> 1.447366).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.4635326862335205
Epoch: 8, Steps: 59 | Train Loss: 0.5724092 Vali Loss: 1.4382086 Test Loss: 0.4381379
Validation loss decreased (1.447366 --> 1.438209).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.382634401321411
Epoch: 9, Steps: 59 | Train Loss: 0.5710443 Vali Loss: 1.4421625 Test Loss: 0.4372529
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.337155342102051
Epoch: 10, Steps: 59 | Train Loss: 0.5702603 Vali Loss: 1.4370530 Test Loss: 0.4366765
Validation loss decreased (1.438209 --> 1.437053).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.4168972969055176
Epoch: 11, Steps: 59 | Train Loss: 0.5697189 Vali Loss: 1.4368620 Test Loss: 0.4364323
Validation loss decreased (1.437053 --> 1.436862).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.171722650527954
Epoch: 12, Steps: 59 | Train Loss: 0.5694114 Vali Loss: 1.4344593 Test Loss: 0.4363762
Validation loss decreased (1.436862 --> 1.434459).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.6599133014678955
Epoch: 13, Steps: 59 | Train Loss: 0.5690440 Vali Loss: 1.4358696 Test Loss: 0.4363899
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.4322476387023926
Epoch: 14, Steps: 59 | Train Loss: 0.5693113 Vali Loss: 1.4363219 Test Loss: 0.4363856
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.550661087036133
Epoch: 15, Steps: 59 | Train Loss: 0.5689359 Vali Loss: 1.4356985 Test Loss: 0.4364778
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.2947161197662354
Epoch: 16, Steps: 59 | Train Loss: 0.5690144 Vali Loss: 1.4327531 Test Loss: 0.4364875
Validation loss decreased (1.434459 --> 1.432753).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.833369731903076
Epoch: 17, Steps: 59 | Train Loss: 0.5686795 Vali Loss: 1.4314709 Test Loss: 0.4366079
Validation loss decreased (1.432753 --> 1.431471).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.7752227783203125
Epoch: 18, Steps: 59 | Train Loss: 0.5687406 Vali Loss: 1.4342666 Test Loss: 0.4367924
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.5088047981262207
Epoch: 19, Steps: 59 | Train Loss: 0.5685348 Vali Loss: 1.4350296 Test Loss: 0.4367573
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.2477550506591797
Epoch: 20, Steps: 59 | Train Loss: 0.5686451 Vali Loss: 1.4298811 Test Loss: 0.4368869
Validation loss decreased (1.431471 --> 1.429881).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.7664694786071777
Epoch: 21, Steps: 59 | Train Loss: 0.5684381 Vali Loss: 1.4330025 Test Loss: 0.4369096
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.674095630645752
Epoch: 22, Steps: 59 | Train Loss: 0.5684541 Vali Loss: 1.4320422 Test Loss: 0.4369668
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.438525915145874
Epoch: 23, Steps: 59 | Train Loss: 0.5685138 Vali Loss: 1.4322942 Test Loss: 0.4370075
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.391909599304199
Epoch: 24, Steps: 59 | Train Loss: 0.5685588 Vali Loss: 1.4346113 Test Loss: 0.4370662
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.4319894313812256
Epoch: 25, Steps: 59 | Train Loss: 0.5679628 Vali Loss: 1.4358423 Test Loss: 0.4370988
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_336_720_FITS_ETTh1_ftM_sl336_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43583101987838745, mae:0.4534346766471863, rse:0.6246987581253052, corr:[0.22825411 0.23283222 0.23310085 0.23612574 0.23376767 0.23090188
 0.23184045 0.23254465 0.23151946 0.23189928 0.2322325  0.23111925
 0.23077893 0.23123729 0.2309048  0.23030597 0.23009826 0.22994614
 0.2297384  0.229677   0.2295112  0.22974493 0.23059304 0.23130128
 0.23124883 0.23159179 0.23185289 0.23136006 0.23101392 0.23118728
 0.23091954 0.230242   0.2298504  0.22965986 0.22937457 0.22918485
 0.22940536 0.22945891 0.22907986 0.2287384  0.22907315 0.22944161
 0.22965744 0.22964412 0.22951624 0.22959214 0.23010221 0.23065154
 0.23074545 0.23028909 0.2296645  0.22913465 0.22845209 0.22772016
 0.22719264 0.22674039 0.22656971 0.22618084 0.22579771 0.22562662
 0.22559264 0.2254139  0.22510351 0.22510326 0.22542308 0.22558147
 0.225669   0.22559446 0.22562404 0.22567064 0.2254434  0.22518998
 0.22474486 0.22402649 0.22352254 0.22358923 0.22354648 0.22300968
 0.22264725 0.2224039  0.22192293 0.22124709 0.22074999 0.22061045
 0.22040632 0.22023837 0.22027434 0.22032163 0.22021833 0.2199301
 0.21945947 0.21920063 0.21948083 0.22009797 0.22070377 0.2217165
 0.22287114 0.22356251 0.2242423  0.22486088 0.22503957 0.22492915
 0.22472209 0.22454557 0.22439487 0.22400011 0.22341669 0.2229581
 0.22276962 0.22274534 0.22264007 0.2226428  0.2229581  0.22317311
 0.22329189 0.22331089 0.2232583  0.22306538 0.22294405 0.22314864
 0.2232913  0.22298624 0.22263427 0.222338   0.22185956 0.22148646
 0.22125858 0.22108443 0.2207562  0.22028685 0.21985139 0.21968196
 0.21974614 0.21966264 0.21948314 0.21953447 0.2196832  0.21959154
 0.21951416 0.2196111  0.21955617 0.21937455 0.21931806 0.21941702
 0.21926889 0.2189526  0.2186621  0.21792562 0.21716413 0.21687073
 0.21684892 0.21662565 0.21642831 0.21638505 0.21625835 0.2159897
 0.2158494  0.21594909 0.21588491 0.21563736 0.2155723  0.2157022
 0.21595529 0.21606241 0.21597715 0.21594654 0.21605359 0.21651411
 0.21710393 0.21771897 0.21843474 0.21894445 0.21891911 0.21877576
 0.21879947 0.21896042 0.218875   0.2187215  0.21857713 0.2184548
 0.21845719 0.21849135 0.21845241 0.21844006 0.2186146  0.21888214
 0.21904959 0.21925852 0.21952459 0.21950099 0.2192276  0.21913433
 0.21905147 0.2186667  0.21813455 0.21775211 0.2174384  0.21703278
 0.21673077 0.21668634 0.21665362 0.21648863 0.21628696 0.21613286
 0.21603805 0.21626592 0.21640107 0.2160755  0.21590352 0.21618783
 0.2162676  0.2158196  0.21542954 0.21520166 0.21465284 0.21434255
 0.21452175 0.21459143 0.21432279 0.21417667 0.214162   0.21392362
 0.21363668 0.21366455 0.21365854 0.21335727 0.21282853 0.21240804
 0.21227305 0.21232162 0.21215574 0.21173388 0.21166676 0.2120404
 0.21201845 0.21140522 0.21112132 0.21130827 0.21140079 0.21154653
 0.21198146 0.21194471 0.2119251  0.21224697 0.21248104 0.2124876
 0.21256158 0.21260944 0.21238254 0.21215868 0.21217057 0.21192166
 0.21144208 0.21140026 0.21182145 0.211807   0.2117615  0.21197993
 0.2121144  0.21186681 0.21168897 0.2117502  0.21172422 0.21154292
 0.21127746 0.21111026 0.21113041 0.2112093  0.21106553 0.21088327
 0.21076782 0.21069261 0.21041825 0.21021728 0.21008348 0.21006516
 0.2101532  0.2101766  0.2099118  0.20979488 0.21000041 0.21014257
 0.20988613 0.20972261 0.20994817 0.21001013 0.2098598  0.21028683
 0.21101469 0.21139352 0.21178907 0.21213058 0.21220684 0.21241823
 0.21272919 0.21282552 0.21266147 0.2124055  0.21229807 0.21223821
 0.21208772 0.21201192 0.21225572 0.2124019  0.21238519 0.21215428
 0.21216701 0.21231183 0.21236436 0.21243413 0.21252994 0.21281694
 0.21325009 0.21361801 0.21380705 0.21374948 0.2135365  0.21310681
 0.2125837  0.21232599 0.21236403 0.21222425 0.21165673 0.21142642
 0.21163487 0.21174805 0.2116374  0.211778   0.21197082 0.21204458
 0.21220486 0.21249267 0.21253923 0.21236286 0.2121924  0.21237254
 0.21273753 0.21278363 0.21241067 0.21204418 0.21196523 0.21190317
 0.21164365 0.21159299 0.21177602 0.21165787 0.21145657 0.21171041
 0.2120334  0.21196088 0.21183103 0.21188928 0.2117378  0.21154734
 0.2116771  0.21195188 0.21209015 0.21214017 0.21174759 0.21118212
 0.2111406  0.21156171 0.21162918 0.2110675  0.2107274  0.21048532
 0.21000217 0.20954725 0.20937967 0.20922491 0.20885646 0.20867091
 0.20865966 0.20836721 0.20802233 0.2079277  0.20795667 0.2080994
 0.20844224 0.20845163 0.20796435 0.20766683 0.20771687 0.20833269
 0.20931411 0.21039371 0.21144758 0.2119097  0.21148194 0.21048877
 0.20999908 0.209764   0.20937341 0.20892955 0.20865373 0.20868452
 0.20871632 0.20869906 0.20871891 0.20864215 0.2086777  0.20919442
 0.20955294 0.20969588 0.20989019 0.21014354 0.21010694 0.2103326
 0.21111117 0.21184951 0.21227723 0.21225381 0.21204562 0.21189025
 0.21172807 0.21147594 0.2113393  0.21114337 0.21081857 0.21057416
 0.21058255 0.21059938 0.21037258 0.21026053 0.21036309 0.21032354
 0.2103583  0.21097048 0.21140224 0.21151532 0.21176755 0.21269321
 0.21320929 0.21299893 0.21280217 0.21293668 0.21297777 0.2126455
 0.21231873 0.21218798 0.21177287 0.21162328 0.21186726 0.21194178
 0.21167636 0.21149358 0.2114795  0.2113767  0.2113181  0.21137132
 0.21128957 0.21127376 0.2115471  0.21188022 0.21221715 0.21300933
 0.21361631 0.21363765 0.21361825 0.21375045 0.21349126 0.2130879
 0.212789   0.2125187  0.21212922 0.21191768 0.2120853  0.21226525
 0.21236867 0.21237499 0.21235585 0.21246965 0.21287784 0.21313083
 0.2130129  0.21298414 0.21322218 0.21306577 0.21285325 0.21306875
 0.21330976 0.21324208 0.21313089 0.21292573 0.2125052  0.2123332
 0.21240987 0.21206515 0.21145535 0.211329   0.21139173 0.21124125
 0.21103628 0.21091212 0.21096498 0.21107927 0.21115357 0.21108896
 0.21101263 0.21113154 0.21142673 0.21169813 0.2121046  0.2129185
 0.21356018 0.21399383 0.21457894 0.21486911 0.21441221 0.21383274
 0.2135617  0.21318655 0.21260041 0.21217495 0.2119401  0.21193111
 0.21210381 0.21224986 0.21219468 0.21234132 0.2129271  0.21335535
 0.21349317 0.21371385 0.21411069 0.21446085 0.21495712 0.2155971
 0.21578443 0.21561229 0.21561182 0.21546856 0.21492906 0.21442641
 0.21413046 0.2138631  0.21359465 0.21350133 0.21326585 0.21298096
 0.21304639 0.21326952 0.21349071 0.21365358 0.2138611  0.21397042
 0.21401614 0.21414016 0.21401879 0.21367967 0.2135262  0.21334916
 0.21275678 0.2122206  0.21227573 0.21206982 0.21102996 0.21008717
 0.20959695 0.20915715 0.20869616 0.20834123 0.20785001 0.2075925
 0.20757417 0.20740901 0.20700476 0.20691687 0.20725948 0.20745134
 0.20733663 0.20726043 0.20747682 0.20768781 0.20776622 0.20779584
 0.20774685 0.20779938 0.20777038 0.20743418 0.20692468 0.20663448
 0.20631807 0.20555077 0.20504925 0.20494294 0.20491475 0.20488204
 0.204913   0.20458144 0.20415436 0.20403565 0.20416929 0.20424661
 0.20432888 0.20438221 0.20438229 0.20443603 0.20442235 0.20433429
 0.20417422 0.20405656 0.20391281 0.20354669 0.2028439  0.20204955
 0.20137481 0.20097439 0.2007941  0.2005051  0.20012729 0.1999028
 0.19988891 0.19952428 0.19920304 0.19917238 0.19920734 0.19916078
 0.19921795 0.19938219 0.1996478  0.19980839 0.19987325 0.20007034
 0.20029452 0.20017454 0.20014417 0.19976102 0.19899869 0.19826707
 0.19792333 0.19776782 0.19735233 0.1969978  0.19686884 0.19685343
 0.19680434 0.19660032 0.19653405 0.1965975  0.1964926  0.1965044
 0.19659738 0.1965301  0.19651172 0.19679232 0.19710863 0.19752817
 0.19764134 0.19730642 0.19712342 0.19707227 0.19651079 0.19593975
 0.1955309  0.19491804 0.1944281  0.19416659 0.19401422 0.1937142
 0.19339281 0.19308048 0.19303651 0.19287097 0.19294359 0.1932611
 0.19335271 0.19316931 0.19307935 0.19301449 0.19287173 0.19301438
 0.19304284 0.19250475 0.19171746 0.19104809 0.19006717 0.18907505
 0.18823794 0.1877415  0.18745036 0.18726894 0.18728761 0.18738122
 0.18755217 0.18756022 0.18743195 0.18719766 0.18733671 0.18763015
 0.18784744 0.18797687 0.18838827 0.18873255 0.18874942 0.18900461
 0.18929453 0.18944372 0.18931405 0.18899097 0.18836758 0.18764897
 0.18715154 0.18696478 0.18691424 0.18657263 0.18599412 0.1855162
 0.18565996 0.18538716 0.18489939 0.18461262 0.18486646 0.18490036
 0.18424542 0.18438302 0.18570614 0.18409793 0.18300064 0.18318555]
