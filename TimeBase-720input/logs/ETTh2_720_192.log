Args in experiment:
Namespace(is_training=1, model_id='ETTh2_720_192', model='LightTimeBaseTST', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=192, period_len=24, basis_num=4, use_period_norm=1, use_orthogonal=1, orthogonal_weight=0.08, fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=7, individual=0, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='learned', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=30, batch_size=512, patience=5, learning_rate=0.06, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=1, use_multi_gpu=0, devices='0,1', test_flop=False)
Use GPU: cuda:1
164
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
MACs: 25536.0
Params: 164.0
25536.0 MACs
>>>>>>>start training : ETTh2_720_192_LightTimeBaseTST_ETTh2_ftM_sl720_pl192_test_0_seed2023>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Max Memory (MB): 51.2119140625
Epoch: 1 cost time: 0.7589728832244873
Epoch: 1, Steps: 16 | Train Loss: 0.8493744 Vali Loss: 0.5667068 Test Loss: 0.5779523
Validation loss decreased (inf --> 0.566707).  Saving model ...
Updating learning rate to 0.06
Max Memory (MB): 51.2119140625
Epoch: 2 cost time: 0.7620639801025391
Epoch: 2, Steps: 16 | Train Loss: 0.7398018 Vali Loss: 0.5158545 Test Loss: 0.5288562
Validation loss decreased (0.566707 --> 0.515855).  Saving model ...
Updating learning rate to 0.06
Max Memory (MB): 51.2119140625
Epoch: 3 cost time: 0.7517621517181396
Epoch: 3, Steps: 16 | Train Loss: 0.8231478 Vali Loss: 0.4885192 Test Loss: 0.4886930
Validation loss decreased (0.515855 --> 0.488519).  Saving model ...
Updating learning rate to 0.06
Max Memory (MB): 51.2119140625
Epoch: 4 cost time: 0.7581758499145508
Epoch: 4, Steps: 16 | Train Loss: 0.6998831 Vali Loss: 0.4398264 Test Loss: 0.4582353
Validation loss decreased (0.488519 --> 0.439826).  Saving model ...
Updating learning rate to 0.048
Max Memory (MB): 51.2119140625
Epoch: 5 cost time: 0.7736155986785889
Epoch: 5, Steps: 16 | Train Loss: 0.6446855 Vali Loss: 0.3241479 Test Loss: 0.4297716
Validation loss decreased (0.439826 --> 0.324148).  Saving model ...
Updating learning rate to 0.038400000000000004
Max Memory (MB): 51.2119140625
Epoch: 6 cost time: 0.8311355113983154
Epoch: 6, Steps: 16 | Train Loss: 0.5723311 Vali Loss: 0.3262571 Test Loss: 0.4158791
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.030720000000000008
Max Memory (MB): 51.2119140625
Epoch: 7 cost time: 0.7716162204742432
Epoch: 7, Steps: 16 | Train Loss: 0.5562267 Vali Loss: 0.3008710 Test Loss: 0.4094414
Validation loss decreased (0.324148 --> 0.300871).  Saving model ...
Updating learning rate to 0.024576000000000004
Max Memory (MB): 51.2119140625
Epoch: 8 cost time: 0.8585801124572754
Epoch: 8, Steps: 16 | Train Loss: 0.5506225 Vali Loss: 0.2957183 Test Loss: 0.4068793
Validation loss decreased (0.300871 --> 0.295718).  Saving model ...
Updating learning rate to 0.019660800000000003
Max Memory (MB): 51.2119140625
Epoch: 9 cost time: 0.7379970550537109
Epoch: 9, Steps: 16 | Train Loss: 0.5499905 Vali Loss: 0.2895115 Test Loss: 0.4065125
Validation loss decreased (0.295718 --> 0.289512).  Saving model ...
Updating learning rate to 0.015728640000000006
Max Memory (MB): 51.2119140625
Epoch: 10 cost time: 0.7925148010253906
Epoch: 10, Steps: 16 | Train Loss: 0.5487051 Vali Loss: 0.2935770 Test Loss: 0.4073201
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.012582912000000003
Max Memory (MB): 51.2119140625
Epoch: 11 cost time: 0.7909896373748779
Epoch: 11, Steps: 16 | Train Loss: 0.5374035 Vali Loss: 0.2902668 Test Loss: 0.4098340
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.010066329600000005
Max Memory (MB): 51.2119140625
Epoch: 12 cost time: 0.7943525314331055
Epoch: 12, Steps: 16 | Train Loss: 0.5515790 Vali Loss: 0.2951581 Test Loss: 0.4051450
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.008053063680000003
Max Memory (MB): 51.2119140625
Epoch: 13 cost time: 0.7454464435577393
Epoch: 13, Steps: 16 | Train Loss: 0.5437248 Vali Loss: 0.2956141 Test Loss: 0.4074141
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.006442450944000003
Max Memory (MB): 51.2119140625
Epoch: 14 cost time: 0.7682654857635498
Epoch: 14, Steps: 16 | Train Loss: 0.5485328 Vali Loss: 0.2907492 Test Loss: 0.4087839
EarlyStopping counter: 5 out of 5
Early stopping
Final Max Memory (MB): 51.2119140625
>>>>>>>testing : ETTh2_720_192_LightTimeBaseTST_ETTh2_ftM_sl720_pl192_test_0_seed2023<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3413870141506195, mae:0.3870078921318054, rse:0.4695873260498047
