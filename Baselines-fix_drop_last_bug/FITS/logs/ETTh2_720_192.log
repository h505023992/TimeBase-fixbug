Args in experiment:
Namespace(is_training=1, model_id='ETTh2_720_192', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=196, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0091400146484375
Epoch: 1, Steps: 60 | Train Loss: 0.5995916 Vali Loss: 0.4704251 Test Loss: 0.4498280
Validation loss decreased (inf --> 0.470425).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.9607253074645996
Epoch: 2, Steps: 60 | Train Loss: 0.4620670 Vali Loss: 0.4151778 Test Loss: 0.4142804
Validation loss decreased (0.470425 --> 0.415178).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.9856369495391846
Epoch: 3, Steps: 60 | Train Loss: 0.3986214 Vali Loss: 0.3907103 Test Loss: 0.3998318
Validation loss decreased (0.415178 --> 0.390710).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.08009672164917
Epoch: 4, Steps: 60 | Train Loss: 0.3596725 Vali Loss: 0.3783205 Test Loss: 0.3937613
Validation loss decreased (0.390710 --> 0.378321).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2056686878204346
Epoch: 5, Steps: 60 | Train Loss: 0.3310795 Vali Loss: 0.3703570 Test Loss: 0.3901604
Validation loss decreased (0.378321 --> 0.370357).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.31656813621521
Epoch: 6, Steps: 60 | Train Loss: 0.3105102 Vali Loss: 0.3649830 Test Loss: 0.3880320
Validation loss decreased (0.370357 --> 0.364983).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.494385242462158
Epoch: 7, Steps: 60 | Train Loss: 0.2928645 Vali Loss: 0.3605082 Test Loss: 0.3865610
Validation loss decreased (0.364983 --> 0.360508).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.1253535747528076
Epoch: 8, Steps: 60 | Train Loss: 0.2778319 Vali Loss: 0.3572001 Test Loss: 0.3851535
Validation loss decreased (0.360508 --> 0.357200).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.195580005645752
Epoch: 9, Steps: 60 | Train Loss: 0.2646111 Vali Loss: 0.3536358 Test Loss: 0.3840338
Validation loss decreased (0.357200 --> 0.353636).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.09352970123291
Epoch: 10, Steps: 60 | Train Loss: 0.2530922 Vali Loss: 0.3505541 Test Loss: 0.3830140
Validation loss decreased (0.353636 --> 0.350554).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.170797109603882
Epoch: 11, Steps: 60 | Train Loss: 0.2438247 Vali Loss: 0.3480225 Test Loss: 0.3817592
Validation loss decreased (0.350554 --> 0.348022).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.266105890274048
Epoch: 12, Steps: 60 | Train Loss: 0.2355961 Vali Loss: 0.3455434 Test Loss: 0.3806978
Validation loss decreased (0.348022 --> 0.345543).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.3540096282958984
Epoch: 13, Steps: 60 | Train Loss: 0.2272690 Vali Loss: 0.3432052 Test Loss: 0.3796998
Validation loss decreased (0.345543 --> 0.343205).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.371035575866699
Epoch: 14, Steps: 60 | Train Loss: 0.2206792 Vali Loss: 0.3410143 Test Loss: 0.3787127
Validation loss decreased (0.343205 --> 0.341014).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.8781402111053467
Epoch: 15, Steps: 60 | Train Loss: 0.2147713 Vali Loss: 0.3390661 Test Loss: 0.3777937
Validation loss decreased (0.341014 --> 0.339066).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.939934730529785
Epoch: 16, Steps: 60 | Train Loss: 0.2089776 Vali Loss: 0.3369007 Test Loss: 0.3769289
Validation loss decreased (0.339066 --> 0.336901).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.1057121753692627
Epoch: 17, Steps: 60 | Train Loss: 0.2041735 Vali Loss: 0.3355025 Test Loss: 0.3762019
Validation loss decreased (0.336901 --> 0.335502).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.1469099521636963
Epoch: 18, Steps: 60 | Train Loss: 0.1992127 Vali Loss: 0.3335991 Test Loss: 0.3754188
Validation loss decreased (0.335502 --> 0.333599).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.5494766235351562
Epoch: 19, Steps: 60 | Train Loss: 0.1953138 Vali Loss: 0.3321165 Test Loss: 0.3746575
Validation loss decreased (0.333599 --> 0.332116).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.611302614212036
Epoch: 20, Steps: 60 | Train Loss: 0.1912171 Vali Loss: 0.3305752 Test Loss: 0.3740877
Validation loss decreased (0.332116 --> 0.330575).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.4648396968841553
Epoch: 21, Steps: 60 | Train Loss: 0.1883603 Vali Loss: 0.3291174 Test Loss: 0.3733498
Validation loss decreased (0.330575 --> 0.329117).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.5493698120117188
Epoch: 22, Steps: 60 | Train Loss: 0.1850351 Vali Loss: 0.3281723 Test Loss: 0.3726572
Validation loss decreased (0.329117 --> 0.328172).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.118548154830933
Epoch: 23, Steps: 60 | Train Loss: 0.1819336 Vali Loss: 0.3268855 Test Loss: 0.3721738
Validation loss decreased (0.328172 --> 0.326885).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.6597511768341064
Epoch: 24, Steps: 60 | Train Loss: 0.1794018 Vali Loss: 0.3257664 Test Loss: 0.3716421
Validation loss decreased (0.326885 --> 0.325766).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.615601062774658
Epoch: 25, Steps: 60 | Train Loss: 0.1765818 Vali Loss: 0.3247204 Test Loss: 0.3711497
Validation loss decreased (0.325766 --> 0.324720).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.054705619812012
Epoch: 26, Steps: 60 | Train Loss: 0.1745733 Vali Loss: 0.3236084 Test Loss: 0.3707810
Validation loss decreased (0.324720 --> 0.323608).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.9049923419952393
Epoch: 27, Steps: 60 | Train Loss: 0.1732188 Vali Loss: 0.3228443 Test Loss: 0.3702106
Validation loss decreased (0.323608 --> 0.322844).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.5552818775177
Epoch: 28, Steps: 60 | Train Loss: 0.1711231 Vali Loss: 0.3218451 Test Loss: 0.3698303
Validation loss decreased (0.322844 --> 0.321845).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.6097874641418457
Epoch: 29, Steps: 60 | Train Loss: 0.1693480 Vali Loss: 0.3211590 Test Loss: 0.3695478
Validation loss decreased (0.321845 --> 0.321159).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.578002691268921
Epoch: 30, Steps: 60 | Train Loss: 0.1674408 Vali Loss: 0.3203088 Test Loss: 0.3690558
Validation loss decreased (0.321159 --> 0.320309).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.435220718383789
Epoch: 1, Steps: 60 | Train Loss: 0.5401196 Vali Loss: 0.2969059 Test Loss: 0.3564756
Validation loss decreased (inf --> 0.296906).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.590705156326294
Epoch: 2, Steps: 60 | Train Loss: 0.5218168 Vali Loss: 0.2895664 Test Loss: 0.3535278
Validation loss decreased (0.296906 --> 0.289566).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.4267237186431885
Epoch: 3, Steps: 60 | Train Loss: 0.5174475 Vali Loss: 0.2874797 Test Loss: 0.3525875
Validation loss decreased (0.289566 --> 0.287480).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.1826138496398926
Epoch: 4, Steps: 60 | Train Loss: 0.5140360 Vali Loss: 0.2856890 Test Loss: 0.3524866
Validation loss decreased (0.287480 --> 0.285689).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.0339314937591553
Epoch: 5, Steps: 60 | Train Loss: 0.5123316 Vali Loss: 0.2848023 Test Loss: 0.3525364
Validation loss decreased (0.285689 --> 0.284802).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.074702024459839
Epoch: 6, Steps: 60 | Train Loss: 0.5116900 Vali Loss: 0.2835992 Test Loss: 0.3524268
Validation loss decreased (0.284802 --> 0.283599).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.825798988342285
Epoch: 7, Steps: 60 | Train Loss: 0.5111155 Vali Loss: 0.2832574 Test Loss: 0.3521312
Validation loss decreased (0.283599 --> 0.283257).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.9429068565368652
Epoch: 8, Steps: 60 | Train Loss: 0.5097599 Vali Loss: 0.2828805 Test Loss: 0.3522117
Validation loss decreased (0.283257 --> 0.282880).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9632797241210938
Epoch: 9, Steps: 60 | Train Loss: 0.5086883 Vali Loss: 0.2828496 Test Loss: 0.3519625
Validation loss decreased (0.282880 --> 0.282850).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.9720747470855713
Epoch: 10, Steps: 60 | Train Loss: 0.5091988 Vali Loss: 0.2825655 Test Loss: 0.3519427
Validation loss decreased (0.282850 --> 0.282566).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.977649211883545
Epoch: 11, Steps: 60 | Train Loss: 0.5085231 Vali Loss: 0.2819859 Test Loss: 0.3519361
Validation loss decreased (0.282566 --> 0.281986).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.9309511184692383
Epoch: 12, Steps: 60 | Train Loss: 0.5095982 Vali Loss: 0.2816472 Test Loss: 0.3519275
Validation loss decreased (0.281986 --> 0.281647).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.7811946868896484
Epoch: 13, Steps: 60 | Train Loss: 0.5098856 Vali Loss: 0.2815641 Test Loss: 0.3517812
Validation loss decreased (0.281647 --> 0.281564).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.9075253009796143
Epoch: 14, Steps: 60 | Train Loss: 0.5066204 Vali Loss: 0.2811090 Test Loss: 0.3516441
Validation loss decreased (0.281564 --> 0.281109).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.7633585929870605
Epoch: 15, Steps: 60 | Train Loss: 0.5077464 Vali Loss: 0.2807856 Test Loss: 0.3516841
Validation loss decreased (0.281109 --> 0.280786).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.411971569061279
Epoch: 16, Steps: 60 | Train Loss: 0.5064947 Vali Loss: 0.2809468 Test Loss: 0.3514982
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.789231538772583
Epoch: 17, Steps: 60 | Train Loss: 0.5082329 Vali Loss: 0.2808005 Test Loss: 0.3514576
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.722231864929199
Epoch: 18, Steps: 60 | Train Loss: 0.5082970 Vali Loss: 0.2807125 Test Loss: 0.3514012
Validation loss decreased (0.280786 --> 0.280713).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.8150646686553955
Epoch: 19, Steps: 60 | Train Loss: 0.5089800 Vali Loss: 0.2806511 Test Loss: 0.3513649
Validation loss decreased (0.280713 --> 0.280651).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.6644749641418457
Epoch: 20, Steps: 60 | Train Loss: 0.5084295 Vali Loss: 0.2803730 Test Loss: 0.3514093
Validation loss decreased (0.280651 --> 0.280373).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.198840618133545
Epoch: 21, Steps: 60 | Train Loss: 0.5077632 Vali Loss: 0.2803790 Test Loss: 0.3515115
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.2678372859954834
Epoch: 22, Steps: 60 | Train Loss: 0.5072557 Vali Loss: 0.2803582 Test Loss: 0.3513719
Validation loss decreased (0.280373 --> 0.280358).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.430714845657349
Epoch: 23, Steps: 60 | Train Loss: 0.5064013 Vali Loss: 0.2801509 Test Loss: 0.3513605
Validation loss decreased (0.280358 --> 0.280151).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.2615573406219482
Epoch: 24, Steps: 60 | Train Loss: 0.5062397 Vali Loss: 0.2800721 Test Loss: 0.3512757
Validation loss decreased (0.280151 --> 0.280072).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.5714337825775146
Epoch: 25, Steps: 60 | Train Loss: 0.5057363 Vali Loss: 0.2799464 Test Loss: 0.3513249
Validation loss decreased (0.280072 --> 0.279946).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.471457481384277
Epoch: 26, Steps: 60 | Train Loss: 0.5059465 Vali Loss: 0.2801520 Test Loss: 0.3512702
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.5400021076202393
Epoch: 27, Steps: 60 | Train Loss: 0.5076097 Vali Loss: 0.2799541 Test Loss: 0.3513010
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.3715946674346924
Epoch: 28, Steps: 60 | Train Loss: 0.5053974 Vali Loss: 0.2797550 Test Loss: 0.3513010
Validation loss decreased (0.279946 --> 0.279755).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.5815513134002686
Epoch: 29, Steps: 60 | Train Loss: 0.5067442 Vali Loss: 0.2798037 Test Loss: 0.3513272
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.080806016921997
Epoch: 30, Steps: 60 | Train Loss: 0.5069703 Vali Loss: 0.2798984 Test Loss: 0.3512011
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00011296777049628277
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3317214250564575, mae:0.3744456171989441, rse:0.46187832951545715, corr:[0.2659288  0.2682429  0.2671968  0.26772368 0.26729718 0.2661651
 0.26588452 0.26563564 0.26469058 0.2634194  0.26234618 0.26103005
 0.25952762 0.2582091  0.25744596 0.2572772  0.25714985 0.25674927
 0.25587615 0.25464755 0.25332668 0.25214487 0.25092927 0.24920824
 0.24714726 0.2452929  0.24375701 0.24208261 0.24058346 0.23957983
 0.23865917 0.23708495 0.23539245 0.23431903 0.233343   0.23213302
 0.23102565 0.23013446 0.22901416 0.22794458 0.22756441 0.22734924
 0.22653812 0.22532807 0.22429116 0.22323468 0.22203213 0.22059792
 0.21887392 0.21694021 0.21524553 0.21385983 0.212616   0.21075214
 0.20850587 0.2068816  0.20510466 0.20274206 0.20120189 0.20064195
 0.20016113 0.1994042  0.19916919 0.19920869 0.19862607 0.19817686
 0.19769491 0.19675837 0.19599499 0.195891   0.19562544 0.19443972
 0.19282083 0.19192465 0.19137813 0.19001395 0.18865235 0.18795456
 0.1875662  0.18682276 0.1865352  0.18605657 0.18531278 0.18508394
 0.1851736  0.18491165 0.18429507 0.18423918 0.18423069 0.18374296
 0.18290994 0.18222082 0.18203075 0.18158767 0.18148232 0.18184146
 0.18125184 0.17939015 0.17795664 0.17750987 0.17686403 0.17550832
 0.17475349 0.17417057 0.1731167  0.17191915 0.17181171 0.17248997
 0.17179109 0.17045505 0.16966084 0.16964677 0.16927439 0.16910212
 0.16886815 0.16800752 0.16713439 0.1664555  0.1657509  0.16413797
 0.16242479 0.161087   0.1601042  0.15887894 0.157554   0.15647599
 0.15572831 0.1551222  0.15474176 0.15361306 0.15195793 0.15088968
 0.1509191  0.1505429  0.14949776 0.14926955 0.14953378 0.14901307
 0.14788102 0.14742693 0.14752719 0.14697167 0.14613076 0.1450328
 0.14289443 0.14070684 0.13971892 0.13915427 0.13772458 0.13652413
 0.13675795 0.13622183 0.13485575 0.13408932 0.13408095 0.13332236
 0.13273874 0.13349305 0.13418762 0.13317509 0.13152453 0.13233697
 0.1337007  0.13298821 0.13183856 0.13206439 0.13286859 0.13248466
 0.13174556 0.13042088 0.1290298  0.12821642 0.1283367  0.126902
 0.12474518 0.1235968  0.12289882 0.12125272 0.11940254 0.12004791
 0.12034682 0.11929918 0.12014479 0.12215228 0.12080381 0.11929291
 0.12066208 0.12062453 0.12001662 0.12275881 0.12369653 0.11809697]
