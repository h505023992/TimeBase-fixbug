Args in experiment:
Namespace(is_training=1, model_id='traffic_96_720', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=34, base_T=36, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_96_720_FITS_custom_ftM_sl96_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=34, out_features=289, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1084161536.0
params:  10115.0
Trainable parameters:  10115
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 42.45892810821533
Epoch: 1, Steps: 89 | Train Loss: 2.8179276 Vali Loss: 2.5369768 Test Loss: 3.0717139
Validation loss decreased (inf --> 2.536977).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 41.94599390029907
Epoch: 2, Steps: 89 | Train Loss: 1.7543063 Vali Loss: 1.7479789 Test Loss: 2.0921631
Validation loss decreased (2.536977 --> 1.747979).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 40.38588738441467
Epoch: 3, Steps: 89 | Train Loss: 1.2617128 Vali Loss: 1.3879626 Test Loss: 1.6453629
Validation loss decreased (1.747979 --> 1.387963).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 39.046191930770874
Epoch: 4, Steps: 89 | Train Loss: 1.0307876 Vali Loss: 1.2112257 Test Loss: 1.4272377
Validation loss decreased (1.387963 --> 1.211226).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 40.83716034889221
Epoch: 5, Steps: 89 | Train Loss: 0.9124828 Vali Loss: 1.1132317 Test Loss: 1.3083876
Validation loss decreased (1.211226 --> 1.113232).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 43.7458074092865
Epoch: 6, Steps: 89 | Train Loss: 0.8436285 Vali Loss: 1.0518544 Test Loss: 1.2338539
Validation loss decreased (1.113232 --> 1.051854).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 39.24146389961243
Epoch: 7, Steps: 89 | Train Loss: 0.7967689 Vali Loss: 1.0069301 Test Loss: 1.1799463
Validation loss decreased (1.051854 --> 1.006930).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 38.71690034866333
Epoch: 8, Steps: 89 | Train Loss: 0.7607277 Vali Loss: 0.9711516 Test Loss: 1.1367264
Validation loss decreased (1.006930 --> 0.971152).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 38.45398163795471
Epoch: 9, Steps: 89 | Train Loss: 0.7305675 Vali Loss: 0.9393601 Test Loss: 1.0999535
Validation loss decreased (0.971152 --> 0.939360).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 39.68188428878784
Epoch: 10, Steps: 89 | Train Loss: 0.7042101 Vali Loss: 0.9124041 Test Loss: 1.0673229
Validation loss decreased (0.939360 --> 0.912404).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 41.262646198272705
Epoch: 11, Steps: 89 | Train Loss: 0.6808023 Vali Loss: 0.8870293 Test Loss: 1.0384406
Validation loss decreased (0.912404 --> 0.887029).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 38.680413246154785
Epoch: 12, Steps: 89 | Train Loss: 0.6598725 Vali Loss: 0.8651232 Test Loss: 1.0125570
Validation loss decreased (0.887029 --> 0.865123).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 39.58413314819336
Epoch: 13, Steps: 89 | Train Loss: 0.6409027 Vali Loss: 0.8449044 Test Loss: 0.9891486
Validation loss decreased (0.865123 --> 0.844904).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 38.9546537399292
Epoch: 14, Steps: 89 | Train Loss: 0.6237955 Vali Loss: 0.8275349 Test Loss: 0.9678735
Validation loss decreased (0.844904 --> 0.827535).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 40.32955241203308
Epoch: 15, Steps: 89 | Train Loss: 0.6081571 Vali Loss: 0.8106476 Test Loss: 0.9485341
Validation loss decreased (0.827535 --> 0.810648).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 41.72264575958252
Epoch: 16, Steps: 89 | Train Loss: 0.5939070 Vali Loss: 0.7951922 Test Loss: 0.9312350
Validation loss decreased (0.810648 --> 0.795192).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 41.34858298301697
Epoch: 17, Steps: 89 | Train Loss: 0.5809569 Vali Loss: 0.7810569 Test Loss: 0.9152998
Validation loss decreased (0.795192 --> 0.781057).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 40.13811016082764
Epoch: 18, Steps: 89 | Train Loss: 0.5690548 Vali Loss: 0.7695491 Test Loss: 0.9006776
Validation loss decreased (0.781057 --> 0.769549).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 40.16814088821411
Epoch: 19, Steps: 89 | Train Loss: 0.5582435 Vali Loss: 0.7573912 Test Loss: 0.8871852
Validation loss decreased (0.769549 --> 0.757391).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 41.4434711933136
Epoch: 20, Steps: 89 | Train Loss: 0.5481382 Vali Loss: 0.7475741 Test Loss: 0.8750716
Validation loss decreased (0.757391 --> 0.747574).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 42.082695960998535
Epoch: 21, Steps: 89 | Train Loss: 0.5389474 Vali Loss: 0.7374054 Test Loss: 0.8636039
Validation loss decreased (0.747574 --> 0.737405).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 40.81751203536987
Epoch: 22, Steps: 89 | Train Loss: 0.5304222 Vali Loss: 0.7282901 Test Loss: 0.8533388
Validation loss decreased (0.737405 --> 0.728290).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 37.46854114532471
Epoch: 23, Steps: 89 | Train Loss: 0.5225826 Vali Loss: 0.7202678 Test Loss: 0.8437784
Validation loss decreased (0.728290 --> 0.720268).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 38.746535301208496
Epoch: 24, Steps: 89 | Train Loss: 0.5153690 Vali Loss: 0.7123754 Test Loss: 0.8348362
Validation loss decreased (0.720268 --> 0.712375).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 39.783698320388794
Epoch: 25, Steps: 89 | Train Loss: 0.5086704 Vali Loss: 0.7052398 Test Loss: 0.8268263
Validation loss decreased (0.712375 --> 0.705240).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 41.51908493041992
Epoch: 26, Steps: 89 | Train Loss: 0.5023889 Vali Loss: 0.6985006 Test Loss: 0.8191723
Validation loss decreased (0.705240 --> 0.698501).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 38.578638792037964
Epoch: 27, Steps: 89 | Train Loss: 0.4966704 Vali Loss: 0.6922239 Test Loss: 0.8121471
Validation loss decreased (0.698501 --> 0.692224).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 38.43311357498169
Epoch: 28, Steps: 89 | Train Loss: 0.4912709 Vali Loss: 0.6874183 Test Loss: 0.8055909
Validation loss decreased (0.692224 --> 0.687418).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 38.24975252151489
Epoch: 29, Steps: 89 | Train Loss: 0.4862544 Vali Loss: 0.6819466 Test Loss: 0.7996027
Validation loss decreased (0.687418 --> 0.681947).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 39.97733497619629
Epoch: 30, Steps: 89 | Train Loss: 0.4816294 Vali Loss: 0.6768713 Test Loss: 0.7939187
Validation loss decreased (0.681947 --> 0.676871).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11465
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=34, out_features=289, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1084161536.0
params:  10115.0
Trainable parameters:  10115
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 40.00050473213196
Epoch: 1, Steps: 89 | Train Loss: 0.4950167 Vali Loss: 0.6191692 Test Loss: 0.7290892
Validation loss decreased (inf --> 0.619169).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 39.29623627662659
Epoch: 2, Steps: 89 | Train Loss: 0.4548252 Vali Loss: 0.5863000 Test Loss: 0.6924400
Validation loss decreased (0.619169 --> 0.586300).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 38.25757193565369
Epoch: 3, Steps: 89 | Train Loss: 0.4316161 Vali Loss: 0.5678926 Test Loss: 0.6721366
Validation loss decreased (0.586300 --> 0.567893).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 38.69579005241394
Epoch: 4, Steps: 89 | Train Loss: 0.4182086 Vali Loss: 0.5574983 Test Loss: 0.6609282
Validation loss decreased (0.567893 --> 0.557498).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 40.36905026435852
Epoch: 5, Steps: 89 | Train Loss: 0.4105532 Vali Loss: 0.5517157 Test Loss: 0.6549530
Validation loss decreased (0.557498 --> 0.551716).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 41.36165904998779
Epoch: 6, Steps: 89 | Train Loss: 0.4065134 Vali Loss: 0.5485772 Test Loss: 0.6518557
Validation loss decreased (0.551716 --> 0.548577).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 38.16870737075806
Epoch: 7, Steps: 89 | Train Loss: 0.4042007 Vali Loss: 0.5474890 Test Loss: 0.6503127
Validation loss decreased (0.548577 --> 0.547489).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 39.16952729225159
Epoch: 8, Steps: 89 | Train Loss: 0.4030527 Vali Loss: 0.5461692 Test Loss: 0.6498844
Validation loss decreased (0.547489 --> 0.546169).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 39.914814710617065
Epoch: 9, Steps: 89 | Train Loss: 0.4022206 Vali Loss: 0.5452026 Test Loss: 0.6494266
Validation loss decreased (0.546169 --> 0.545203).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 40.56512236595154
Epoch: 10, Steps: 89 | Train Loss: 0.4019759 Vali Loss: 0.5454726 Test Loss: 0.6490742
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 40.8017475605011
Epoch: 11, Steps: 89 | Train Loss: 0.4017575 Vali Loss: 0.5450577 Test Loss: 0.6491384
Validation loss decreased (0.545203 --> 0.545058).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 39.73789715766907
Epoch: 12, Steps: 89 | Train Loss: 0.4016926 Vali Loss: 0.5450239 Test Loss: 0.6489791
Validation loss decreased (0.545058 --> 0.545024).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 38.8648316860199
Epoch: 13, Steps: 89 | Train Loss: 0.4015939 Vali Loss: 0.5451388 Test Loss: 0.6488945
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 39.06350374221802
Epoch: 14, Steps: 89 | Train Loss: 0.4015417 Vali Loss: 0.5454117 Test Loss: 0.6490821
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 40.33138680458069
Epoch: 15, Steps: 89 | Train Loss: 0.4014702 Vali Loss: 0.5443847 Test Loss: 0.6490241
Validation loss decreased (0.545024 --> 0.544385).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 42.33673572540283
Epoch: 16, Steps: 89 | Train Loss: 0.4015479 Vali Loss: 0.5442836 Test Loss: 0.6490371
Validation loss decreased (0.544385 --> 0.544284).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 39.48410367965698
Epoch: 17, Steps: 89 | Train Loss: 0.4014712 Vali Loss: 0.5449576 Test Loss: 0.6490384
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 39.530009508132935
Epoch: 18, Steps: 89 | Train Loss: 0.4014457 Vali Loss: 0.5452241 Test Loss: 0.6488876
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 39.77440047264099
Epoch: 19, Steps: 89 | Train Loss: 0.4014388 Vali Loss: 0.5451570 Test Loss: 0.6490111
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 41.42512822151184
Epoch: 20, Steps: 89 | Train Loss: 0.4014317 Vali Loss: 0.5450288 Test Loss: 0.6489159
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 40.95195508003235
Epoch: 21, Steps: 89 | Train Loss: 0.4014473 Vali Loss: 0.5454323 Test Loss: 0.6489319
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_96_720_FITS_custom_ftM_sl96_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.6477224826812744, mae:0.3862338066101074, rse:0.6581051349639893, corr:[0.25430474 0.26366478 0.26653522 0.2634698  0.26382533 0.2653252
 0.26403952 0.26498166 0.26590735 0.26448056 0.2646326  0.26539147
 0.26426983 0.26412722 0.2647105  0.26416928 0.26412398 0.26460922
 0.2644049  0.2645855  0.26446906 0.26294103 0.26249745 0.2682446
 0.28045058 0.28307846 0.28127328 0.27937576 0.27833495 0.27807137
 0.27756897 0.2764535  0.27589703 0.2764128  0.27630982 0.2759436
 0.27632117 0.27643242 0.27631375 0.27654818 0.27650654 0.2764871
 0.27713513 0.27746648 0.27766255 0.27894947 0.28063923 0.28015357
 0.2712796  0.26552343 0.26471674 0.2653957  0.26601788 0.26568574
 0.2653724  0.26508063 0.26417854 0.26367527 0.26412335 0.26433685
 0.2644224  0.26502112 0.2648999  0.26416782 0.26404816 0.2643594
 0.26463157 0.26537055 0.26677638 0.26883292 0.27067494 0.26849055
 0.26157397 0.26006863 0.26152146 0.26197216 0.26173848 0.26187435
 0.26202795 0.26187274 0.26186073 0.26194102 0.26188925 0.26205516
 0.2622944  0.26212415 0.2618361  0.26181704 0.26184794 0.26180857
 0.26214412 0.26198933 0.2615797  0.2617848  0.26241276 0.26262146
 0.26193625 0.2621427  0.26245403 0.2625356  0.2625799  0.26254314
 0.26256284 0.2624814  0.26225382 0.26232204 0.26255217 0.26245716
 0.2622666  0.2624038  0.26248485 0.26228637 0.26213253 0.2620472
 0.26211706 0.2622396  0.2624419  0.26264343 0.26277083 0.26264164
 0.26245654 0.26246062 0.2626186  0.26254436 0.26237157 0.26252133
 0.26268774 0.26256087 0.26239663 0.26225057 0.26198795 0.2619772
 0.26213902 0.2621613  0.26219782 0.2623533  0.26237184 0.26187822
 0.26207635 0.262275   0.2623083  0.26233596 0.26205307 0.2618234
 0.26292786 0.26327565 0.26304325 0.26289532 0.26265985 0.26248577
 0.26260272 0.26266322 0.26234967 0.26234028 0.26247883 0.26237935
 0.26221105 0.26225272 0.2623365  0.2624705  0.26282147 0.26293325
 0.2627906  0.26283562 0.26287788 0.26218364 0.26110798 0.26227692
 0.2680993  0.27022472 0.2691514  0.26791352 0.26679182 0.2661222
 0.2657796  0.2653326  0.26501206 0.26513287 0.26503098 0.2649603
 0.26516035 0.265233   0.2654152  0.26602417 0.26623645 0.2658475
 0.26581138 0.26611605 0.26587355 0.2651155  0.26505762 0.27004817
 0.2799145  0.2812372  0.27922717 0.27768087 0.27682623 0.27614796
 0.2757242  0.2751665  0.27448025 0.27486077 0.2755729  0.27544042
 0.2753847  0.27585152 0.27603754 0.2759289  0.27612138 0.27664864
 0.27735525 0.2776148  0.27752793 0.27861798 0.2806929  0.27981293
 0.27046648 0.2649462  0.26436496 0.26453695 0.26495385 0.26504847
 0.26478764 0.26432914 0.2636492  0.26304665 0.26281404 0.26272175
 0.26288354 0.26333535 0.26377356 0.2639874  0.26393273 0.26394358
 0.26427606 0.2647155  0.2657714  0.2678055  0.2700073  0.2684942
 0.2628787  0.26139757 0.2623694  0.26323938 0.2635985  0.2636034
 0.26353493 0.26357248 0.2637035  0.2637371  0.26349026 0.26314992
 0.2629938  0.26310652 0.26328912 0.26346168 0.2635295  0.26338843
 0.2633524  0.26346302 0.26346955 0.26341507 0.26368657 0.26392153
 0.2631624  0.263278   0.26392284 0.26425418 0.26420537 0.26410273
 0.26395428 0.2637453  0.2636974  0.26384953 0.26378956 0.26363498
 0.26367894 0.26374015 0.2637406  0.26390797 0.2641208  0.26399016
 0.26369122 0.26341963 0.26346034 0.26361138 0.26354012 0.26347423
 0.26381096 0.26417443 0.2640099  0.2637254  0.26369125 0.26368597
 0.2635637  0.26341057 0.26328003 0.26329744 0.26346445 0.26359972
 0.26351523 0.26337734 0.26340702 0.2635688  0.2636786  0.2635203
 0.26373947 0.26377437 0.26352936 0.26341468 0.2634205  0.2634649
 0.26429895 0.26438323 0.26428664 0.264334   0.2643492  0.26435855
 0.26415306 0.26383996 0.26370633 0.26361835 0.2633218  0.26316756
 0.2632455  0.26346967 0.26386142 0.26415154 0.26418316 0.26402915
 0.26415014 0.2641866  0.2638741  0.2630565  0.26215312 0.26351517
 0.2688557  0.2709022  0.26947218 0.26790398 0.2667508  0.26591447
 0.26548836 0.26533392 0.26507777 0.26497248 0.2650092  0.26500392
 0.26510277 0.26530537 0.26518217 0.26490366 0.26495093 0.2651251
 0.2652703  0.2654771  0.26527852 0.26445425 0.26432753 0.26962668
 0.2800171  0.28227636 0.28043413 0.2788369  0.2779528  0.2774431
 0.27708438 0.27656963 0.2760558  0.27607507 0.27610728 0.27602053
 0.2763205  0.27654958 0.27634335 0.27645192 0.27688897 0.27718592
 0.27772167 0.27807352 0.27798018 0.278798   0.28053114 0.27918726
 0.27032343 0.2651039  0.26471606 0.26496178 0.26512164 0.26498407
 0.2647856  0.2645222  0.26396084 0.26361567 0.2638712  0.2641216
 0.2641369  0.26426157 0.2643493  0.26451308 0.2649213  0.26521465
 0.26546702 0.2660049  0.26687485 0.2682862  0.26976717 0.26811358
 0.26254836 0.260944   0.26236227 0.26341286 0.26347446 0.26347286
 0.2636456  0.26352417 0.2634084  0.26357663 0.26379544 0.26393276
 0.26396725 0.2638334  0.26375362 0.26396158 0.2641007  0.26397797
 0.26406088 0.26411664 0.2640515  0.26418132 0.2644255  0.26441374
 0.26371112 0.2639013  0.2643226  0.2644229  0.26440334 0.26424143
 0.26396272 0.26377553 0.26380366 0.26393193 0.26406103 0.26408693
 0.26403806 0.26401868 0.26407903 0.2643115  0.264584   0.26450205
 0.2643155  0.26431155 0.2644713  0.26443478 0.264348   0.264534
 0.2646918  0.26460674 0.26445928 0.26432547 0.2640764  0.2638646
 0.2637782  0.26378307 0.2638745  0.26380986 0.2635664  0.26355335
 0.26381823 0.2638776  0.26372427 0.26381952 0.26404917 0.26396963
 0.26382712 0.26396132 0.26429433 0.26433665 0.26392648 0.26387322
 0.26494268 0.26528332 0.2650669  0.26503304 0.26493976 0.26463014
 0.2645504  0.26476493 0.2646643  0.26431438 0.26425827 0.26430547
 0.2641865  0.26420885 0.264296   0.2641974  0.2641869  0.26443005
 0.26466328 0.26480615 0.26464522 0.26361305 0.2624357  0.26396212
 0.26935315 0.2712008  0.2697838  0.26836735 0.267092   0.26616213
 0.26559946 0.26519752 0.26499936 0.26497924 0.26461354 0.26437318
 0.26460758 0.26467672 0.26446787 0.264491   0.26470757 0.26487276
 0.26506075 0.2651651  0.26472145 0.26394433 0.2643012  0.26983798
 0.27936164 0.28090447 0.27899197 0.27793065 0.2774173  0.27656314
 0.27589306 0.27562714 0.27518937 0.27505502 0.27513093 0.27504304
 0.27522007 0.275717   0.27584043 0.27584586 0.27625778 0.2765615
 0.27670193 0.2768341  0.27687055 0.27770218 0.2791383  0.2771904
 0.26837176 0.2634305  0.26304084 0.26341578 0.263888   0.26398993
 0.26374862 0.26331112 0.26271057 0.26230177 0.2623318  0.26263237
 0.26292437 0.2628965  0.2627169  0.26270878 0.26259285 0.26252812
 0.26304707 0.26356193 0.26397824 0.26537582 0.2672589  0.26591286
 0.26167896 0.26128277 0.26259837 0.26341763 0.2637108  0.2636081
 0.2633328  0.26318452 0.2631256  0.26289007 0.2627456  0.2628405
 0.262807   0.26262018 0.2625261  0.2626149  0.2627769  0.26282126
 0.26277152 0.2627979  0.26275015 0.2626212  0.26291844 0.26331723
 0.26267603 0.26271135 0.26335126 0.26368406 0.26348636 0.2632129
 0.26290545 0.2625323  0.262784   0.2632663  0.26303425 0.26273364
 0.26306403 0.2630595  0.26250792 0.26250803 0.26288173 0.26287824
 0.2628865  0.2630576  0.26289776 0.26254848 0.26249278 0.26256382
 0.26269567 0.26297152 0.26309362 0.26305526 0.26295605 0.2626338
 0.26222235 0.26208553 0.26200256 0.26169285 0.26161358 0.26200858
 0.26238638 0.26239097 0.26220408 0.26222417 0.26254204 0.26276016
 0.2626637  0.2626868  0.2627553  0.26227427 0.2616061  0.26183
 0.26314673 0.26337248 0.26326376 0.26321205 0.26290423 0.26296544
 0.26309374 0.2627224  0.26258126 0.26278955 0.26253968 0.2623162
 0.26258776 0.26249304 0.26208153 0.26237595 0.26299992 0.2631125
 0.2631555  0.26343617 0.2631819  0.26207468 0.2608504  0.26188567
 0.2672952  0.27009115 0.26900852 0.26729748 0.26626033 0.26533797
 0.26469737 0.2648098  0.26452643 0.2638064  0.26365694 0.2636064
 0.2633648  0.26376858 0.26432347 0.26427975 0.26461887 0.26538724
 0.26519236 0.26464558 0.26470944 0.26427132 0.2638253  0.2689506
 0.27823803 0.27927923 0.2775842  0.2763874  0.27526078 0.27515507
 0.2751305  0.2739298  0.2737655  0.2742598  0.27356648 0.27423626
 0.27585688 0.27568167 0.2762242  0.27755266 0.2766986  0.27626935
 0.2775221  0.27533224 0.27314714 0.27602705 0.27293634 0.27622458]
