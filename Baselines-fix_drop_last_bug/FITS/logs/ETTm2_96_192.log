Args in experiment:
Namespace(is_training=1, model_id='ETTm2_96_192', model='FITS', data='ETTm2', root_path='./dataset/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=40, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_96_192_FITS_ETTm2_ftM_sl96_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34273
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=40, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4300800.0
params:  4920.0
Trainable parameters:  4920
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4125555
	speed: 0.0457s/iter; left time: 361.5527s
	iters: 200, epoch: 1 | loss: 0.2369011
	speed: 0.0408s/iter; left time: 318.7675s
Epoch: 1 cost time: 11.099639415740967
Epoch: 1, Steps: 267 | Train Loss: 0.3182684 Vali Loss: 0.2027345 Test Loss: 0.2820475
Validation loss decreased (inf --> 0.202735).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2762601
	speed: 0.1869s/iter; left time: 1428.8867s
	iters: 200, epoch: 2 | loss: 0.1978960
	speed: 0.0385s/iter; left time: 290.5572s
Epoch: 2 cost time: 11.22409701347351
Epoch: 2, Steps: 267 | Train Loss: 0.2531195 Vali Loss: 0.1870965 Test Loss: 0.2636055
Validation loss decreased (0.202735 --> 0.187096).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2075885
	speed: 0.2017s/iter; left time: 1488.2663s
	iters: 200, epoch: 3 | loss: 0.2541361
	speed: 0.0414s/iter; left time: 301.6065s
Epoch: 3 cost time: 11.70524549484253
Epoch: 3, Steps: 267 | Train Loss: 0.2391488 Vali Loss: 0.1809371 Test Loss: 0.2565531
Validation loss decreased (0.187096 --> 0.180937).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3923181
	speed: 0.1891s/iter; left time: 1344.2045s
	iters: 200, epoch: 4 | loss: 0.1979963
	speed: 0.0387s/iter; left time: 271.5193s
Epoch: 4 cost time: 11.2850821018219
Epoch: 4, Steps: 267 | Train Loss: 0.2330125 Vali Loss: 0.1773904 Test Loss: 0.2525272
Validation loss decreased (0.180937 --> 0.177390).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3075570
	speed: 0.2094s/iter; left time: 1432.8795s
	iters: 200, epoch: 5 | loss: 0.1783569
	speed: 0.0457s/iter; left time: 307.9874s
Epoch: 5 cost time: 12.658554553985596
Epoch: 5, Steps: 267 | Train Loss: 0.2292050 Vali Loss: 0.1750620 Test Loss: 0.2499105
Validation loss decreased (0.177390 --> 0.175062).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2494466
	speed: 0.1998s/iter; left time: 1313.6680s
	iters: 200, epoch: 6 | loss: 0.2386996
	speed: 0.0412s/iter; left time: 266.6694s
Epoch: 6 cost time: 11.897551774978638
Epoch: 6, Steps: 267 | Train Loss: 0.2273727 Vali Loss: 0.1735275 Test Loss: 0.2483411
Validation loss decreased (0.175062 --> 0.173527).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2220052
	speed: 0.2167s/iter; left time: 1367.1170s
	iters: 200, epoch: 7 | loss: 0.1657536
	speed: 0.0463s/iter; left time: 287.3578s
Epoch: 7 cost time: 12.96018648147583
Epoch: 7, Steps: 267 | Train Loss: 0.2260255 Vali Loss: 0.1726391 Test Loss: 0.2473012
Validation loss decreased (0.173527 --> 0.172639).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1322999
	speed: 0.2115s/iter; left time: 1278.0299s
	iters: 200, epoch: 8 | loss: 0.2137011
	speed: 0.0443s/iter; left time: 263.3136s
Epoch: 8 cost time: 12.332226991653442
Epoch: 8, Steps: 267 | Train Loss: 0.2254159 Vali Loss: 0.1720091 Test Loss: 0.2466990
Validation loss decreased (0.172639 --> 0.172009).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2715035
	speed: 0.2236s/iter; left time: 1291.5427s
	iters: 200, epoch: 9 | loss: 0.2586631
	speed: 0.0512s/iter; left time: 290.7180s
Epoch: 9 cost time: 14.179453372955322
Epoch: 9, Steps: 267 | Train Loss: 0.2245991 Vali Loss: 0.1717297 Test Loss: 0.2463215
Validation loss decreased (0.172009 --> 0.171730).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3074893
	speed: 0.2395s/iter; left time: 1318.9261s
	iters: 200, epoch: 10 | loss: 0.1983037
	speed: 0.0513s/iter; left time: 277.1918s
Epoch: 10 cost time: 15.065604448318481
Epoch: 10, Steps: 267 | Train Loss: 0.2246275 Vali Loss: 0.1714046 Test Loss: 0.2461580
Validation loss decreased (0.171730 --> 0.171405).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1669389
	speed: 0.2615s/iter; left time: 1370.4751s
	iters: 200, epoch: 11 | loss: 0.2762387
	speed: 0.0611s/iter; left time: 314.0047s
Epoch: 11 cost time: 16.615572452545166
Epoch: 11, Steps: 267 | Train Loss: 0.2242927 Vali Loss: 0.1713998 Test Loss: 0.2459659
Validation loss decreased (0.171405 --> 0.171400).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1797722
	speed: 0.2990s/iter; left time: 1487.2967s
	iters: 200, epoch: 12 | loss: 0.3309059
	speed: 0.0617s/iter; left time: 300.7843s
Epoch: 12 cost time: 17.745239734649658
Epoch: 12, Steps: 267 | Train Loss: 0.2245241 Vali Loss: 0.1712908 Test Loss: 0.2459006
Validation loss decreased (0.171400 --> 0.171291).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2282390
	speed: 0.2888s/iter; left time: 1359.4072s
	iters: 200, epoch: 13 | loss: 0.1429087
	speed: 0.0701s/iter; left time: 322.7382s
Epoch: 13 cost time: 18.374160528182983
Epoch: 13, Steps: 267 | Train Loss: 0.2245583 Vali Loss: 0.1713526 Test Loss: 0.2459156
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2137628
	speed: 0.2945s/iter; left time: 1307.4701s
	iters: 200, epoch: 14 | loss: 0.2204904
	speed: 0.0586s/iter; left time: 254.3799s
Epoch: 14 cost time: 17.13553810119629
Epoch: 14, Steps: 267 | Train Loss: 0.2245281 Vali Loss: 0.1711350 Test Loss: 0.2459342
Validation loss decreased (0.171291 --> 0.171135).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2370213
	speed: 0.2943s/iter; left time: 1228.0811s
	iters: 200, epoch: 15 | loss: 0.1309232
	speed: 0.0688s/iter; left time: 280.1568s
Epoch: 15 cost time: 19.06709599494934
Epoch: 15, Steps: 267 | Train Loss: 0.2241950 Vali Loss: 0.1714935 Test Loss: 0.2459147
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2720939
	speed: 0.3102s/iter; left time: 1211.7761s
	iters: 200, epoch: 16 | loss: 0.2229014
	speed: 0.0749s/iter; left time: 285.0004s
Epoch: 16 cost time: 20.56416082382202
Epoch: 16, Steps: 267 | Train Loss: 0.2243164 Vali Loss: 0.1714753 Test Loss: 0.2459933
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3744006
	speed: 0.3823s/iter; left time: 1391.0788s
	iters: 200, epoch: 17 | loss: 0.1741958
	speed: 0.0807s/iter; left time: 285.5520s
Epoch: 17 cost time: 22.59541630744934
Epoch: 17, Steps: 267 | Train Loss: 0.2241791 Vali Loss: 0.1712474 Test Loss: 0.2458834
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2061689
	speed: 0.3474s/iter; left time: 1171.3016s
	iters: 200, epoch: 18 | loss: 0.1580166
	speed: 0.0715s/iter; left time: 233.9400s
Epoch: 18 cost time: 20.03137731552124
Epoch: 18, Steps: 267 | Train Loss: 0.2243071 Vali Loss: 0.1713059 Test Loss: 0.2459172
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2464607
	speed: 0.3337s/iter; left time: 1036.1675s
	iters: 200, epoch: 19 | loss: 0.1512394
	speed: 0.0667s/iter; left time: 200.3809s
Epoch: 19 cost time: 18.087594270706177
Epoch: 19, Steps: 267 | Train Loss: 0.2244371 Vali Loss: 0.1714638 Test Loss: 0.2459473
EarlyStopping counter: 5 out of 5
Early stopping
train 34273
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=40, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4300800.0
params:  4920.0
Trainable parameters:  4920
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3193870
	speed: 0.0759s/iter; left time: 600.7633s
	iters: 200, epoch: 1 | loss: 0.2744455
	speed: 0.0691s/iter; left time: 540.0495s
Epoch: 1 cost time: 19.1948344707489
Epoch: 1, Steps: 267 | Train Loss: 0.3345916 Vali Loss: 0.1712009 Test Loss: 0.2457419
Validation loss decreased (inf --> 0.171201).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3607520
	speed: 0.3056s/iter; left time: 2335.7229s
	iters: 200, epoch: 2 | loss: 0.5140773
	speed: 0.0753s/iter; left time: 568.4104s
Epoch: 2 cost time: 19.28937554359436
Epoch: 2, Steps: 267 | Train Loss: 0.3343667 Vali Loss: 0.1714015 Test Loss: 0.2456233
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2324296
	speed: 0.3069s/iter; left time: 2263.8838s
	iters: 200, epoch: 3 | loss: 0.3704489
	speed: 0.0767s/iter; left time: 558.1214s
Epoch: 3 cost time: 20.48951292037964
Epoch: 3, Steps: 267 | Train Loss: 0.3342309 Vali Loss: 0.1714140 Test Loss: 0.2457744
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2696244
	speed: 0.3622s/iter; left time: 2575.5278s
	iters: 200, epoch: 4 | loss: 0.3997386
	speed: 0.0806s/iter; left time: 565.1305s
Epoch: 4 cost time: 22.729873418807983
Epoch: 4, Steps: 267 | Train Loss: 0.3342195 Vali Loss: 0.1714377 Test Loss: 0.2457531
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2641761
	speed: 0.3355s/iter; left time: 2295.5905s
	iters: 200, epoch: 5 | loss: 0.2978305
	speed: 0.0648s/iter; left time: 437.2118s
Epoch: 5 cost time: 19.363101720809937
Epoch: 5, Steps: 267 | Train Loss: 0.3340581 Vali Loss: 0.1716447 Test Loss: 0.2459419
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2664737
	speed: 0.3205s/iter; left time: 2107.7379s
	iters: 200, epoch: 6 | loss: 0.5270883
	speed: 0.0654s/iter; left time: 423.2507s
Epoch: 6 cost time: 18.277595281600952
Epoch: 6, Steps: 267 | Train Loss: 0.3340123 Vali Loss: 0.1715706 Test Loss: 0.2459918
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTm2_96_192_FITS_ETTm2_ftM_sl96_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.24656535685062408, mae:0.3047643303871155, rse:0.40193989872932434, corr:[0.5713178  0.56631917 0.56369364 0.56345135 0.5607752  0.5604318
 0.5581537  0.5572459  0.555863   0.5552765  0.55402493 0.55263776
 0.55227757 0.55140984 0.5512188  0.5503115  0.55008763 0.549298
 0.54787916 0.5471429  0.5454373  0.5447149  0.54387915 0.5429893
 0.5428851  0.54193616 0.5415054  0.54102284 0.54026234 0.53963625
 0.5385434  0.5376924  0.5373618  0.53700674 0.53627586 0.5359427
 0.5354349  0.5347155  0.5340927  0.5332272  0.5324466  0.53155065
 0.5313111  0.53137636 0.5311763  0.5308993  0.5300735  0.5293343
 0.52845913 0.5272764  0.5264155  0.5255817  0.52512807 0.5244485
 0.5235268  0.5232597  0.52261853 0.52219784 0.52196527 0.5215566
 0.5213802  0.52106565 0.52104497 0.5208624  0.5207157  0.52089983
 0.5208062  0.520726   0.5205276  0.520601   0.52074593 0.5206405
 0.5208402  0.52094454 0.520851   0.5208209  0.5207551  0.52079433
 0.5207066  0.5205748  0.5205132  0.52029693 0.5201387  0.52001995
 0.51970917 0.51940763 0.51936215 0.5191711  0.5188288  0.518595
 0.51824933 0.51793015 0.51745933 0.5167939  0.51629436 0.5149473
 0.5132535  0.5119199  0.51028234 0.50857246 0.50712645 0.5057328
 0.50441045 0.5032019  0.50194675 0.50044644 0.4987961  0.4973336
 0.49589786 0.49472055 0.49366856 0.49223015 0.49095404 0.48980707
 0.48839128 0.48702595 0.4856779  0.4843328  0.48319244 0.48228484
 0.48106393 0.47988525 0.47892857 0.47754207 0.47666377 0.4757718
 0.47447455 0.47376776 0.47262022 0.4715528  0.47075304 0.46954256
 0.46875688 0.46771044 0.46660742 0.46634203 0.46567705 0.46524787
 0.46497715 0.46453208 0.46398425 0.4635244  0.46279562 0.46191782
 0.461142   0.46022278 0.45888695 0.45790762 0.45700783 0.45589122
 0.45552588 0.45481846 0.4538636  0.4539854  0.45343527 0.4528666
 0.45264098 0.45201483 0.45194012 0.45170593 0.4516196  0.45181116
 0.4515134  0.45126757 0.45090696 0.4511001  0.45111752 0.4505951
 0.4514008  0.45147696 0.45153162 0.45234478 0.45243618 0.45300394
 0.45334068 0.4534634  0.45371154 0.4536335  0.45416936 0.45438257
 0.45468754 0.4546927  0.45449805 0.45484415 0.4546539  0.45511517
 0.45517355 0.45515496 0.45576838 0.4559219  0.4566132  0.4555965 ]
