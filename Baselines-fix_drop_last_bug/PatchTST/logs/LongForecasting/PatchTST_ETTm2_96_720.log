Args in experiment:
Namespace(random_seed=2021, is_training=1, model_id='96_720', model='PatchTST', data='ETTm2', root_path='./dataset/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=720, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', loss='mse', lradj='TST', pct_start=0.4, use_amp=False, use_gpu=True, gpu=1, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)
Use GPU: cuda:1
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.
MACs: 41201664.0
Params: 1506256.0
41.20M MACs
>>>>>>>start training : 96_720_PatchTST_ETTm2_ftM_sl96_ll48_pl720_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 10801
test 10801
	iters: 100, epoch: 1 | loss: 0.5979890
	speed: 0.0989s/iter; left time: 773.8001s
	iters: 200, epoch: 1 | loss: 0.8437418
	speed: 0.0892s/iter; left time: 688.7719s
Epoch: 1 cost time: 24.156097650527954
Epoch: 1, Steps: 264 | Train Loss: 0.6410446 Vali Loss: 0.3100437 Test Loss: 0.4343968
Validation loss decreased (inf --> 0.310044).  Saving model ...
Updating learning rate to 5.636587467186684e-06
	iters: 100, epoch: 2 | loss: 0.6329985
	speed: 0.3802s/iter; left time: 2873.2951s
	iters: 200, epoch: 2 | loss: 0.5002269
	speed: 0.0919s/iter; left time: 685.5181s
Epoch: 2 cost time: 24.573757886886597
Epoch: 2, Steps: 264 | Train Loss: 0.6153904 Vali Loss: 0.2989296 Test Loss: 0.4197997
Validation loss decreased (0.310044 --> 0.298930).  Saving model ...
Updating learning rate to 1.043474909634038e-05
	iters: 100, epoch: 3 | loss: 0.5623776
	speed: 0.3816s/iter; left time: 2783.0177s
	iters: 200, epoch: 3 | loss: 0.6567346
	speed: 0.0868s/iter; left time: 624.6690s
Epoch: 3 cost time: 24.58709979057312
Epoch: 3, Steps: 264 | Train Loss: 0.5988228 Vali Loss: 0.2939098 Test Loss: 0.4114997
Validation loss decreased (0.298930 --> 0.293910).  Saving model ...
Updating learning rate to 1.806729275463572e-05
	iters: 100, epoch: 4 | loss: 0.6858138
	speed: 0.3920s/iter; left time: 2755.3199s
	iters: 200, epoch: 4 | loss: 0.4739079
	speed: 0.0881s/iter; left time: 610.3683s
Epoch: 4 cost time: 24.64430069923401
Epoch: 4, Steps: 264 | Train Loss: 0.5874401 Vali Loss: 0.2926554 Test Loss: 0.4065856
Validation loss decreased (0.293910 --> 0.292655).  Saving model ...
Updating learning rate to 2.8013746554825966e-05
	iters: 100, epoch: 5 | loss: 0.6140069
	speed: 0.3778s/iter; left time: 2555.5064s
	iters: 200, epoch: 5 | loss: 0.5120377
	speed: 0.0911s/iter; left time: 607.2810s
Epoch: 5 cost time: 24.493919372558594
Epoch: 5, Steps: 264 | Train Loss: 0.5808639 Vali Loss: 0.2905931 Test Loss: 0.4048381
Validation loss decreased (0.292655 --> 0.290593).  Saving model ...
Updating learning rate to 3.959585042889686e-05
	iters: 100, epoch: 6 | loss: 0.5877042
	speed: 0.3823s/iter; left time: 2485.0164s
	iters: 200, epoch: 6 | loss: 0.5117237
	speed: 0.0909s/iter; left time: 581.9030s
Epoch: 6 cost time: 25.41079068183899
Epoch: 6, Steps: 264 | Train Loss: 0.5752403 Vali Loss: 0.2903231 Test Loss: 0.4031815
Validation loss decreased (0.290593 --> 0.290323).  Saving model ...
Updating learning rate to 5.2023807458350115e-05
	iters: 100, epoch: 7 | loss: 0.3869366
	speed: 0.3749s/iter; left time: 2338.4293s
	iters: 200, epoch: 7 | loss: 0.6898118
	speed: 0.0917s/iter; left time: 562.8489s
Epoch: 7 cost time: 24.874645233154297
Epoch: 7, Steps: 264 | Train Loss: 0.5703931 Vali Loss: 0.2900678 Test Loss: 0.4058122
Validation loss decreased (0.290323 --> 0.290068).  Saving model ...
Updating learning rate to 6.44501410299716e-05
	iters: 100, epoch: 8 | loss: 0.5558590
	speed: 0.3793s/iter; left time: 2265.3707s
	iters: 200, epoch: 8 | loss: 0.5393769
	speed: 0.0933s/iter; left time: 548.0603s
Epoch: 8 cost time: 25.453348398208618
Epoch: 8, Steps: 264 | Train Loss: 0.5645124 Vali Loss: 0.2913375 Test Loss: 0.4081493
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.602748523599488e-05
	iters: 100, epoch: 9 | loss: 0.7574785
	speed: 0.3858s/iter; left time: 2202.3306s
	iters: 200, epoch: 9 | loss: 0.6028914
	speed: 0.0881s/iter; left time: 494.2588s
Epoch: 9 cost time: 24.847737312316895
Epoch: 9, Steps: 264 | Train Loss: 0.5606044 Vali Loss: 0.2929679 Test Loss: 0.4074332
EarlyStopping counter: 2 out of 5
Updating learning rate to 8.596636772513305e-05
	iters: 100, epoch: 10 | loss: 0.4315164
	speed: 0.3959s/iter; left time: 2155.4188s
	iters: 200, epoch: 10 | loss: 0.6377991
	speed: 0.0959s/iter; left time: 512.6884s
Epoch: 10 cost time: 25.175355434417725
Epoch: 10, Steps: 264 | Train Loss: 0.5555904 Vali Loss: 0.2896710 Test Loss: 0.4126677
Validation loss decreased (0.290068 --> 0.289671).  Saving model ...
Updating learning rate to 9.358904472573757e-05
	iters: 100, epoch: 11 | loss: 0.6106098
	speed: 0.3907s/iter; left time: 2024.0889s
	iters: 200, epoch: 11 | loss: 0.6473133
	speed: 0.0889s/iter; left time: 451.6236s
Epoch: 11 cost time: 25.180450677871704
Epoch: 11, Steps: 264 | Train Loss: 0.5516289 Vali Loss: 0.2951813 Test Loss: 0.4191625
EarlyStopping counter: 1 out of 5
Updating learning rate to 9.837571716924175e-05
	iters: 100, epoch: 12 | loss: 0.6563362
	speed: 0.3931s/iter; left time: 1932.9470s
	iters: 200, epoch: 12 | loss: 0.5531932
	speed: 0.0870s/iter; left time: 419.1041s
Epoch: 12 cost time: 24.719167232513428
Epoch: 12, Steps: 264 | Train Loss: 0.5461078 Vali Loss: 0.2915064 Test Loss: 0.4189633
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.999998907339768e-05
	iters: 100, epoch: 13 | loss: 0.4077193
	speed: 0.3952s/iter; left time: 1838.7480s
	iters: 200, epoch: 13 | loss: 0.6250174
	speed: 0.0898s/iter; left time: 408.6558s
Epoch: 13 cost time: 25.204941987991333
Epoch: 13, Steps: 264 | Train Loss: 0.5420918 Vali Loss: 0.2934281 Test Loss: 0.4207056
EarlyStopping counter: 3 out of 5
Updating learning rate to 9.923463992827402e-05
	iters: 100, epoch: 14 | loss: 0.4599496
	speed: 0.3985s/iter; left time: 1748.9466s
	iters: 200, epoch: 14 | loss: 0.4012980
	speed: 0.0893s/iter; left time: 382.8669s
Epoch: 14 cost time: 24.7535400390625
Epoch: 14, Steps: 264 | Train Loss: 0.5373233 Vali Loss: 0.2960551 Test Loss: 0.4265474
EarlyStopping counter: 4 out of 5
Updating learning rate to 9.697332723975848e-05
	iters: 100, epoch: 15 | loss: 0.5085016
	speed: 0.3858s/iter; left time: 1591.3785s
	iters: 200, epoch: 15 | loss: 0.5326753
	speed: 0.0892s/iter; left time: 358.9636s
Epoch: 15 cost time: 24.545570611953735
Epoch: 15, Steps: 264 | Train Loss: 0.5321403 Vali Loss: 0.2962191 Test Loss: 0.4240802
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : 96_720_PatchTST_ETTm2_ftM_sl96_ll48_pl720_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4103853106498718, mae:0.4037404954433441, rse:0.5149206519126892
