Args in experiment:
Namespace(is_training=1, model_id='weather_96_720', model='FITS', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=3, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=22, base_T=144, H_order=12)
Use GPU: cuda:0
>>>>>>>start training : weather_96_720_FITS_custom_ftM_sl96_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36072
val 4551
test 9820
Model(
  (freq_upsampler): Linear(in_features=22, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11058432.0
params:  4301.0
Trainable parameters:  4301
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.4432766
	speed: 0.1160s/iter; left time: 966.1302s
	iters: 200, epoch: 1 | loss: 0.9374751
	speed: 0.0915s/iter; left time: 753.1299s
Epoch: 1 cost time: 28.65125322341919
Epoch: 1, Steps: 281 | Train Loss: 1.1736731 Vali Loss: 0.9750182 Test Loss: 0.4212505
Validation loss decreased (inf --> 0.975018).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7410275
	speed: 0.3374s/iter; left time: 2715.8801s
	iters: 200, epoch: 2 | loss: 0.6966144
	speed: 0.0875s/iter; left time: 695.7244s
Epoch: 2 cost time: 25.328068494796753
Epoch: 2, Steps: 281 | Train Loss: 0.7706644 Vali Loss: 0.8233489 Test Loss: 0.3792174
Validation loss decreased (0.975018 --> 0.823349).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.7001207
	speed: 0.3354s/iter; left time: 2605.5115s
	iters: 200, epoch: 3 | loss: 0.5795711
	speed: 0.0845s/iter; left time: 648.2843s
Epoch: 3 cost time: 24.912348985671997
Epoch: 3, Steps: 281 | Train Loss: 0.6877275 Vali Loss: 0.7902480 Test Loss: 0.3705601
Validation loss decreased (0.823349 --> 0.790248).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6883470
	speed: 0.3302s/iter; left time: 2472.2058s
	iters: 200, epoch: 4 | loss: 0.6528386
	speed: 0.0831s/iter; left time: 614.2575s
Epoch: 4 cost time: 24.611729383468628
Epoch: 4, Steps: 281 | Train Loss: 0.6661730 Vali Loss: 0.7813856 Test Loss: 0.3684100
Validation loss decreased (0.790248 --> 0.781386).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5919921
	speed: 0.3685s/iter; left time: 2655.4582s
	iters: 200, epoch: 5 | loss: 0.6527949
	speed: 0.1006s/iter; left time: 715.0640s
Epoch: 5 cost time: 30.12476086616516
Epoch: 5, Steps: 281 | Train Loss: 0.6580927 Vali Loss: 0.7744310 Test Loss: 0.3677774
Validation loss decreased (0.781386 --> 0.774431).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6380855
	speed: 0.3735s/iter; left time: 2586.7063s
	iters: 200, epoch: 6 | loss: 0.6727366
	speed: 0.0963s/iter; left time: 657.2519s
Epoch: 6 cost time: 27.77604079246521
Epoch: 6, Steps: 281 | Train Loss: 0.6546139 Vali Loss: 0.7733704 Test Loss: 0.3675987
Validation loss decreased (0.774431 --> 0.773370).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6273226
	speed: 0.3729s/iter; left time: 2477.8177s
	iters: 200, epoch: 7 | loss: 0.5959509
	speed: 0.1050s/iter; left time: 687.2366s
Epoch: 7 cost time: 29.44365954399109
Epoch: 7, Steps: 281 | Train Loss: 0.6526444 Vali Loss: 0.7732576 Test Loss: 0.3675930
Validation loss decreased (0.773370 --> 0.773258).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.7904214
	speed: 0.3946s/iter; left time: 2511.4888s
	iters: 200, epoch: 8 | loss: 0.5662051
	speed: 0.0927s/iter; left time: 580.7566s
Epoch: 8 cost time: 27.586788654327393
Epoch: 8, Steps: 281 | Train Loss: 0.6515385 Vali Loss: 0.7725446 Test Loss: 0.3674798
Validation loss decreased (0.773258 --> 0.772545).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5973460
	speed: 0.3699s/iter; left time: 2250.0130s
	iters: 200, epoch: 9 | loss: 0.7759735
	speed: 0.0914s/iter; left time: 546.5985s
Epoch: 9 cost time: 27.944454431533813
Epoch: 9, Steps: 281 | Train Loss: 0.6511042 Vali Loss: 0.7717221 Test Loss: 0.3675035
Validation loss decreased (0.772545 --> 0.771722).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6399126
	speed: 0.3607s/iter; left time: 2092.9413s
	iters: 200, epoch: 10 | loss: 0.6095033
	speed: 0.0887s/iter; left time: 505.6642s
Epoch: 10 cost time: 26.816346168518066
Epoch: 10, Steps: 281 | Train Loss: 0.6507259 Vali Loss: 0.7702430 Test Loss: 0.3675675
Validation loss decreased (0.771722 --> 0.770243).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6136475
	speed: 0.3626s/iter; left time: 2002.1882s
	iters: 200, epoch: 11 | loss: 0.5477851
	speed: 0.0943s/iter; left time: 510.9673s
Epoch: 11 cost time: 27.33233141899109
Epoch: 11, Steps: 281 | Train Loss: 0.6507484 Vali Loss: 0.7708112 Test Loss: 0.3674161
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6633878
	speed: 0.3625s/iter; left time: 1899.2555s
	iters: 200, epoch: 12 | loss: 0.6481103
	speed: 0.1010s/iter; left time: 519.1913s
Epoch: 12 cost time: 28.683048009872437
Epoch: 12, Steps: 281 | Train Loss: 0.6504265 Vali Loss: 0.7704961 Test Loss: 0.3674088
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.7414517
	speed: 0.3867s/iter; left time: 1917.7965s
	iters: 200, epoch: 13 | loss: 0.5834108
	speed: 0.0969s/iter; left time: 470.6967s
Epoch: 13 cost time: 27.568472385406494
Epoch: 13, Steps: 281 | Train Loss: 0.6503011 Vali Loss: 0.7700413 Test Loss: 0.3674181
Validation loss decreased (0.770243 --> 0.770041).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6558634
	speed: 0.3713s/iter; left time: 1736.9976s
	iters: 200, epoch: 14 | loss: 0.5723287
	speed: 0.0886s/iter; left time: 405.3992s
Epoch: 14 cost time: 28.149073123931885
Epoch: 14, Steps: 281 | Train Loss: 0.6502792 Vali Loss: 0.7723972 Test Loss: 0.3674015
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5860162
	speed: 0.3775s/iter; left time: 1659.9346s
	iters: 200, epoch: 15 | loss: 0.6003726
	speed: 0.0990s/iter; left time: 425.2283s
Epoch: 15 cost time: 28.927299737930298
Epoch: 15, Steps: 281 | Train Loss: 0.6502772 Vali Loss: 0.7703828 Test Loss: 0.3673726
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.6676894
	speed: 0.3199s/iter; left time: 1316.5457s
	iters: 200, epoch: 16 | loss: 0.6322991
	speed: 0.0731s/iter; left time: 293.6135s
Epoch: 16 cost time: 22.03189730644226
Epoch: 16, Steps: 281 | Train Loss: 0.6499976 Vali Loss: 0.7716708 Test Loss: 0.3673663
EarlyStopping counter: 3 out of 3
Early stopping
train 36072
val 4551
test 9820
Model(
  (freq_upsampler): Linear(in_features=22, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11058432.0
params:  4301.0
Trainable parameters:  4301
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7539812
	speed: 0.0929s/iter; left time: 774.1958s
	iters: 200, epoch: 1 | loss: 0.6706274
	speed: 0.0865s/iter; left time: 711.8098s
Epoch: 1 cost time: 26.621753931045532
Epoch: 1, Steps: 281 | Train Loss: 0.7278140 Vali Loss: 0.7699742 Test Loss: 0.3671807
Validation loss decreased (inf --> 0.769974).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7205160
	speed: 0.3921s/iter; left time: 3156.7333s
	iters: 200, epoch: 2 | loss: 0.5932624
	speed: 0.0982s/iter; left time: 780.8769s
Epoch: 2 cost time: 28.866368770599365
Epoch: 2, Steps: 281 | Train Loss: 0.7273067 Vali Loss: 0.7706888 Test Loss: 0.3672140
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6953114
	speed: 0.3615s/iter; left time: 2808.7971s
	iters: 200, epoch: 3 | loss: 0.8037603
	speed: 0.0973s/iter; left time: 745.9433s
Epoch: 3 cost time: 27.54057240486145
Epoch: 3, Steps: 281 | Train Loss: 0.7270577 Vali Loss: 0.7702267 Test Loss: 0.3669040
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.7161978
	speed: 0.3966s/iter; left time: 2969.6013s
	iters: 200, epoch: 4 | loss: 0.7198030
	speed: 0.0960s/iter; left time: 709.1019s
Epoch: 4 cost time: 29.545067071914673
Epoch: 4, Steps: 281 | Train Loss: 0.7270594 Vali Loss: 0.7686207 Test Loss: 0.3670497
Validation loss decreased (0.769974 --> 0.768621).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.7225812
	speed: 0.3733s/iter; left time: 2690.2507s
	iters: 200, epoch: 5 | loss: 0.6900413
	speed: 0.0876s/iter; left time: 622.6769s
Epoch: 5 cost time: 27.591630697250366
Epoch: 5, Steps: 281 | Train Loss: 0.7267746 Vali Loss: 0.7690191 Test Loss: 0.3669227
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.7557579
	speed: 0.3562s/iter; left time: 2467.3817s
	iters: 200, epoch: 6 | loss: 0.6954529
	speed: 0.0931s/iter; left time: 635.4680s
Epoch: 6 cost time: 28.152268648147583
Epoch: 6, Steps: 281 | Train Loss: 0.7265279 Vali Loss: 0.7673080 Test Loss: 0.3669035
Validation loss decreased (0.768621 --> 0.767308).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.7109906
	speed: 0.3657s/iter; left time: 2429.8476s
	iters: 200, epoch: 7 | loss: 0.7166795
	speed: 0.0818s/iter; left time: 535.3592s
Epoch: 7 cost time: 24.50417733192444
Epoch: 7, Steps: 281 | Train Loss: 0.7266976 Vali Loss: 0.7678972 Test Loss: 0.3667989
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6681154
	speed: 0.3493s/iter; left time: 2223.1537s
	iters: 200, epoch: 8 | loss: 0.6756013
	speed: 0.0956s/iter; left time: 598.8276s
Epoch: 8 cost time: 27.238962650299072
Epoch: 8, Steps: 281 | Train Loss: 0.7267657 Vali Loss: 0.7682993 Test Loss: 0.3667610
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.7162657
	speed: 0.3727s/iter; left time: 2266.9915s
	iters: 200, epoch: 9 | loss: 0.7807105
	speed: 0.1002s/iter; left time: 599.6924s
Epoch: 9 cost time: 29.385942935943604
Epoch: 9, Steps: 281 | Train Loss: 0.7268092 Vali Loss: 0.7695959 Test Loss: 0.3668054
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : weather_96_720_FITS_custom_ftM_sl96_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3655398190021515, mae:0.3545023798942566, rse:0.7956084609031677, corr:[0.47883272 0.47826633 0.4767403  0.47584242 0.47467127 0.4729124
 0.4708207  0.46854258 0.46592638 0.4630491  0.46036994 0.45766926
 0.45456636 0.45145777 0.44818157 0.4447328  0.44121784 0.43738994
 0.43358046 0.42963725 0.42556316 0.4218254  0.41848433 0.41500413
 0.41148242 0.4086022  0.40708652 0.4060763  0.4049479  0.4040335
 0.40402785 0.40493548 0.40624857 0.4074279  0.40882197 0.41028023
 0.41182914 0.413411   0.41486663 0.415867   0.41648945 0.41716194
 0.4180214  0.41885433 0.4192001  0.41923591 0.41929042 0.41963592
 0.41960055 0.4191486  0.41875243 0.4186203  0.41870075 0.41868597
 0.41848612 0.41831085 0.41820285 0.41791528 0.4176309  0.4174058
 0.41725653 0.41721106 0.41700926 0.4167278  0.41640887 0.41612872
 0.41575813 0.4154346  0.41532138 0.41517252 0.41508695 0.4148432
 0.4146924  0.41462553 0.41442838 0.4140826  0.413684   0.41327155
 0.41300997 0.41267288 0.4123441  0.41197821 0.4115824  0.41137552
 0.4112913  0.41104698 0.41077766 0.41020638 0.40985042 0.4097916
 0.4098277  0.40977347 0.40948465 0.4091913  0.40932477 0.40969816
 0.40991628 0.40979567 0.40971184 0.40967223 0.40972197 0.4097011
 0.409683   0.40976804 0.40992144 0.41002697 0.40994483 0.40992495
 0.40990055 0.4099161  0.41004446 0.41016608 0.41008022 0.40987
 0.40963247 0.4093525  0.40913004 0.40899742 0.40890288 0.40874848
 0.40842983 0.40817687 0.40795735 0.40790397 0.4079134  0.40768373
 0.40734363 0.40697086 0.40668535 0.406357   0.4057895  0.40505445
 0.40436453 0.40373066 0.4033051  0.40294525 0.40260527 0.40225238
 0.4016915  0.4008634  0.399883   0.39884818 0.39789715 0.3967662
 0.39535883 0.3937934  0.3923305  0.3906688  0.38866755 0.38626403
 0.38371578 0.38096172 0.3780068  0.374754   0.37133673 0.36773053
 0.3639616  0.35984305 0.35549304 0.3511129  0.3468774  0.34241432
 0.3378942  0.33402866 0.33084065 0.32774794 0.3240351  0.32070148
 0.31889924 0.31853843 0.31906196 0.3201031  0.32128498 0.32382005
 0.32687104 0.33031413 0.33334625 0.33643463 0.33950505 0.34250325
 0.34545067 0.348001   0.35022756 0.3523138  0.354118   0.35577494
 0.35729593 0.3587599  0.3601978  0.3617097  0.362998   0.3640322
 0.36494732 0.36550888 0.3660529  0.36676842 0.3673611  0.36766377
 0.36774394 0.36776614 0.36798954 0.36840767 0.36870623 0.368668
 0.36861432 0.36854193 0.3685033  0.36838934 0.3681364  0.36787155
 0.36759248 0.36724567 0.36690482 0.36659232 0.3662078  0.365825
 0.36545733 0.36513558 0.3648773  0.36452597 0.364069   0.36370552
 0.36331362 0.3629771  0.3626901  0.36234394 0.3619649  0.3616479
 0.36136976 0.36110052 0.36086094 0.3605247  0.3600528  0.35965118
 0.35934156 0.3590446  0.35876754 0.35853934 0.35826725 0.35804385
 0.35802433 0.35813412 0.35813385 0.35813528 0.3582099  0.35834575
 0.35851058 0.3587091  0.3587118  0.35871032 0.3587314  0.35889995
 0.35916677 0.3593805  0.35952726 0.35974488 0.35972148 0.35958222
 0.35948527 0.35945675 0.35955423 0.35970032 0.35973868 0.35945383
 0.35907662 0.35872924 0.35874578 0.35910055 0.35947934 0.35950908
 0.35929802 0.3589528  0.35874313 0.35859734 0.35839075 0.3580436
 0.35758856 0.35707426 0.35655096 0.35595366 0.35527843 0.35466978
 0.35399058 0.35298958 0.35148734 0.34976572 0.34802285 0.34646013
 0.34481108 0.34296316 0.34079176 0.33847678 0.33623877 0.33390534
 0.33117706 0.3280756  0.3249989  0.3220937  0.31894216 0.3152695
 0.31098342 0.30653602 0.30207425 0.2979803  0.29418236 0.2904233
 0.28628108 0.28215852 0.2786049  0.27595663 0.27409554 0.27276468
 0.27161428 0.27139893 0.27232593 0.27433413 0.27644205 0.27835754
 0.28061026 0.28338423 0.28619692 0.2887368  0.29140717 0.2941325
 0.29691347 0.29926893 0.30165577 0.30385283 0.30605787 0.30816078
 0.31023657 0.3120242  0.31351534 0.31492472 0.31618357 0.31720534
 0.31793973 0.3185776  0.31910312 0.31929886 0.3195733  0.32012072
 0.320774   0.32138327 0.3219112  0.3222861  0.3224832  0.32273892
 0.3228311  0.32274207 0.32254484 0.32216525 0.32181683 0.32128292
 0.3206914  0.3202672  0.32001153 0.31985396 0.3196106  0.31932947
 0.31885815 0.31851307 0.31833053 0.31800914 0.31764296 0.3172471
 0.31698474 0.31676295 0.31654784 0.31630757 0.31607965 0.31582436
 0.31550348 0.31505483 0.31466788 0.31450978 0.3144649  0.31431296
 0.31400323 0.31359267 0.3133594  0.31327432 0.31333652 0.31329793
 0.31311157 0.31287858 0.31284073 0.31288978 0.3131002  0.31337175
 0.31373444 0.31408647 0.314295   0.3143849  0.31451294 0.3149471
 0.31535366 0.3155365  0.3156371  0.31550673 0.31547806 0.31552592
 0.31572372 0.3157476  0.31575343 0.31575206 0.31587362 0.31593257
 0.3160695  0.31615418 0.3162896  0.31649664 0.3167187  0.3170563
 0.31722417 0.31724542 0.3172432  0.31743047 0.31753483 0.31735495
 0.31683382 0.31623513 0.3156136  0.31508172 0.3143213  0.3133286
 0.3122446  0.31125122 0.3102455  0.3089022  0.30726552 0.30557808
 0.3038146  0.30182183 0.29997525 0.29811323 0.29600558 0.2934386
 0.2902386  0.28678483 0.28337336 0.28010765 0.27643988 0.27233353
 0.2680894  0.26377457 0.2591663  0.25430518 0.24935657 0.24461666
 0.23988278 0.23543113 0.23156656 0.2285624  0.22608715 0.22364502
 0.22195083 0.22157733 0.2227019  0.22532797 0.2280627  0.23124301
 0.23465641 0.23817445 0.24156639 0.24466491 0.24817923 0.2520371
 0.25573775 0.25887436 0.26149794 0.26404026 0.26669928 0.26905245
 0.27093267 0.27252695 0.27404958 0.27577525 0.27721283 0.2783653
 0.27947065 0.28058356 0.28147724 0.28212014 0.28263637 0.2831234
 0.28346333 0.28358698 0.28361094 0.2836039  0.28365174 0.28348184
 0.28325087 0.28287983 0.2824356  0.28194323 0.28139704 0.2809699
 0.28073177 0.28056997 0.2802153  0.27987638 0.27971232 0.27967575
 0.2795621  0.27941945 0.27927235 0.27919444 0.27922046 0.27901348
 0.27886477 0.278758   0.2786444  0.27829382 0.27779213 0.27746487
 0.27721706 0.27689782 0.2764635  0.27621213 0.2760029  0.27568722
 0.27531862 0.27496275 0.27475014 0.27473167 0.27456018 0.27421018
 0.2739095  0.2738904  0.27385432 0.2740359  0.27415314 0.27424738
 0.27429238 0.27424967 0.27422982 0.2743665  0.27459192 0.2747343
 0.27461573 0.27449784 0.27458394 0.27497813 0.27527836 0.2754172
 0.27547106 0.2756503  0.27600124 0.2762459  0.2762412  0.27613407
 0.27621427 0.27656814 0.27693737 0.27699283 0.27685404 0.27677816
 0.27675387 0.27677134 0.2765241  0.27611512 0.27549818 0.2749477
 0.27446574 0.27372408 0.27286032 0.27187765 0.27095792 0.27015036
 0.2691512  0.2678772  0.26664418 0.2655092  0.2642303  0.26239818
 0.25981548 0.25692835 0.25424212 0.2515721  0.24839333 0.24430302
 0.24021572 0.23667875 0.23347364 0.22989461 0.2255609  0.22120464
 0.21722358 0.21337959 0.20897552 0.20412315 0.19936699 0.19526951
 0.19167277 0.18810356 0.18505695 0.18305229 0.18188205 0.18113743
 0.18071358 0.18124893 0.18300006 0.18537593 0.18760644 0.18981515
 0.19202316 0.19477199 0.19851007 0.20227034 0.20569555 0.20887685
 0.21198188 0.2152679  0.2189291  0.22205244 0.22520892 0.22772987
 0.23000135 0.23231646 0.23471719 0.23680785 0.2384233  0.23962241
 0.24081643 0.2422298  0.24351583 0.2444125  0.2450773  0.24559164
 0.24616687 0.24667148 0.24714908 0.24745479 0.2475977  0.24762069
 0.24763367 0.24771383 0.24801658 0.2482221  0.24834551 0.24824117
 0.24808529 0.24783252 0.24758816 0.24733703 0.24713905 0.24700384
 0.24687892 0.24663562 0.24634628 0.24599452 0.24570684 0.24525705
 0.24486332 0.24451761 0.24417219 0.24363956 0.24289899 0.24222355
 0.24187426 0.24170266 0.24132772 0.24077109 0.24024378 0.23982677
 0.2394935  0.23923233 0.23911422 0.23905104 0.23896413 0.23866716
 0.23824252 0.23806211 0.23808068 0.23807645 0.23790148 0.23776841
 0.23779233 0.237957   0.2381287  0.23825902 0.23842473 0.23860529
 0.23876688 0.2388911  0.23900864 0.23913677 0.2392749  0.23954457
 0.23987706 0.24018045 0.24032813 0.24047445 0.24077927 0.24114577
 0.24139673 0.24136789 0.24119824 0.24121451 0.24142092 0.24159463
 0.24163583 0.24152263 0.24139635 0.24144618 0.24171829 0.24187097
 0.24147037 0.24041234 0.23930441 0.2390442  0.23921514 0.23890646
 0.23780936 0.23675872 0.23608626 0.23537342 0.23390329 0.23250705]
