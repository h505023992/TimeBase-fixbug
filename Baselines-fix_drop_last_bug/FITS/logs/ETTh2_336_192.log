Args in experiment:
Namespace(is_training=1, model_id='ETTh2_336_192', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=100, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_336_192_FITS_ETTh2_ftM_sl336_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8113
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=100, out_features=157, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14067200.0
params:  15857.0
Trainable parameters:  15857
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.394505739212036
Epoch: 1, Steps: 63 | Train Loss: 0.5586281 Vali Loss: 0.4046081 Test Loss: 0.4747010
Validation loss decreased (inf --> 0.404608).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.2870869636535645
Epoch: 2, Steps: 63 | Train Loss: 0.4415429 Vali Loss: 0.3648438 Test Loss: 0.4359617
Validation loss decreased (0.404608 --> 0.364844).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.1468687057495117
Epoch: 3, Steps: 63 | Train Loss: 0.3800867 Vali Loss: 0.3440819 Test Loss: 0.4175462
Validation loss decreased (0.364844 --> 0.344082).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.314121961593628
Epoch: 4, Steps: 63 | Train Loss: 0.3440438 Vali Loss: 0.3314738 Test Loss: 0.4079156
Validation loss decreased (0.344082 --> 0.331474).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.446617603302002
Epoch: 5, Steps: 63 | Train Loss: 0.3209498 Vali Loss: 0.3241590 Test Loss: 0.4027566
Validation loss decreased (0.331474 --> 0.324159).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.3086469173431396
Epoch: 6, Steps: 63 | Train Loss: 0.3038330 Vali Loss: 0.3188494 Test Loss: 0.3989183
Validation loss decreased (0.324159 --> 0.318849).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3673315048217773
Epoch: 7, Steps: 63 | Train Loss: 0.2901027 Vali Loss: 0.3148793 Test Loss: 0.3964177
Validation loss decreased (0.318849 --> 0.314879).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.3961949348449707
Epoch: 8, Steps: 63 | Train Loss: 0.2796002 Vali Loss: 0.3117265 Test Loss: 0.3941616
Validation loss decreased (0.314879 --> 0.311726).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9718880653381348
Epoch: 9, Steps: 63 | Train Loss: 0.2714820 Vali Loss: 0.3090059 Test Loss: 0.3922721
Validation loss decreased (0.311726 --> 0.309006).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.0685231685638428
Epoch: 10, Steps: 63 | Train Loss: 0.2636458 Vali Loss: 0.3068097 Test Loss: 0.3905292
Validation loss decreased (0.309006 --> 0.306810).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.14894962310791
Epoch: 11, Steps: 63 | Train Loss: 0.2577303 Vali Loss: 0.3048945 Test Loss: 0.3887488
Validation loss decreased (0.306810 --> 0.304895).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.361120700836182
Epoch: 12, Steps: 63 | Train Loss: 0.2520353 Vali Loss: 0.3031243 Test Loss: 0.3874126
Validation loss decreased (0.304895 --> 0.303124).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.080512046813965
Epoch: 13, Steps: 63 | Train Loss: 0.2469627 Vali Loss: 0.3018382 Test Loss: 0.3858548
Validation loss decreased (0.303124 --> 0.301838).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.178413152694702
Epoch: 14, Steps: 63 | Train Loss: 0.2433565 Vali Loss: 0.3004390 Test Loss: 0.3846798
Validation loss decreased (0.301838 --> 0.300439).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.287627935409546
Epoch: 15, Steps: 63 | Train Loss: 0.2399081 Vali Loss: 0.2992780 Test Loss: 0.3833877
Validation loss decreased (0.300439 --> 0.299278).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.1352896690368652
Epoch: 16, Steps: 63 | Train Loss: 0.2370141 Vali Loss: 0.2983277 Test Loss: 0.3821855
Validation loss decreased (0.299278 --> 0.298328).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.203376054763794
Epoch: 17, Steps: 63 | Train Loss: 0.2341811 Vali Loss: 0.2973667 Test Loss: 0.3810151
Validation loss decreased (0.298328 --> 0.297367).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.2322144508361816
Epoch: 18, Steps: 63 | Train Loss: 0.2311770 Vali Loss: 0.2965938 Test Loss: 0.3799894
Validation loss decreased (0.297367 --> 0.296594).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.1821227073669434
Epoch: 19, Steps: 63 | Train Loss: 0.2293954 Vali Loss: 0.2954161 Test Loss: 0.3789864
Validation loss decreased (0.296594 --> 0.295416).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.8464109897613525
Epoch: 20, Steps: 63 | Train Loss: 0.2260656 Vali Loss: 0.2949874 Test Loss: 0.3781449
Validation loss decreased (0.295416 --> 0.294987).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.1021499633789062
Epoch: 21, Steps: 63 | Train Loss: 0.2247379 Vali Loss: 0.2942984 Test Loss: 0.3772858
Validation loss decreased (0.294987 --> 0.294298).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.4176294803619385
Epoch: 22, Steps: 63 | Train Loss: 0.2232542 Vali Loss: 0.2937058 Test Loss: 0.3764508
Validation loss decreased (0.294298 --> 0.293706).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.0982706546783447
Epoch: 23, Steps: 63 | Train Loss: 0.2215303 Vali Loss: 0.2931027 Test Loss: 0.3757925
Validation loss decreased (0.293706 --> 0.293103).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.436309576034546
Epoch: 24, Steps: 63 | Train Loss: 0.2200779 Vali Loss: 0.2926026 Test Loss: 0.3750750
Validation loss decreased (0.293103 --> 0.292603).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2882256507873535
Epoch: 25, Steps: 63 | Train Loss: 0.2190951 Vali Loss: 0.2921029 Test Loss: 0.3742997
Validation loss decreased (0.292603 --> 0.292103).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.253513813018799
Epoch: 26, Steps: 63 | Train Loss: 0.2182598 Vali Loss: 0.2918096 Test Loss: 0.3736888
Validation loss decreased (0.292103 --> 0.291810).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.177278518676758
Epoch: 27, Steps: 63 | Train Loss: 0.2167390 Vali Loss: 0.2913535 Test Loss: 0.3731682
Validation loss decreased (0.291810 --> 0.291353).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.2639858722686768
Epoch: 28, Steps: 63 | Train Loss: 0.2160708 Vali Loss: 0.2909113 Test Loss: 0.3725644
Validation loss decreased (0.291353 --> 0.290911).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.9136810302734375
Epoch: 29, Steps: 63 | Train Loss: 0.2155224 Vali Loss: 0.2905981 Test Loss: 0.3721093
Validation loss decreased (0.290911 --> 0.290598).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.360874891281128
Epoch: 30, Steps: 63 | Train Loss: 0.2141458 Vali Loss: 0.2903372 Test Loss: 0.3715913
Validation loss decreased (0.290598 --> 0.290337).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 8113
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=100, out_features=157, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14067200.0
params:  15857.0
Trainable parameters:  15857
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.104691505432129
Epoch: 1, Steps: 63 | Train Loss: 0.5204833 Vali Loss: 0.2843775 Test Loss: 0.3617102
Validation loss decreased (inf --> 0.284378).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.7174811363220215
Epoch: 2, Steps: 63 | Train Loss: 0.5108577 Vali Loss: 0.2817267 Test Loss: 0.3579452
Validation loss decreased (0.284378 --> 0.281727).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.9412550926208496
Epoch: 3, Steps: 63 | Train Loss: 0.5073709 Vali Loss: 0.2803304 Test Loss: 0.3568811
Validation loss decreased (0.281727 --> 0.280330).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.99782133102417
Epoch: 4, Steps: 63 | Train Loss: 0.5069889 Vali Loss: 0.2798762 Test Loss: 0.3558966
Validation loss decreased (0.280330 --> 0.279876).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.3127169609069824
Epoch: 5, Steps: 63 | Train Loss: 0.5057569 Vali Loss: 0.2794136 Test Loss: 0.3557424
Validation loss decreased (0.279876 --> 0.279414).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.1427788734436035
Epoch: 6, Steps: 63 | Train Loss: 0.5064223 Vali Loss: 0.2792216 Test Loss: 0.3554455
Validation loss decreased (0.279414 --> 0.279222).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1698265075683594
Epoch: 7, Steps: 63 | Train Loss: 0.5066474 Vali Loss: 0.2785864 Test Loss: 0.3554941
Validation loss decreased (0.279222 --> 0.278586).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.2932615280151367
Epoch: 8, Steps: 63 | Train Loss: 0.5040284 Vali Loss: 0.2781792 Test Loss: 0.3555499
Validation loss decreased (0.278586 --> 0.278179).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.06180477142334
Epoch: 9, Steps: 63 | Train Loss: 0.5035339 Vali Loss: 0.2782283 Test Loss: 0.3553044
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.964341640472412
Epoch: 10, Steps: 63 | Train Loss: 0.5038991 Vali Loss: 0.2781055 Test Loss: 0.3551879
Validation loss decreased (0.278179 --> 0.278106).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.083749294281006
Epoch: 11, Steps: 63 | Train Loss: 0.5025797 Vali Loss: 0.2778165 Test Loss: 0.3551328
Validation loss decreased (0.278106 --> 0.277817).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.341402053833008
Epoch: 12, Steps: 63 | Train Loss: 0.5050370 Vali Loss: 0.2777617 Test Loss: 0.3548626
Validation loss decreased (0.277817 --> 0.277762).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.1583666801452637
Epoch: 13, Steps: 63 | Train Loss: 0.5043719 Vali Loss: 0.2777064 Test Loss: 0.3547779
Validation loss decreased (0.277762 --> 0.277706).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.1687028408050537
Epoch: 14, Steps: 63 | Train Loss: 0.5029139 Vali Loss: 0.2775804 Test Loss: 0.3547731
Validation loss decreased (0.277706 --> 0.277580).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.460984945297241
Epoch: 15, Steps: 63 | Train Loss: 0.5034620 Vali Loss: 0.2774149 Test Loss: 0.3548019
Validation loss decreased (0.277580 --> 0.277415).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.2602920532226562
Epoch: 16, Steps: 63 | Train Loss: 0.5026590 Vali Loss: 0.2773325 Test Loss: 0.3546574
Validation loss decreased (0.277415 --> 0.277332).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.2942302227020264
Epoch: 17, Steps: 63 | Train Loss: 0.5038782 Vali Loss: 0.2774052 Test Loss: 0.3546863
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.412370443344116
Epoch: 18, Steps: 63 | Train Loss: 0.5027340 Vali Loss: 0.2770673 Test Loss: 0.3545046
Validation loss decreased (0.277332 --> 0.277067).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.42264986038208
Epoch: 19, Steps: 63 | Train Loss: 0.5033345 Vali Loss: 0.2771335 Test Loss: 0.3547144
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.9954922199249268
Epoch: 20, Steps: 63 | Train Loss: 0.5009505 Vali Loss: 0.2770159 Test Loss: 0.3546322
Validation loss decreased (0.277067 --> 0.277016).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.590132713317871
Epoch: 21, Steps: 63 | Train Loss: 0.5018776 Vali Loss: 0.2769261 Test Loss: 0.3548006
Validation loss decreased (0.277016 --> 0.276926).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.2064661979675293
Epoch: 22, Steps: 63 | Train Loss: 0.5025850 Vali Loss: 0.2769518 Test Loss: 0.3545748
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.0686018466949463
Epoch: 23, Steps: 63 | Train Loss: 0.5028421 Vali Loss: 0.2768649 Test Loss: 0.3546181
Validation loss decreased (0.276926 --> 0.276865).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.3296871185302734
Epoch: 24, Steps: 63 | Train Loss: 0.5019465 Vali Loss: 0.2768747 Test Loss: 0.3546193
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.0486903190612793
Epoch: 25, Steps: 63 | Train Loss: 0.5029684 Vali Loss: 0.2768398 Test Loss: 0.3545832
Validation loss decreased (0.276865 --> 0.276840).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.4237303733825684
Epoch: 26, Steps: 63 | Train Loss: 0.5015655 Vali Loss: 0.2767541 Test Loss: 0.3545212
Validation loss decreased (0.276840 --> 0.276754).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.2993297576904297
Epoch: 27, Steps: 63 | Train Loss: 0.5016443 Vali Loss: 0.2766940 Test Loss: 0.3544817
Validation loss decreased (0.276754 --> 0.276694).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.298691987991333
Epoch: 28, Steps: 63 | Train Loss: 0.5024778 Vali Loss: 0.2766069 Test Loss: 0.3545907
Validation loss decreased (0.276694 --> 0.276607).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.2844982147216797
Epoch: 29, Steps: 63 | Train Loss: 0.4998139 Vali Loss: 0.2763667 Test Loss: 0.3544209
Validation loss decreased (0.276607 --> 0.276367).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.12717604637146
Epoch: 30, Steps: 63 | Train Loss: 0.5028920 Vali Loss: 0.2766505 Test Loss: 0.3544505
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011296777049628277
>>>>>>>testing : ETTh2_336_192_FITS_ETTh2_ftM_sl336_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3351394534111023, mae:0.3757368326187134, rse:0.464251846075058, corr:[0.2671054  0.2695275  0.26778954 0.26823443 0.26669347 0.2648621
 0.2645918  0.26369563 0.2621704  0.26116964 0.2602073  0.25846037
 0.25685692 0.2560091  0.25538442 0.25490344 0.2543994  0.25394106
 0.2532262  0.2520807  0.25066626 0.24953297 0.24848543 0.2466599
 0.24412976 0.24161012 0.23980188 0.23850784 0.2371536  0.23586622
 0.23502679 0.23354319 0.23136963 0.22966333 0.22886316 0.22792497
 0.22646198 0.2254907  0.2247185  0.22367465 0.22303303 0.22301313
 0.22245361 0.22114396 0.21968508 0.21831277 0.21716528 0.21540499
 0.2132567  0.21118909 0.20956951 0.20799817 0.2067541  0.20556375
 0.20344496 0.2009576  0.19913335 0.19775167 0.19637813 0.19511293
 0.19478042 0.19453532 0.19434412 0.19405839 0.19379808 0.19333273
 0.19256139 0.19181973 0.1910679  0.19027907 0.18966623 0.18908589
 0.18766434 0.18673818 0.1865948  0.18580838 0.18441507 0.18380304
 0.18343578 0.18234015 0.18203065 0.18211156 0.18177062 0.18124278
 0.1812192  0.1813323  0.18112524 0.18092509 0.1804427  0.17969052
 0.17918001 0.17913492 0.17901988 0.17874964 0.17891622 0.17852695
 0.17726257 0.17631473 0.17571656 0.1747566  0.17402951 0.17347176
 0.17255205 0.17158113 0.17182027 0.17195328 0.17160207 0.17159326
 0.17160518 0.17064942 0.1693108  0.16902557 0.1685653  0.16785958
 0.1674465  0.16705945 0.16648266 0.16568711 0.16458221 0.16279288
 0.16094385 0.15931104 0.15794471 0.15670922 0.15580203 0.15499337
 0.15390077 0.15315291 0.15299085 0.15212058 0.1510661  0.15095712
 0.15059102 0.14989693 0.14960904 0.14956732 0.14898057 0.14890644
 0.14913629 0.14821038 0.14761552 0.14777613 0.14735068 0.1456168
 0.14427327 0.14347515 0.14215684 0.14100793 0.14069837 0.1400211
 0.13915461 0.13870715 0.138498   0.13787436 0.13812743 0.13843971
 0.13768788 0.1369794  0.13741072 0.13804437 0.13764118 0.13778839
 0.13754888 0.13718216 0.13747704 0.13768955 0.13695483 0.13597688
 0.13545568 0.13351548 0.13146669 0.13135315 0.13149932 0.1294728
 0.12818095 0.12729982 0.12541589 0.12427466 0.1241211  0.12307096
 0.12148856 0.12209015 0.12142199 0.12048194 0.12105214 0.12062101
 0.11786087 0.1176857  0.11914727 0.11555494 0.11597136 0.11887833]
