Args in experiment:
Namespace(is_training=1, model_id='ETTh2_720_336', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=196, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0323235988616943
Epoch: 1, Steps: 59 | Train Loss: 0.6719186 Vali Loss: 0.5864709 Test Loss: 0.4403478
Validation loss decreased (inf --> 0.586471).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.2203786373138428
Epoch: 2, Steps: 59 | Train Loss: 0.5226008 Vali Loss: 0.5212353 Test Loss: 0.4089462
Validation loss decreased (0.586471 --> 0.521235).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.444637060165405
Epoch: 3, Steps: 59 | Train Loss: 0.4530672 Vali Loss: 0.4904372 Test Loss: 0.3975318
Validation loss decreased (0.521235 --> 0.490437).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.480438709259033
Epoch: 4, Steps: 59 | Train Loss: 0.4135862 Vali Loss: 0.4675640 Test Loss: 0.3927922
Validation loss decreased (0.490437 --> 0.467564).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.5451858043670654
Epoch: 5, Steps: 59 | Train Loss: 0.3864932 Vali Loss: 0.4621397 Test Loss: 0.3901186
Validation loss decreased (0.467564 --> 0.462140).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.437985897064209
Epoch: 6, Steps: 59 | Train Loss: 0.3652937 Vali Loss: 0.4525985 Test Loss: 0.3886129
Validation loss decreased (0.462140 --> 0.452598).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3384764194488525
Epoch: 7, Steps: 59 | Train Loss: 0.3490504 Vali Loss: 0.4436511 Test Loss: 0.3870438
Validation loss decreased (0.452598 --> 0.443651).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.1446621417999268
Epoch: 8, Steps: 59 | Train Loss: 0.3351671 Vali Loss: 0.4381627 Test Loss: 0.3856057
Validation loss decreased (0.443651 --> 0.438163).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.2830677032470703
Epoch: 9, Steps: 59 | Train Loss: 0.3230336 Vali Loss: 0.4327947 Test Loss: 0.3843774
Validation loss decreased (0.438163 --> 0.432795).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.501178503036499
Epoch: 10, Steps: 59 | Train Loss: 0.3129107 Vali Loss: 0.4319652 Test Loss: 0.3830997
Validation loss decreased (0.432795 --> 0.431965).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.1611030101776123
Epoch: 11, Steps: 59 | Train Loss: 0.3038511 Vali Loss: 0.4258460 Test Loss: 0.3818820
Validation loss decreased (0.431965 --> 0.425846).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3280510902404785
Epoch: 12, Steps: 59 | Train Loss: 0.2958590 Vali Loss: 0.4254926 Test Loss: 0.3806409
Validation loss decreased (0.425846 --> 0.425493).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.551889181137085
Epoch: 13, Steps: 59 | Train Loss: 0.2891972 Vali Loss: 0.4210650 Test Loss: 0.3795323
Validation loss decreased (0.425493 --> 0.421065).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.473360538482666
Epoch: 14, Steps: 59 | Train Loss: 0.2830074 Vali Loss: 0.4200918 Test Loss: 0.3783664
Validation loss decreased (0.421065 --> 0.420092).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.5629847049713135
Epoch: 15, Steps: 59 | Train Loss: 0.2774803 Vali Loss: 0.4165982 Test Loss: 0.3774858
Validation loss decreased (0.420092 --> 0.416598).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.4148247241973877
Epoch: 16, Steps: 59 | Train Loss: 0.2727538 Vali Loss: 0.4161816 Test Loss: 0.3765110
Validation loss decreased (0.416598 --> 0.416182).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.2584750652313232
Epoch: 17, Steps: 59 | Train Loss: 0.2687942 Vali Loss: 0.4152461 Test Loss: 0.3756678
Validation loss decreased (0.416182 --> 0.415246).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.96358323097229
Epoch: 18, Steps: 59 | Train Loss: 0.2649373 Vali Loss: 0.4123809 Test Loss: 0.3748861
Validation loss decreased (0.415246 --> 0.412381).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.0963640213012695
Epoch: 19, Steps: 59 | Train Loss: 0.2613548 Vali Loss: 0.4138133 Test Loss: 0.3741198
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.33972692489624
Epoch: 20, Steps: 59 | Train Loss: 0.2580802 Vali Loss: 0.4109622 Test Loss: 0.3734536
Validation loss decreased (0.412381 --> 0.410962).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.098130702972412
Epoch: 21, Steps: 59 | Train Loss: 0.2553705 Vali Loss: 0.4093077 Test Loss: 0.3728115
Validation loss decreased (0.410962 --> 0.409308).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.0893895626068115
Epoch: 22, Steps: 59 | Train Loss: 0.2522473 Vali Loss: 0.4091574 Test Loss: 0.3723001
Validation loss decreased (0.409308 --> 0.409157).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.13849663734436
Epoch: 23, Steps: 59 | Train Loss: 0.2506033 Vali Loss: 0.4083400 Test Loss: 0.3717510
Validation loss decreased (0.409157 --> 0.408340).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.403578519821167
Epoch: 24, Steps: 59 | Train Loss: 0.2484648 Vali Loss: 0.4066209 Test Loss: 0.3713566
Validation loss decreased (0.408340 --> 0.406621).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.3914268016815186
Epoch: 25, Steps: 59 | Train Loss: 0.2461813 Vali Loss: 0.4076591 Test Loss: 0.3708541
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.5301153659820557
Epoch: 26, Steps: 59 | Train Loss: 0.2445937 Vali Loss: 0.4040515 Test Loss: 0.3704501
Validation loss decreased (0.406621 --> 0.404051).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.4687368869781494
Epoch: 27, Steps: 59 | Train Loss: 0.2428097 Vali Loss: 0.4049163 Test Loss: 0.3700784
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.1547763347625732
Epoch: 28, Steps: 59 | Train Loss: 0.2413789 Vali Loss: 0.4028886 Test Loss: 0.3697769
Validation loss decreased (0.404051 --> 0.402889).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.119849443435669
Epoch: 29, Steps: 59 | Train Loss: 0.2402724 Vali Loss: 0.4057644 Test Loss: 0.3695269
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.339287996292114
Epoch: 30, Steps: 59 | Train Loss: 0.2390918 Vali Loss: 0.4031188 Test Loss: 0.3691130
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00011296777049628277
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.047498941421509
Epoch: 1, Steps: 59 | Train Loss: 0.6325719 Vali Loss: 0.3885656 Test Loss: 0.3641875
Validation loss decreased (inf --> 0.388566).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.0244975090026855
Epoch: 2, Steps: 59 | Train Loss: 0.6199391 Vali Loss: 0.3851399 Test Loss: 0.3614788
Validation loss decreased (0.388566 --> 0.385140).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.279573678970337
Epoch: 3, Steps: 59 | Train Loss: 0.6166818 Vali Loss: 0.3831299 Test Loss: 0.3604107
Validation loss decreased (0.385140 --> 0.383130).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.4598007202148438
Epoch: 4, Steps: 59 | Train Loss: 0.6136457 Vali Loss: 0.3812568 Test Loss: 0.3598542
Validation loss decreased (0.383130 --> 0.381257).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.3936820030212402
Epoch: 5, Steps: 59 | Train Loss: 0.6135911 Vali Loss: 0.3811825 Test Loss: 0.3595787
Validation loss decreased (0.381257 --> 0.381183).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.3932201862335205
Epoch: 6, Steps: 59 | Train Loss: 0.6122220 Vali Loss: 0.3809711 Test Loss: 0.3593673
Validation loss decreased (0.381183 --> 0.380971).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.2789652347564697
Epoch: 7, Steps: 59 | Train Loss: 0.6127907 Vali Loss: 0.3790119 Test Loss: 0.3589525
Validation loss decreased (0.380971 --> 0.379012).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.2217297554016113
Epoch: 8, Steps: 59 | Train Loss: 0.6119207 Vali Loss: 0.3791379 Test Loss: 0.3591054
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.287571907043457
Epoch: 9, Steps: 59 | Train Loss: 0.6107280 Vali Loss: 0.3766174 Test Loss: 0.3593187
Validation loss decreased (0.379012 --> 0.376617).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.558681488037109
Epoch: 10, Steps: 59 | Train Loss: 0.6111785 Vali Loss: 0.3776327 Test Loss: 0.3588992
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.3164350986480713
Epoch: 11, Steps: 59 | Train Loss: 0.6101250 Vali Loss: 0.3776072 Test Loss: 0.3589318
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3299832344055176
Epoch: 12, Steps: 59 | Train Loss: 0.6118107 Vali Loss: 0.3750614 Test Loss: 0.3589806
Validation loss decreased (0.376617 --> 0.375061).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.398095607757568
Epoch: 13, Steps: 59 | Train Loss: 0.6116476 Vali Loss: 0.3787564 Test Loss: 0.3587239
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.551605701446533
Epoch: 14, Steps: 59 | Train Loss: 0.6109426 Vali Loss: 0.3742017 Test Loss: 0.3590026
Validation loss decreased (0.375061 --> 0.374202).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.489042043685913
Epoch: 15, Steps: 59 | Train Loss: 0.6099859 Vali Loss: 0.3762258 Test Loss: 0.3587561
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.492060899734497
Epoch: 16, Steps: 59 | Train Loss: 0.6090910 Vali Loss: 0.3765340 Test Loss: 0.3587761
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.3108770847320557
Epoch: 17, Steps: 59 | Train Loss: 0.6094587 Vali Loss: 0.3766454 Test Loss: 0.3586209
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.1383955478668213
Epoch: 18, Steps: 59 | Train Loss: 0.6100093 Vali Loss: 0.3755468 Test Loss: 0.3585864
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.4006094932556152
Epoch: 19, Steps: 59 | Train Loss: 0.6081612 Vali Loss: 0.3755751 Test Loss: 0.3586132
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35470330715179443, mae:0.3956565856933594, rse:0.4761810898780823, corr:[0.262428   0.2655225  0.2635895  0.2641058  0.26371866 0.2623615
 0.26230738 0.2621536  0.26066667 0.25918087 0.25840184 0.25716856
 0.25560752 0.25442946 0.25380176 0.2531485  0.2522493  0.25136682
 0.25072035 0.24998873 0.24892761 0.24779552 0.24666874 0.24540217
 0.24365947 0.24203634 0.24085252 0.23988156 0.23890859 0.2381578
 0.23760927 0.23675776 0.23563917 0.23463342 0.23359512 0.23248391
 0.23181115 0.23163237 0.23082253 0.22939451 0.22859298 0.22847034
 0.22801128 0.22707891 0.226314   0.22555147 0.22404647 0.22196752
 0.22040261 0.21915494 0.2175188  0.21591851 0.21477857 0.21323077
 0.21140386 0.2102049  0.2087058  0.20607975 0.20381834 0.20309202
 0.2030069  0.20270222 0.20253938 0.20246652 0.20172061 0.20094848
 0.20026366 0.19967149 0.19943415 0.19965239 0.19925556 0.19797876
 0.19694668 0.19683956 0.19619046 0.19404742 0.19249521 0.19248834
 0.192204   0.19065568 0.18990631 0.1900656  0.1896703  0.18890066
 0.18905303 0.189638   0.18923074 0.18845432 0.18842113 0.18830858
 0.18704134 0.18589418 0.18647645 0.18698446 0.18622454 0.18568256
 0.18625976 0.18619795 0.18476687 0.18364425 0.18378401 0.18344961
 0.18222865 0.18127601 0.18138729 0.18146889 0.18163882 0.18231523
 0.18244307 0.18150418 0.18021995 0.17973582 0.1793442  0.1788512
 0.17845637 0.17813766 0.17743802 0.17623766 0.17557423 0.17503114
 0.17380886 0.17194736 0.17023481 0.16879956 0.16782154 0.16777627
 0.16800791 0.1674995  0.16636963 0.16521308 0.16416468 0.16307458
 0.16240801 0.16190843 0.16140613 0.161306   0.16155122 0.16118255
 0.15970236 0.15882672 0.1595777  0.16010451 0.15884805 0.15678588
 0.15530829 0.15436055 0.15320083 0.15220621 0.15138422 0.15007935
 0.14852735 0.14775519 0.1481112  0.14797583 0.14675573 0.14537317
 0.14458036 0.14419286 0.14394073 0.14367352 0.14332281 0.1436673
 0.14387889 0.14290491 0.14172737 0.14204484 0.14297706 0.14187658
 0.13934596 0.13782425 0.13777678 0.13707638 0.13554606 0.13457161
 0.13466957 0.1339684  0.13258012 0.13153195 0.13040797 0.12928526
 0.12864219 0.12871666 0.12874398 0.12844595 0.12802286 0.12797266
 0.12752137 0.12675153 0.12670057 0.12713425 0.12770934 0.1277778
 0.12737791 0.12652586 0.1257132  0.12593065 0.12631255 0.12565826
 0.12468056 0.12433067 0.12411692 0.12288925 0.12185403 0.1223037
 0.12305596 0.12280673 0.12188239 0.12210824 0.12318398 0.1237928
 0.1232862  0.12240563 0.12207277 0.12260439 0.123528   0.12350328
 0.12237152 0.12097422 0.11993217 0.11953904 0.11949079 0.12000769
 0.12020211 0.12053    0.12098487 0.12113366 0.12000264 0.11790743
 0.11642434 0.11642709 0.11713452 0.1184025  0.11935598 0.11944857
 0.11857194 0.1179024  0.11797883 0.118464   0.11928642 0.12045672
 0.12084877 0.11953907 0.11801755 0.11794492 0.11792669 0.11681768
 0.11650498 0.11785866 0.11922234 0.11854591 0.11708868 0.11709712
 0.11793208 0.11901011 0.11984696 0.121232   0.12182147 0.12221013
 0.12307419 0.1245807  0.12555252 0.1253147  0.12556037 0.12638874
 0.1273734  0.12732191 0.12665796 0.12632334 0.12670907 0.12736124
 0.12742516 0.12742846 0.12781045 0.12721929 0.12632708 0.12579446
 0.12614627 0.12616651 0.1257031  0.12627764 0.12778841 0.12824453
 0.12731251 0.12675154 0.12670659 0.12660673 0.12680613 0.12749432
 0.12758023 0.126391   0.12493231 0.12452967 0.12446756 0.12331779
 0.12178009 0.12088078 0.12107652 0.12150721 0.12132908 0.12004455
 0.11888468 0.1194385  0.12000303 0.12000383 0.1201184  0.12191191
 0.1222816  0.12133303 0.12109352 0.12216583 0.12254355 0.12199804
 0.1221768  0.12185127 0.12060647 0.11949471 0.11898984 0.11733965
 0.11532046 0.11608267 0.11783495 0.11812443 0.11730925 0.11846161
 0.11916864 0.11805131 0.11790165 0.11938526 0.1195403  0.11877441
 0.11988933 0.12048911 0.11836449 0.11796942 0.12076607 0.11323941]
