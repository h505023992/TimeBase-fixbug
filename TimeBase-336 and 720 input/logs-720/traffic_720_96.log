Args in experiment:
Namespace(is_training=1, model_id='traffic_720_96', model='LightTimeBaseTST', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=96, period_len=24, basis_num=6, use_period_norm=1, use_orthogonal=1, orthogonal_weight=0.04, fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=7, individual=0, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='learned', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.05, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=3, use_multi_gpu=0, devices='0,1', test_flop=False)
Use GPU: cuda:3
214
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
MACs: 4220352.0
Params: 214.0
4.22M MACs
>>>>>>>start training : traffic_720_96_LightTimeBaseTST_custom_ftM_sl720_pl96_test_0_seed2023>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1661
test 3413
Max Memory (MB): 1126.0185546875
Epoch: 1 cost time: 25.33198070526123
Epoch: 1, Steps: 90 | Train Loss: 0.4099335 Vali Loss: 0.4959882 Test Loss: 0.5947158
Validation loss decreased (inf --> 0.495988).  Saving model ...
Updating learning rate to 0.05
Max Memory (MB): 1314.01953125
Epoch: 2 cost time: 25.464375257492065
Epoch: 2, Steps: 90 | Train Loss: 0.2782025 Vali Loss: 0.3621370 Test Loss: 0.4389544
Validation loss decreased (0.495988 --> 0.362137).  Saving model ...
Updating learning rate to 0.05
Max Memory (MB): 1314.01953125
Epoch: 3 cost time: 26.18340277671814
Epoch: 3, Steps: 90 | Train Loss: 0.2492969 Vali Loss: 0.3437625 Test Loss: 0.4110652
Validation loss decreased (0.362137 --> 0.343763).  Saving model ...
Updating learning rate to 0.05
Max Memory (MB): 1314.01953125
Epoch: 4 cost time: 26.102980136871338
Epoch: 4, Steps: 90 | Train Loss: 0.2455601 Vali Loss: 0.3419180 Test Loss: 0.4192145
Validation loss decreased (0.343763 --> 0.341918).  Saving model ...
Updating learning rate to 0.04000000000000001
Max Memory (MB): 1314.01953125
Epoch: 5 cost time: 25.216614961624146
Epoch: 5, Steps: 90 | Train Loss: 0.2425967 Vali Loss: 0.3348716 Test Loss: 0.4067698
Validation loss decreased (0.341918 --> 0.334872).  Saving model ...
Updating learning rate to 0.03200000000000001
Max Memory (MB): 1314.01953125
Epoch: 6 cost time: 24.952011108398438
Epoch: 6, Steps: 90 | Train Loss: 0.2420741 Vali Loss: 0.3394184 Test Loss: 0.4076955
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.025600000000000008
Max Memory (MB): 1314.01953125
Epoch: 7 cost time: 25.24755096435547
Epoch: 7, Steps: 90 | Train Loss: 0.2409864 Vali Loss: 0.3417570 Test Loss: 0.4085470
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.020480000000000005
Max Memory (MB): 1314.01953125
Epoch: 8 cost time: 25.86536455154419
Epoch: 8, Steps: 90 | Train Loss: 0.2402404 Vali Loss: 0.3336013 Test Loss: 0.4011539
Validation loss decreased (0.334872 --> 0.333601).  Saving model ...
Updating learning rate to 0.016384000000000006
Max Memory (MB): 1314.01953125
Epoch: 9 cost time: 25.78020405769348
Epoch: 9, Steps: 90 | Train Loss: 0.2402124 Vali Loss: 0.3407805 Test Loss: 0.4107641
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.013107200000000006
Max Memory (MB): 1314.01953125
Epoch: 10 cost time: 25.32826519012451
Epoch: 10, Steps: 90 | Train Loss: 0.2396994 Vali Loss: 0.3322658 Test Loss: 0.4002849
Validation loss decreased (0.333601 --> 0.332266).  Saving model ...
Updating learning rate to 0.010485760000000004
Max Memory (MB): 1314.01953125
Epoch: 11 cost time: 25.585875511169434
Epoch: 11, Steps: 90 | Train Loss: 0.2376573 Vali Loss: 0.3331186 Test Loss: 0.3992604
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.008388608000000004
Max Memory (MB): 1314.01953125
Epoch: 12 cost time: 24.860868215560913
Epoch: 12, Steps: 90 | Train Loss: 0.2377510 Vali Loss: 0.3331677 Test Loss: 0.3991544
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.006710886400000004
Max Memory (MB): 1314.01953125
Epoch: 13 cost time: 24.81866145133972
Epoch: 13, Steps: 90 | Train Loss: 0.2375347 Vali Loss: 0.3331529 Test Loss: 0.4003484
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.005368709120000003
Max Memory (MB): 1314.01953125
Epoch: 14 cost time: 24.702767848968506
Epoch: 14, Steps: 90 | Train Loss: 0.2370559 Vali Loss: 0.3319244 Test Loss: 0.3995846
Validation loss decreased (0.332266 --> 0.331924).  Saving model ...
Updating learning rate to 0.0042949672960000025
Max Memory (MB): 1314.01953125
Epoch: 15 cost time: 24.840446949005127
Epoch: 15, Steps: 90 | Train Loss: 0.2371466 Vali Loss: 0.3322629 Test Loss: 0.3992973
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0034359738368000023
Max Memory (MB): 1314.01953125
Epoch: 16 cost time: 24.536890029907227
Epoch: 16, Steps: 90 | Train Loss: 0.2364780 Vali Loss: 0.3311133 Test Loss: 0.3986563
Validation loss decreased (0.331924 --> 0.331113).  Saving model ...
Updating learning rate to 0.002748779069440002
Max Memory (MB): 1314.01953125
Epoch: 17 cost time: 25.18612313270569
Epoch: 17, Steps: 90 | Train Loss: 0.2362229 Vali Loss: 0.3304809 Test Loss: 0.3981043
Validation loss decreased (0.331113 --> 0.330481).  Saving model ...
Updating learning rate to 0.002199023255552002
Max Memory (MB): 1314.01953125
Epoch: 18 cost time: 25.088905572891235
Epoch: 18, Steps: 90 | Train Loss: 0.2363072 Vali Loss: 0.3307298 Test Loss: 0.3984320
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0017592186044416017
Max Memory (MB): 1314.01953125
Epoch: 19 cost time: 25.178667783737183
Epoch: 19, Steps: 90 | Train Loss: 0.2363588 Vali Loss: 0.3311043 Test Loss: 0.3992979
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0014073748835532812
Max Memory (MB): 1314.01953125
Epoch: 20 cost time: 24.98844313621521
Epoch: 20, Steps: 90 | Train Loss: 0.2361922 Vali Loss: 0.3309643 Test Loss: 0.3977228
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0011258999068426252
Max Memory (MB): 1314.01953125
Epoch: 21 cost time: 24.69853901863098
Epoch: 21, Steps: 90 | Train Loss: 0.2359724 Vali Loss: 0.3301284 Test Loss: 0.3975911
Validation loss decreased (0.330481 --> 0.330128).  Saving model ...
Updating learning rate to 0.0009007199254741002
Max Memory (MB): 1314.01953125
Epoch: 22 cost time: 25.094719886779785
Epoch: 22, Steps: 90 | Train Loss: 0.2359068 Vali Loss: 0.3306348 Test Loss: 0.3971716
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0007205759403792802
Max Memory (MB): 1314.01953125
Epoch: 23 cost time: 25.021867513656616
Epoch: 23, Steps: 90 | Train Loss: 0.2357465 Vali Loss: 0.3304124 Test Loss: 0.3979326
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0005764607523034242
Max Memory (MB): 1314.01953125
Epoch: 24 cost time: 25.067123889923096
Epoch: 24, Steps: 90 | Train Loss: 0.2357899 Vali Loss: 0.3308579 Test Loss: 0.3973390
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00046116860184273935
Max Memory (MB): 1314.01953125
Epoch: 25 cost time: 24.883109092712402
Epoch: 25, Steps: 90 | Train Loss: 0.2358439 Vali Loss: 0.3308002 Test Loss: 0.3973770
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0003689348814741915
Max Memory (MB): 1314.01953125
Epoch: 26 cost time: 25.273861169815063
Epoch: 26, Steps: 90 | Train Loss: 0.2357911 Vali Loss: 0.3303935 Test Loss: 0.3973742
EarlyStopping counter: 5 out of 5
Early stopping
Final Max Memory (MB): 1314.01953125
>>>>>>>testing : traffic_720_96_LightTimeBaseTST_custom_ftM_sl720_pl96_test_0_seed2023<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.39420831203460693, mae:0.26799294352531433, rse:0.5198958516120911
