Args in experiment:
Namespace(is_training=1, model_id='weather_720_720', model='LightTimeBaseTST', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=720, period_len=4, basis_num=6, use_period_norm=1, use_orthogonal=1, orthogonal_weight=0.04, fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=7, individual=0, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='learned', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=30, batch_size=512, patience=5, learning_rate=0.05, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=3, use_multi_gpu=0, devices='0,1', test_flop=False)
Use GPU: cuda:3
2346
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
MACs: 181440.0
Params: 2346.0
181440.0 MACs
>>>>>>>start training : weather_720_720_LightTimeBaseTST_custom_ftM_sl720_pl720_test_0_seed2023>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Max Memory (MB): 284.87646484375
Epoch: 1 cost time: 5.135833740234375
Epoch: 1, Steps: 70 | Train Loss: 0.7995681 Vali Loss: 0.7666121 Test Loss: 0.3944227
Validation loss decreased (inf --> 0.766612).  Saving model ...
Updating learning rate to 0.05
Max Memory (MB): 284.87646484375
Epoch: 2 cost time: 5.271068572998047
Epoch: 2, Steps: 70 | Train Loss: 0.6655981 Vali Loss: 0.6611816 Test Loss: 0.3520986
Validation loss decreased (0.766612 --> 0.661182).  Saving model ...
Updating learning rate to 0.05
Max Memory (MB): 284.87646484375
Epoch: 3 cost time: 5.262495517730713
Epoch: 3, Steps: 70 | Train Loss: 0.6175148 Vali Loss: 0.6435639 Test Loss: 0.3470449
Validation loss decreased (0.661182 --> 0.643564).  Saving model ...
Updating learning rate to 0.05
Max Memory (MB): 284.87646484375
Epoch: 4 cost time: 5.392307281494141
Epoch: 4, Steps: 70 | Train Loss: 0.6130817 Vali Loss: 0.6486806 Test Loss: 0.3509196
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.04000000000000001
Max Memory (MB): 284.87646484375
Epoch: 5 cost time: 5.142139434814453
Epoch: 5, Steps: 70 | Train Loss: 0.6126259 Vali Loss: 0.6378383 Test Loss: 0.3389405
Validation loss decreased (0.643564 --> 0.637838).  Saving model ...
Updating learning rate to 0.03200000000000001
Max Memory (MB): 284.87646484375
Epoch: 6 cost time: 5.060522556304932
Epoch: 6, Steps: 70 | Train Loss: 0.6065727 Vali Loss: 0.6319169 Test Loss: 0.3375199
Validation loss decreased (0.637838 --> 0.631917).  Saving model ...
Updating learning rate to 0.025600000000000008
Max Memory (MB): 284.87646484375
Epoch: 7 cost time: 5.599512100219727
Epoch: 7, Steps: 70 | Train Loss: 0.6061273 Vali Loss: 0.6258965 Test Loss: 0.3344814
Validation loss decreased (0.631917 --> 0.625896).  Saving model ...
Updating learning rate to 0.020480000000000005
Max Memory (MB): 284.87646484375
Epoch: 8 cost time: 5.024196147918701
Epoch: 8, Steps: 70 | Train Loss: 0.6037945 Vali Loss: 0.6340584 Test Loss: 0.3376985
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.016384000000000006
Max Memory (MB): 284.87646484375
Epoch: 9 cost time: 5.462632179260254
Epoch: 9, Steps: 70 | Train Loss: 0.6010267 Vali Loss: 0.6237052 Test Loss: 0.3347165
Validation loss decreased (0.625896 --> 0.623705).  Saving model ...
Updating learning rate to 0.013107200000000006
Max Memory (MB): 284.87646484375
Epoch: 10 cost time: 5.132837772369385
Epoch: 10, Steps: 70 | Train Loss: 0.6005952 Vali Loss: 0.6284005 Test Loss: 0.3415099
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.010485760000000004
Max Memory (MB): 284.87646484375
Epoch: 11 cost time: 5.022064924240112
Epoch: 11, Steps: 70 | Train Loss: 0.6004356 Vali Loss: 0.6230239 Test Loss: 0.3375422
Validation loss decreased (0.623705 --> 0.623024).  Saving model ...
Updating learning rate to 0.008388608000000004
Max Memory (MB): 284.87646484375
Epoch: 12 cost time: 5.355542182922363
Epoch: 12, Steps: 70 | Train Loss: 0.5992247 Vali Loss: 0.6309568 Test Loss: 0.3379808
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.006710886400000004
Max Memory (MB): 284.87646484375
Epoch: 13 cost time: 5.233212471008301
Epoch: 13, Steps: 70 | Train Loss: 0.5975761 Vali Loss: 0.6238509 Test Loss: 0.3358272
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.005368709120000003
Max Memory (MB): 284.87646484375
Epoch: 14 cost time: 5.23714280128479
Epoch: 14, Steps: 70 | Train Loss: 0.5964657 Vali Loss: 0.6236610 Test Loss: 0.3353082
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0042949672960000025
Max Memory (MB): 284.87646484375
Epoch: 15 cost time: 5.482772350311279
Epoch: 15, Steps: 70 | Train Loss: 0.5958093 Vali Loss: 0.6228668 Test Loss: 0.3338378
Validation loss decreased (0.623024 --> 0.622867).  Saving model ...
Updating learning rate to 0.0034359738368000023
Max Memory (MB): 284.87646484375
Epoch: 16 cost time: 5.445083856582642
Epoch: 16, Steps: 70 | Train Loss: 0.5952554 Vali Loss: 0.6205334 Test Loss: 0.3346521
Validation loss decreased (0.622867 --> 0.620533).  Saving model ...
Updating learning rate to 0.002748779069440002
Max Memory (MB): 284.87646484375
Epoch: 17 cost time: 5.4066667556762695
Epoch: 17, Steps: 70 | Train Loss: 0.5954377 Vali Loss: 0.6215779 Test Loss: 0.3339931
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.002199023255552002
Max Memory (MB): 284.87646484375
Epoch: 18 cost time: 5.10297417640686
Epoch: 18, Steps: 70 | Train Loss: 0.5944724 Vali Loss: 0.6229718 Test Loss: 0.3342950
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0017592186044416017
Max Memory (MB): 284.87646484375
Epoch: 19 cost time: 5.412485122680664
Epoch: 19, Steps: 70 | Train Loss: 0.5944939 Vali Loss: 0.6197838 Test Loss: 0.3338902
Validation loss decreased (0.620533 --> 0.619784).  Saving model ...
Updating learning rate to 0.0014073748835532812
Max Memory (MB): 284.87646484375
Epoch: 20 cost time: 5.315495014190674
Epoch: 20, Steps: 70 | Train Loss: 0.5943404 Vali Loss: 0.6215926 Test Loss: 0.3344406
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0011258999068426252
Max Memory (MB): 284.87646484375
Epoch: 21 cost time: 5.270094156265259
Epoch: 21, Steps: 70 | Train Loss: 0.5933918 Vali Loss: 0.6190778 Test Loss: 0.3333361
Validation loss decreased (0.619784 --> 0.619078).  Saving model ...
Updating learning rate to 0.0009007199254741002
Max Memory (MB): 284.87646484375
Epoch: 22 cost time: 5.225106954574585
Epoch: 22, Steps: 70 | Train Loss: 0.5933652 Vali Loss: 0.6193443 Test Loss: 0.3333691
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0007205759403792802
Max Memory (MB): 284.87646484375
Epoch: 23 cost time: 5.2020103931427
Epoch: 23, Steps: 70 | Train Loss: 0.5932558 Vali Loss: 0.6191140 Test Loss: 0.3339255
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0005764607523034242
Max Memory (MB): 284.87646484375
Epoch: 24 cost time: 5.183056354522705
Epoch: 24, Steps: 70 | Train Loss: 0.5942223 Vali Loss: 0.6203761 Test Loss: 0.3335361
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00046116860184273935
Max Memory (MB): 284.87646484375
Epoch: 25 cost time: 5.182610511779785
Epoch: 25, Steps: 70 | Train Loss: 0.5933676 Vali Loss: 0.6206022 Test Loss: 0.3340810
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0003689348814741915
Max Memory (MB): 284.87646484375
Epoch: 26 cost time: 5.317745923995972
Epoch: 26, Steps: 70 | Train Loss: 0.5921864 Vali Loss: 0.6211268 Test Loss: 0.3334779
EarlyStopping counter: 5 out of 5
Early stopping
Final Max Memory (MB): 284.87646484375
>>>>>>>testing : weather_720_720_LightTimeBaseTST_custom_ftM_sl720_pl720_test_0_seed2023<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3233168125152588, mae:0.3439978063106537, rse:0.7482491135597229
