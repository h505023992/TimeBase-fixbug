Args in experiment:
Namespace(is_training=1, model_id='traffic_336_720', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=130, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_336_720_FITS_custom_ftM_sl336_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=130, out_features=408, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5852221440.0
params:  53448.0
Trainable parameters:  53448
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 73.03492712974548
Epoch: 1, Steps: 87 | Train Loss: 1.4737705 Vali Loss: 1.3823069 Test Loss: 1.6614780
Validation loss decreased (inf --> 1.382307).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 73.38740038871765
Epoch: 2, Steps: 87 | Train Loss: 0.9443285 Vali Loss: 1.1017816 Test Loss: 1.3172361
Validation loss decreased (1.382307 --> 1.101782).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 70.55441641807556
Epoch: 3, Steps: 87 | Train Loss: 0.7903162 Vali Loss: 1.0057080 Test Loss: 1.1994610
Validation loss decreased (1.101782 --> 1.005708).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 72.37829971313477
Epoch: 4, Steps: 87 | Train Loss: 0.7190035 Vali Loss: 0.9443446 Test Loss: 1.1249406
Validation loss decreased (1.005708 --> 0.944345).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 70.09033870697021
Epoch: 5, Steps: 87 | Train Loss: 0.6669025 Vali Loss: 0.8937614 Test Loss: 1.0624650
Validation loss decreased (0.944345 --> 0.893761).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 75.78678798675537
Epoch: 6, Steps: 87 | Train Loss: 0.6231911 Vali Loss: 0.8495133 Test Loss: 1.0080792
Validation loss decreased (0.893761 --> 0.849513).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 72.67707800865173
Epoch: 7, Steps: 87 | Train Loss: 0.5855104 Vali Loss: 0.8107659 Test Loss: 0.9598765
Validation loss decreased (0.849513 --> 0.810766).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 68.77122592926025
Epoch: 8, Steps: 87 | Train Loss: 0.5525526 Vali Loss: 0.7761829 Test Loss: 0.9179195
Validation loss decreased (0.810766 --> 0.776183).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 72.12289190292358
Epoch: 9, Steps: 87 | Train Loss: 0.5235911 Vali Loss: 0.7452101 Test Loss: 0.8800751
Validation loss decreased (0.776183 --> 0.745210).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 72.24103307723999
Epoch: 10, Steps: 87 | Train Loss: 0.4978639 Vali Loss: 0.7186163 Test Loss: 0.8469258
Validation loss decreased (0.745210 --> 0.718616).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 73.55439305305481
Epoch: 11, Steps: 87 | Train Loss: 0.4752362 Vali Loss: 0.6940836 Test Loss: 0.8176968
Validation loss decreased (0.718616 --> 0.694084).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 72.22976756095886
Epoch: 12, Steps: 87 | Train Loss: 0.4547990 Vali Loss: 0.6723964 Test Loss: 0.7911285
Validation loss decreased (0.694084 --> 0.672396).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 72.09349513053894
Epoch: 13, Steps: 87 | Train Loss: 0.4367050 Vali Loss: 0.6532702 Test Loss: 0.7676893
Validation loss decreased (0.672396 --> 0.653270).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 71.33103156089783
Epoch: 14, Steps: 87 | Train Loss: 0.4203330 Vali Loss: 0.6358521 Test Loss: 0.7469825
Validation loss decreased (0.653270 --> 0.635852).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 73.85910034179688
Epoch: 15, Steps: 87 | Train Loss: 0.4055636 Vali Loss: 0.6200910 Test Loss: 0.7273001
Validation loss decreased (0.635852 --> 0.620091).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 73.2278094291687
Epoch: 16, Steps: 87 | Train Loss: 0.3922802 Vali Loss: 0.6060922 Test Loss: 0.7101992
Validation loss decreased (0.620091 --> 0.606092).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 72.98036336898804
Epoch: 17, Steps: 87 | Train Loss: 0.3801807 Vali Loss: 0.5932415 Test Loss: 0.6949248
Validation loss decreased (0.606092 --> 0.593242).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 73.87362885475159
Epoch: 18, Steps: 87 | Train Loss: 0.3692318 Vali Loss: 0.5817357 Test Loss: 0.6808081
Validation loss decreased (0.593242 --> 0.581736).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 72.47331357002258
Epoch: 19, Steps: 87 | Train Loss: 0.3592930 Vali Loss: 0.5710400 Test Loss: 0.6682019
Validation loss decreased (0.581736 --> 0.571040).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 70.94494438171387
Epoch: 20, Steps: 87 | Train Loss: 0.3502094 Vali Loss: 0.5609293 Test Loss: 0.6564150
Validation loss decreased (0.571040 --> 0.560929).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 73.61602210998535
Epoch: 21, Steps: 87 | Train Loss: 0.3419027 Vali Loss: 0.5520256 Test Loss: 0.6454199
Validation loss decreased (0.560929 --> 0.552026).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 71.29719567298889
Epoch: 22, Steps: 87 | Train Loss: 0.3342573 Vali Loss: 0.5442927 Test Loss: 0.6355250
Validation loss decreased (0.552026 --> 0.544293).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 71.31308102607727
Epoch: 23, Steps: 87 | Train Loss: 0.3272714 Vali Loss: 0.5364492 Test Loss: 0.6267642
Validation loss decreased (0.544293 --> 0.536449).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 73.1243634223938
Epoch: 24, Steps: 87 | Train Loss: 0.3208904 Vali Loss: 0.5299411 Test Loss: 0.6186162
Validation loss decreased (0.536449 --> 0.529941).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 74.70219564437866
Epoch: 25, Steps: 87 | Train Loss: 0.3149468 Vali Loss: 0.5233061 Test Loss: 0.6110108
Validation loss decreased (0.529941 --> 0.523306).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 74.21917772293091
Epoch: 26, Steps: 87 | Train Loss: 0.3095102 Vali Loss: 0.5176873 Test Loss: 0.6041852
Validation loss decreased (0.523306 --> 0.517687).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 74.01775336265564
Epoch: 27, Steps: 87 | Train Loss: 0.3044593 Vali Loss: 0.5119902 Test Loss: 0.5975449
Validation loss decreased (0.517687 --> 0.511990).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 74.35683107376099
Epoch: 28, Steps: 87 | Train Loss: 0.2998213 Vali Loss: 0.5072263 Test Loss: 0.5917403
Validation loss decreased (0.511990 --> 0.507226).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 72.9787278175354
Epoch: 29, Steps: 87 | Train Loss: 0.2954436 Vali Loss: 0.5028273 Test Loss: 0.5863101
Validation loss decreased (0.507226 --> 0.502827).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 73.24013805389404
Epoch: 30, Steps: 87 | Train Loss: 0.2914859 Vali Loss: 0.4984936 Test Loss: 0.5812268
Validation loss decreased (0.502827 --> 0.498494).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11225
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=130, out_features=408, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5852221440.0
params:  53448.0
Trainable parameters:  53448
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 70.23139643669128
Epoch: 1, Steps: 87 | Train Loss: 0.3450834 Vali Loss: 0.4396527 Test Loss: 0.5114560
Validation loss decreased (inf --> 0.439653).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 70.98290348052979
Epoch: 2, Steps: 87 | Train Loss: 0.3100516 Vali Loss: 0.4157766 Test Loss: 0.4836836
Validation loss decreased (0.439653 --> 0.415777).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 72.97441029548645
Epoch: 3, Steps: 87 | Train Loss: 0.2948881 Vali Loss: 0.4058135 Test Loss: 0.4714392
Validation loss decreased (0.415777 --> 0.405814).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 71.9974639415741
Epoch: 4, Steps: 87 | Train Loss: 0.2887550 Vali Loss: 0.4018176 Test Loss: 0.4672150
Validation loss decreased (0.405814 --> 0.401818).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 73.34212970733643
Epoch: 5, Steps: 87 | Train Loss: 0.2863333 Vali Loss: 0.4008976 Test Loss: 0.4656867
Validation loss decreased (0.401818 --> 0.400898).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 73.96249985694885
Epoch: 6, Steps: 87 | Train Loss: 0.2856913 Vali Loss: 0.4005000 Test Loss: 0.4657549
Validation loss decreased (0.400898 --> 0.400500).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 73.35913825035095
Epoch: 7, Steps: 87 | Train Loss: 0.2854244 Vali Loss: 0.4005724 Test Loss: 0.4654275
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 71.5443787574768
Epoch: 8, Steps: 87 | Train Loss: 0.2853416 Vali Loss: 0.4011970 Test Loss: 0.4654746
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 75.0419352054596
Epoch: 9, Steps: 87 | Train Loss: 0.2852088 Vali Loss: 0.4003603 Test Loss: 0.4658488
Validation loss decreased (0.400500 --> 0.400360).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 67.30514693260193
Epoch: 10, Steps: 87 | Train Loss: 0.2852057 Vali Loss: 0.4001513 Test Loss: 0.4654197
Validation loss decreased (0.400360 --> 0.400151).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 64.42759227752686
Epoch: 11, Steps: 87 | Train Loss: 0.2851985 Vali Loss: 0.4003191 Test Loss: 0.4650492
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 64.09247350692749
Epoch: 12, Steps: 87 | Train Loss: 0.2851933 Vali Loss: 0.4008851 Test Loss: 0.4654159
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 63.81483268737793
Epoch: 13, Steps: 87 | Train Loss: 0.2851624 Vali Loss: 0.4010316 Test Loss: 0.4652750
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 64.81380915641785
Epoch: 14, Steps: 87 | Train Loss: 0.2850480 Vali Loss: 0.4004573 Test Loss: 0.4651439
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 64.74950933456421
Epoch: 15, Steps: 87 | Train Loss: 0.2850800 Vali Loss: 0.4005281 Test Loss: 0.4652063
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_336_720_FITS_custom_ftM_sl336_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4647239148616791, mae:0.30777353048324585, rse:0.5574402213096619, corr:[0.2556467  0.26612306 0.26692837 0.2658995  0.26786044 0.26700827
 0.26695707 0.26775932 0.26719132 0.26766613 0.26755762 0.26671246
 0.2672735  0.26684934 0.2661019  0.2667081  0.2666707  0.26670787
 0.26732346 0.26706877 0.26699662 0.2669214  0.2662869  0.2669151
 0.2677531  0.2673429  0.26767558 0.26790255 0.267689   0.26771942
 0.26732075 0.26703274 0.2673092  0.26695633 0.2666131  0.2666655
 0.26640332 0.26658425 0.26688513 0.26654962 0.2668047  0.2673323
 0.26716194 0.26706815 0.2669413  0.26661402 0.26665753 0.26665944
 0.26671404 0.2671173  0.26729608 0.26736692 0.26776585 0.26747227
 0.26694363 0.2671201  0.26710603 0.26691714 0.26688686 0.26639464
 0.26626432 0.2666006  0.26647717 0.26659468 0.26696372 0.266776
 0.26674995 0.26695597 0.2667653  0.26651514 0.26612017 0.26598918
 0.26633957 0.26613295 0.26561797 0.26569206 0.26568964 0.26548
 0.2656015  0.26565915 0.2657953  0.2662421  0.2661893  0.26592743
 0.26600498 0.2660103  0.2661034  0.26618704 0.26589194 0.26575786
 0.26594833 0.26579928 0.26592115 0.26616156 0.26598367 0.2659448
 0.26586494 0.2656925  0.26568273 0.26548684 0.26554918 0.26592293
 0.2656514  0.26557392 0.26620975 0.2661509  0.26569378 0.26601264
 0.26640666 0.26654315 0.2665275  0.26612484 0.26590538 0.26585498
 0.26580358 0.26599148 0.26598424 0.26558316 0.26590624 0.26624045
 0.2659844  0.2661819  0.26640067 0.26593563 0.26580563 0.26575074
 0.26541185 0.26547143 0.26547006 0.26526874 0.26551443 0.2658085
 0.26602492 0.2663709  0.2662253  0.26597017 0.2661613  0.26578644
 0.26575533 0.26604265 0.2657769  0.26533514 0.2654361  0.265893
 0.26650828 0.26678678 0.26654026 0.26637244 0.26647097 0.26630637
 0.26625475 0.26651886 0.26662442 0.26665646 0.26670477 0.26676646
 0.2669807  0.26725304 0.26746824 0.26738486 0.2669103  0.2665355
 0.2664181  0.26638922 0.2666002  0.26663974 0.2666177  0.26806578
 0.26924452 0.26828185 0.26797077 0.26826832 0.26812387 0.2680078
 0.268021   0.26770183 0.2678309  0.26812008 0.26782387 0.2678795
 0.26848412 0.26866946 0.26847506 0.26834333 0.26858643 0.26912475
 0.26906863 0.26849544 0.26818636 0.2679373  0.2676613  0.26794583
 0.26826477 0.26817983 0.2681471  0.26821753 0.26836467 0.268288
 0.26794097 0.2678786  0.2680549  0.26809126 0.26817673 0.268094
 0.26806578 0.26868165 0.26900488 0.26853216 0.26843926 0.26865917
 0.26849064 0.26822817 0.26774406 0.26714316 0.26695237 0.2669636
 0.26705357 0.2673591  0.26748657 0.26765212 0.26811805 0.26821074
 0.2680777  0.26808888 0.26780623 0.26760226 0.26776314 0.26761845
 0.26761717 0.2680045  0.26792097 0.26767632 0.26776022 0.26764947
 0.26755252 0.26778078 0.26783934 0.26758572 0.26734746 0.26724938
 0.26733    0.2673057  0.26717982 0.26745352 0.26784346 0.26782024
 0.26780847 0.26812097 0.26833224 0.2682213  0.26794302 0.26787144
 0.267876   0.26753178 0.26722512 0.26724908 0.2672363  0.26725867
 0.2673963  0.26725304 0.26711622 0.26717326 0.26708964 0.26711488
 0.26736483 0.26738247 0.26709718 0.2670593  0.26732025 0.2674531
 0.267594   0.2681122  0.26820832 0.26773903 0.26766923 0.26803657
 0.2679825  0.26765496 0.26752484 0.2673256  0.26695076 0.26669896
 0.26691067 0.26707137 0.2668054  0.2665045  0.26654503 0.2664824
 0.266528   0.2670566  0.26722056 0.26688293 0.26690426 0.2668318
 0.26667622 0.2671112  0.26733062 0.26710176 0.26721695 0.26747492
 0.26765528 0.26778328 0.26757538 0.2674331  0.2674696  0.26706496
 0.2671601  0.2677065  0.26777855 0.26755163 0.26750147 0.26739535
 0.26733214 0.26735127 0.26731923 0.26735172 0.26745254 0.26740474
 0.26767159 0.26805067 0.267979   0.26794425 0.26798782 0.26778075
 0.26767832 0.26749772 0.26727068 0.26774848 0.26828745 0.26804167
 0.26785228 0.2679972  0.26804158 0.2680354  0.26823834 0.2693551
 0.27005205 0.26944098 0.269134   0.2689476  0.26878488 0.26919308
 0.26959682 0.26921728 0.2689561  0.26921216 0.26934513 0.26927733
 0.26914856 0.26903057 0.2690589  0.2690567  0.269173   0.26951897
 0.26950696 0.26914167 0.2689832  0.26883996 0.26855963 0.26882192
 0.26926693 0.26925406 0.26916185 0.26912072 0.2690809  0.26923263
 0.26955536 0.26980773 0.26974446 0.2694714  0.26947764 0.2695339
 0.26929036 0.26914203 0.26893455 0.268651   0.2688834  0.26909724
 0.26875567 0.26848894 0.26841587 0.26848242 0.26858923 0.2683735
 0.2682219  0.26844406 0.26847422 0.26845357 0.26866534 0.26856083
 0.26847684 0.26887956 0.2691161  0.26920786 0.26943913 0.26915216
 0.2685553  0.26840046 0.26835838 0.2682599  0.26840454 0.2685227
 0.26863408 0.26878834 0.26854092 0.2682917  0.26833996 0.268338
 0.26821375 0.26799092 0.26782063 0.268098   0.2682686  0.26801202
 0.26802346 0.2682544  0.2684155  0.26873425 0.26883012 0.26844063
 0.2680358  0.26778793 0.26775092 0.26780078 0.26772594 0.2678927
 0.2683113  0.268213   0.26793578 0.26793054 0.26792923 0.26821133
 0.26846826 0.2682483  0.26813248 0.2682762  0.2680135  0.26766136
 0.26777983 0.26818776 0.26840925 0.26819617 0.26796103 0.2679609
 0.2678156  0.26774883 0.26794246 0.26774904 0.26729476 0.26716277
 0.2672381  0.2672448  0.2673999  0.2674136  0.26740682 0.2675345
 0.2675222  0.26752037 0.26760852 0.26738754 0.26718035 0.2671226
 0.26692587 0.26691929 0.26709017 0.26699397 0.26697776 0.26728845
 0.267616   0.26785207 0.26779768 0.26749066 0.26735193 0.26725903
 0.2672722  0.26768893 0.2680085  0.26807573 0.2683309  0.26868355
 0.26891333 0.2688151  0.26841843 0.2679014  0.26770127 0.26761934
 0.26759928 0.26764628 0.26765856 0.267817   0.2679445  0.2678484
 0.26798984 0.26815003 0.26780888 0.26756415 0.2676294  0.26764372
 0.2677745  0.26808575 0.26814884 0.26810324 0.26845264 0.26950714
 0.26992598 0.2689926  0.26858592 0.26853955 0.26840937 0.26842907
 0.2685412  0.26828906 0.2682734  0.26856297 0.26826373 0.26786837
 0.26799923 0.2678894  0.26757982 0.26779    0.26823816 0.2684374
 0.26835516 0.2681713  0.2681588  0.26818803 0.26804915 0.2682861
 0.26879507 0.2687576  0.2686074  0.2683759  0.26801074 0.26799142
 0.26839346 0.26857314 0.26827055 0.26801434 0.2682616  0.26841974
 0.268084   0.26791832 0.2678537  0.26773378 0.2680076  0.26793796
 0.26728335 0.26733896 0.26774895 0.2676839  0.26771638 0.26779288
 0.2675252  0.26744148 0.2675029  0.2674508  0.2675653  0.26746607
 0.26737082 0.2678482  0.26786432 0.26725563 0.26728025 0.26752734
 0.2672558  0.26699123 0.26700333 0.267286   0.26749277 0.26697132
 0.26635715 0.26627696 0.26619792 0.26625773 0.26653427 0.26655167
 0.26662412 0.26679048 0.2666457  0.26670828 0.2670168  0.266808
 0.26652732 0.26660368 0.2665064  0.26626328 0.26612633 0.2660975
 0.26611763 0.26580933 0.2655617  0.26576886 0.26555997 0.2651707
 0.2653217  0.26515293 0.26482677 0.26522845 0.26562023 0.26577303
 0.26589265 0.26574296 0.26583186 0.26597062 0.2656217  0.26557845
 0.2659219  0.2657856  0.26543424 0.26535374 0.26545125 0.26552206
 0.26531252 0.2651074  0.26506194 0.26476163 0.26465216 0.26492146
 0.26470524 0.26428244 0.2641647  0.26394808 0.2642202  0.2648411
 0.26471063 0.26454446 0.26494828 0.2649252  0.26489872 0.26526988
 0.2654288  0.26544383 0.26503208 0.26426798 0.26434332 0.26488692
 0.26508784 0.26527417 0.26509094 0.26475212 0.26502207 0.26496443
 0.2645268  0.26482204 0.26495138 0.2647322  0.26510605 0.26544142
 0.26562172 0.26581782 0.26595286 0.26605263 0.26623192 0.2660408
 0.26608256 0.26621902 0.26574516 0.2654788  0.26549375 0.26526642
 0.2653734  0.26538345 0.2650642  0.2654216  0.26582056 0.26559237
 0.26554012 0.26559508 0.26574358 0.2664875  0.26678172 0.26712635
 0.26773956 0.26723418 0.26706198 0.26727685 0.2670963  0.26694685
 0.26694727 0.2666609  0.2670097  0.26712352 0.26646727 0.266865
 0.26734993 0.26689264 0.26714084 0.26746482 0.2673418  0.2678527
 0.2681442  0.26798284 0.26809463 0.26783532 0.2675544  0.26776847
 0.26731592 0.26697555 0.26768267 0.26763874 0.2677666  0.26775512
 0.26676598 0.267214   0.26773188 0.26704803 0.26775417 0.26779863
 0.26724574 0.26840517 0.26811665 0.26816714 0.26987803 0.26850715
 0.26794282 0.2682075  0.2652117  0.266305   0.2643261  0.26871926]
