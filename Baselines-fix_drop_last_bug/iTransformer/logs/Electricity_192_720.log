Args in experiment:
Namespace(is_training=1, model_id='ECL_192_720', model='iTransformer', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=720, enc_in=321, dec_in=321, c_out=321, d_model=512, n_heads=8, e_layers=3, d_layers=1, d_ff=512, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=64, patience=3, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=2, use_multi_gpu=False, devices='0,1,2,3', exp_name='MTSF', channel_independence=False, inverse=False, class_strategy='projection', target_root_path='./data/electricity/', target_data_path='electricity.csv', efficient_training=False, use_norm=True, partial_start_index=0)
Use GPU: cuda:2
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.
MACs: 1700358144.0
Params: 5203152.0
1.70G MACs
>>>>>>>start training : ECL_192_720_iTransformer_custom_M_ft192_sl48_ll720_pl512_dm8_nh3_el1_dl512_df1_fctimeF_ebTrue_dtExp_projection_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17501
val 1913
test 4541
	iters: 100, epoch: 1 | loss: 0.3241278
	speed: 0.1857s/iter; left time: 488.6712s
	iters: 200, epoch: 1 | loss: 0.2667962
	speed: 0.1572s/iter; left time: 397.9284s
Max Memory (MB): 3381.7255859375
Epoch: 1 cost time: 44.84247946739197
Epoch: 1, Steps: 273 | Train Loss: 0.3432260 Vali Loss: 0.2183577 Test Loss: 0.2498218
Validation loss decreased (inf --> 0.218358).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2766002
	speed: 1.0481s/iter; left time: 2471.4920s
	iters: 200, epoch: 2 | loss: 0.2633276
	speed: 0.1542s/iter; left time: 348.0749s
Max Memory (MB): 3381.7255859375
Epoch: 2 cost time: 43.93135976791382
Epoch: 2, Steps: 273 | Train Loss: 0.2630283 Vali Loss: 0.2036753 Test Loss: 0.2328221
Validation loss decreased (0.218358 --> 0.203675).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2562358
	speed: 1.3193s/iter; left time: 2750.6721s
	iters: 200, epoch: 3 | loss: 0.2498589
	speed: 0.1514s/iter; left time: 300.4784s
Max Memory (MB): 3381.7255859375
Epoch: 3 cost time: 44.83856201171875
Epoch: 3, Steps: 273 | Train Loss: 0.2517708 Vali Loss: 0.1983674 Test Loss: 0.2266948
Validation loss decreased (0.203675 --> 0.198367).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.2671593
	speed: 1.3252s/iter; left time: 2401.2480s
	iters: 200, epoch: 4 | loss: 0.2544413
	speed: 0.1540s/iter; left time: 263.5949s
Max Memory (MB): 3381.7255859375
Epoch: 4 cost time: 45.21322822570801
Epoch: 4, Steps: 273 | Train Loss: 0.2468588 Vali Loss: 0.1970276 Test Loss: 0.2241781
Validation loss decreased (0.198367 --> 0.197028).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.2367982
	speed: 1.3130s/iter; left time: 2020.6637s
	iters: 200, epoch: 5 | loss: 0.2560028
	speed: 0.1403s/iter; left time: 201.9183s
Max Memory (MB): 3381.7255859375
Epoch: 5 cost time: 42.82029438018799
Epoch: 5, Steps: 273 | Train Loss: 0.2437879 Vali Loss: 0.1958640 Test Loss: 0.2215349
Validation loss decreased (0.197028 --> 0.195864).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.2491838
	speed: 1.3198s/iter; left time: 1670.8166s
	iters: 200, epoch: 6 | loss: 0.2515163
	speed: 0.1439s/iter; left time: 167.8410s
Max Memory (MB): 3381.7255859375
Epoch: 6 cost time: 43.21009039878845
Epoch: 6, Steps: 273 | Train Loss: 0.2422131 Vali Loss: 0.1956343 Test Loss: 0.2201706
Validation loss decreased (0.195864 --> 0.195634).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.2392564
	speed: 1.6166s/iter; left time: 1605.2569s
	iters: 200, epoch: 7 | loss: 0.2360751
	speed: 0.1569s/iter; left time: 140.0782s
Max Memory (MB): 3381.7255859375
Epoch: 7 cost time: 45.207621335983276
Epoch: 7, Steps: 273 | Train Loss: 0.2412938 Vali Loss: 0.1953533 Test Loss: 0.2198402
Validation loss decreased (0.195634 --> 0.195353).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.2332797
	speed: 1.3409s/iter; left time: 965.4268s
	iters: 200, epoch: 8 | loss: 0.2409428
	speed: 0.1409s/iter; left time: 87.3364s
Max Memory (MB): 3381.7255859375
Epoch: 8 cost time: 42.3045015335083
Epoch: 8, Steps: 273 | Train Loss: 0.2408495 Vali Loss: 0.1951417 Test Loss: 0.2194760
Validation loss decreased (0.195353 --> 0.195142).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.2399789
	speed: 1.1703s/iter; left time: 523.1084s
	iters: 200, epoch: 9 | loss: 0.2348499
	speed: 0.1397s/iter; left time: 48.4907s
Max Memory (MB): 3381.7255859375
Epoch: 9 cost time: 41.24415564537048
Epoch: 9, Steps: 273 | Train Loss: 0.2406452 Vali Loss: 0.1954809 Test Loss: 0.2192111
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.2270539
	speed: 1.2919s/iter; left time: 224.7901s
	iters: 200, epoch: 10 | loss: 0.2259120
	speed: 0.1453s/iter; left time: 10.7528s
Max Memory (MB): 3381.7255859375
Epoch: 10 cost time: 43.11163592338562
Epoch: 10, Steps: 273 | Train Loss: 0.2404634 Vali Loss: 0.1953514 Test Loss: 0.2192493
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : ECL_192_720_iTransformer_custom_M_ft192_sl48_ll720_pl512_dm8_nh3_el1_dl512_df1_fctimeF_ebTrue_dtExp_projection_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
test shape: (4541, 1, 720, 321) (4541, 1, 720, 321)
test shape: (4541, 720, 321) (4541, 720, 321)
mse:0.21947583556175232, mae:0.3084319829940796
