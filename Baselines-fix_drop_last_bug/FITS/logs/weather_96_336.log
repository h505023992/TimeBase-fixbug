Args in experiment:
Namespace(is_training=1, model_id='weather_96_336', model='FITS', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=3, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=22, base_T=144, H_order=12)
Use GPU: cuda:0
>>>>>>>start training : weather_96_336_FITS_custom_ftM_sl96_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36456
val 4935
test 10204
Model(
  (freq_upsampler): Linear(in_features=22, out_features=99, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5854464.0
params:  2277.0
Trainable parameters:  2277
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.2357782
	speed: 0.1039s/iter; left time: 874.7446s
	iters: 200, epoch: 1 | loss: 0.6733826
	speed: 0.0836s/iter; left time: 695.7416s
Epoch: 1 cost time: 25.753849029541016
Epoch: 1, Steps: 284 | Train Loss: 0.8654912 Vali Loss: 0.7962105 Test Loss: 0.3294404
Validation loss decreased (inf --> 0.796210).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6297608
	speed: 0.3503s/iter; left time: 2849.9930s
	iters: 200, epoch: 2 | loss: 0.6194072
	speed: 0.0922s/iter; left time: 741.3815s
Epoch: 2 cost time: 26.817060232162476
Epoch: 2, Steps: 284 | Train Loss: 0.6126620 Vali Loss: 0.6953761 Test Loss: 0.3021178
Validation loss decreased (0.796210 --> 0.695376).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5703600
	speed: 0.3304s/iter; left time: 2594.8592s
	iters: 200, epoch: 3 | loss: 0.5971628
	speed: 0.0758s/iter; left time: 587.9018s
Epoch: 3 cost time: 22.00780987739563
Epoch: 3, Steps: 284 | Train Loss: 0.5521176 Vali Loss: 0.6729353 Test Loss: 0.2962473
Validation loss decreased (0.695376 --> 0.672935).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4722655
	speed: 0.3198s/iter; left time: 2420.7216s
	iters: 200, epoch: 4 | loss: 0.4899589
	speed: 0.0832s/iter; left time: 621.7317s
Epoch: 4 cost time: 25.46821618080139
Epoch: 4, Steps: 284 | Train Loss: 0.5342324 Vali Loss: 0.6628784 Test Loss: 0.2944843
Validation loss decreased (0.672935 --> 0.662878).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5805171
	speed: 0.3690s/iter; left time: 2688.0628s
	iters: 200, epoch: 5 | loss: 0.5187779
	speed: 0.0854s/iter; left time: 613.5034s
Epoch: 5 cost time: 25.13576579093933
Epoch: 5, Steps: 284 | Train Loss: 0.5264998 Vali Loss: 0.6565087 Test Loss: 0.2937738
Validation loss decreased (0.662878 --> 0.656509).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6789421
	speed: 0.3405s/iter; left time: 2383.5015s
	iters: 200, epoch: 6 | loss: 0.4919350
	speed: 0.0926s/iter; left time: 638.8446s
Epoch: 6 cost time: 26.609662771224976
Epoch: 6, Steps: 284 | Train Loss: 0.5223824 Vali Loss: 0.6543725 Test Loss: 0.2935905
Validation loss decreased (0.656509 --> 0.654372).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4530307
	speed: 0.3536s/iter; left time: 2374.9181s
	iters: 200, epoch: 7 | loss: 0.6359348
	speed: 0.0869s/iter; left time: 574.7575s
Epoch: 7 cost time: 26.18544101715088
Epoch: 7, Steps: 284 | Train Loss: 0.5193216 Vali Loss: 0.6540799 Test Loss: 0.2936373
Validation loss decreased (0.654372 --> 0.654080).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5289407
	speed: 0.3371s/iter; left time: 2168.7297s
	iters: 200, epoch: 8 | loss: 0.6290262
	speed: 0.0949s/iter; left time: 601.2414s
Epoch: 8 cost time: 26.428913831710815
Epoch: 8, Steps: 284 | Train Loss: 0.5183129 Vali Loss: 0.6516511 Test Loss: 0.2934844
Validation loss decreased (0.654080 --> 0.651651).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5147277
	speed: 0.3606s/iter; left time: 2217.1268s
	iters: 200, epoch: 9 | loss: 0.7005299
	speed: 0.0816s/iter; left time: 493.7328s
Epoch: 9 cost time: 24.982974767684937
Epoch: 9, Steps: 284 | Train Loss: 0.5173980 Vali Loss: 0.6522393 Test Loss: 0.2935322
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5224841
	speed: 0.3469s/iter; left time: 2034.8156s
	iters: 200, epoch: 10 | loss: 0.4086502
	speed: 0.0925s/iter; left time: 533.2731s
Epoch: 10 cost time: 25.904982805252075
Epoch: 10, Steps: 284 | Train Loss: 0.5171232 Vali Loss: 0.6504615 Test Loss: 0.2934950
Validation loss decreased (0.651651 --> 0.650461).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4687369
	speed: 0.3089s/iter; left time: 1724.2014s
	iters: 200, epoch: 11 | loss: 0.6526033
	speed: 0.0779s/iter; left time: 426.6998s
Epoch: 11 cost time: 24.382549047470093
Epoch: 11, Steps: 284 | Train Loss: 0.5158057 Vali Loss: 0.6524663 Test Loss: 0.2934409
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5308075
	speed: 0.3531s/iter; left time: 1870.4357s
	iters: 200, epoch: 12 | loss: 0.4505715
	speed: 0.0962s/iter; left time: 499.7302s
Epoch: 12 cost time: 26.619369506835938
Epoch: 12, Steps: 284 | Train Loss: 0.5166227 Vali Loss: 0.6515273 Test Loss: 0.2935527
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5815636
	speed: 0.3518s/iter; left time: 1763.6680s
	iters: 200, epoch: 13 | loss: 0.5076650
	speed: 0.0858s/iter; left time: 421.4323s
Epoch: 13 cost time: 25.328855991363525
Epoch: 13, Steps: 284 | Train Loss: 0.5167349 Vali Loss: 0.6486906 Test Loss: 0.2934546
Validation loss decreased (0.650461 --> 0.648691).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4720047
	speed: 0.3473s/iter; left time: 1642.5841s
	iters: 200, epoch: 14 | loss: 0.4691843
	speed: 0.0943s/iter; left time: 436.4630s
Epoch: 14 cost time: 27.517349004745483
Epoch: 14, Steps: 284 | Train Loss: 0.5164773 Vali Loss: 0.6511101 Test Loss: 0.2934457
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5960767
	speed: 0.3413s/iter; left time: 1517.1620s
	iters: 200, epoch: 15 | loss: 0.4929613
	speed: 0.0894s/iter; left time: 388.3992s
Epoch: 15 cost time: 25.157011032104492
Epoch: 15, Steps: 284 | Train Loss: 0.5161653 Vali Loss: 0.6482962 Test Loss: 0.2934563
Validation loss decreased (0.648691 --> 0.648296).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4257841
	speed: 0.3532s/iter; left time: 1469.7042s
	iters: 200, epoch: 16 | loss: 0.5518002
	speed: 0.0901s/iter; left time: 365.9241s
Epoch: 16 cost time: 27.128071069717407
Epoch: 16, Steps: 284 | Train Loss: 0.5162506 Vali Loss: 0.6500049 Test Loss: 0.2934304
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5492573
	speed: 0.3460s/iter; left time: 1341.6220s
	iters: 200, epoch: 17 | loss: 0.4477320
	speed: 0.0715s/iter; left time: 269.8913s
Epoch: 17 cost time: 21.78727388381958
Epoch: 17, Steps: 284 | Train Loss: 0.5164848 Vali Loss: 0.6505981 Test Loss: 0.2933177
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4467050
	speed: 0.3282s/iter; left time: 1179.2928s
	iters: 200, epoch: 18 | loss: 0.5189875
	speed: 0.0971s/iter; left time: 339.1527s
Epoch: 18 cost time: 28.29885172843933
Epoch: 18, Steps: 284 | Train Loss: 0.5159286 Vali Loss: 0.6505827 Test Loss: 0.2933840
EarlyStopping counter: 3 out of 3
Early stopping
train 36456
val 4935
test 10204
Model(
  (freq_upsampler): Linear(in_features=22, out_features=99, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5854464.0
params:  2277.0
Trainable parameters:  2277
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7436711
	speed: 0.0945s/iter; left time: 795.4065s
	iters: 200, epoch: 1 | loss: 0.6491216
	speed: 0.0922s/iter; left time: 767.3532s
Epoch: 1 cost time: 27.292887926101685
Epoch: 1, Steps: 284 | Train Loss: 0.6448717 Vali Loss: 0.6507019 Test Loss: 0.2936356
Validation loss decreased (inf --> 0.650702).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7606331
	speed: 0.3592s/iter; left time: 2923.0896s
	iters: 200, epoch: 2 | loss: 0.5853547
	speed: 0.0926s/iter; left time: 744.2820s
Epoch: 2 cost time: 28.29228401184082
Epoch: 2, Steps: 284 | Train Loss: 0.6445779 Vali Loss: 0.6489083 Test Loss: 0.2930530
Validation loss decreased (0.650702 --> 0.648908).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5915779
	speed: 0.3584s/iter; left time: 2814.8052s
	iters: 200, epoch: 3 | loss: 0.6305650
	speed: 0.0874s/iter; left time: 677.6583s
Epoch: 3 cost time: 26.392086505889893
Epoch: 3, Steps: 284 | Train Loss: 0.6443993 Vali Loss: 0.6495624 Test Loss: 0.2932789
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.7466350
	speed: 0.3579s/iter; left time: 2708.8737s
	iters: 200, epoch: 4 | loss: 0.5329980
	speed: 0.0925s/iter; left time: 690.8517s
Epoch: 4 cost time: 27.068730115890503
Epoch: 4, Steps: 284 | Train Loss: 0.6444822 Vali Loss: 0.6473474 Test Loss: 0.2931846
Validation loss decreased (0.648908 --> 0.647347).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6199346
	speed: 0.3384s/iter; left time: 2465.2722s
	iters: 200, epoch: 5 | loss: 0.7332973
	speed: 0.0971s/iter; left time: 697.3664s
Epoch: 5 cost time: 26.794092893600464
Epoch: 5, Steps: 284 | Train Loss: 0.6446058 Vali Loss: 0.6483024 Test Loss: 0.2932033
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6643162
	speed: 0.3463s/iter; left time: 2424.6867s
	iters: 200, epoch: 6 | loss: 0.5543845
	speed: 0.0852s/iter; left time: 587.7240s
Epoch: 6 cost time: 26.62772560119629
Epoch: 6, Steps: 284 | Train Loss: 0.6440363 Vali Loss: 0.6467623 Test Loss: 0.2929524
Validation loss decreased (0.647347 --> 0.646762).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.7849276
	speed: 0.3519s/iter; left time: 2363.6318s
	iters: 200, epoch: 7 | loss: 0.6254873
	speed: 0.0952s/iter; left time: 630.0093s
Epoch: 7 cost time: 26.063307285308838
Epoch: 7, Steps: 284 | Train Loss: 0.6438446 Vali Loss: 0.6468436 Test Loss: 0.2928550
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6624357
	speed: 0.3561s/iter; left time: 2290.9085s
	iters: 200, epoch: 8 | loss: 0.6396187
	speed: 0.0883s/iter; left time: 558.9345s
Epoch: 8 cost time: 26.24721336364746
Epoch: 8, Steps: 284 | Train Loss: 0.6440883 Vali Loss: 0.6483790 Test Loss: 0.2931000
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.7188003
	speed: 0.3072s/iter; left time: 1888.8796s
	iters: 200, epoch: 9 | loss: 0.5384496
	speed: 0.0845s/iter; left time: 511.2503s
Epoch: 9 cost time: 23.651113986968994
Epoch: 9, Steps: 284 | Train Loss: 0.6441698 Vali Loss: 0.6465915 Test Loss: 0.2930572
Validation loss decreased (0.646762 --> 0.646592).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5180307
	speed: 0.2961s/iter; left time: 1736.6129s
	iters: 200, epoch: 10 | loss: 0.7034705
	speed: 0.0799s/iter; left time: 460.5514s
Epoch: 10 cost time: 24.348783016204834
Epoch: 10, Steps: 284 | Train Loss: 0.6439142 Vali Loss: 0.6480094 Test Loss: 0.2928224
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6971382
	speed: 0.3383s/iter; left time: 1887.9634s
	iters: 200, epoch: 11 | loss: 0.5016773
	speed: 0.0863s/iter; left time: 473.1245s
Epoch: 11 cost time: 26.378702402114868
Epoch: 11, Steps: 284 | Train Loss: 0.6437877 Vali Loss: 0.6492394 Test Loss: 0.2927172
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6027315
	speed: 0.3593s/iter; left time: 1903.0533s
	iters: 200, epoch: 12 | loss: 0.5372208
	speed: 0.0908s/iter; left time: 472.0090s
Epoch: 12 cost time: 26.945908546447754
Epoch: 12, Steps: 284 | Train Loss: 0.6436667 Vali Loss: 0.6473904 Test Loss: 0.2928134
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : weather_96_336_FITS_custom_ftM_sl96_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10204
mse:0.29288074374198914, mae:0.3077772259712219, rse:0.7107729911804199, corr:[0.48252878 0.4815734  0.4797008  0.47862312 0.4772925  0.47522563
 0.47294098 0.47068155 0.46805    0.46502948 0.46224666 0.45945457
 0.45617938 0.45275462 0.44926226 0.445727   0.4420149  0.43797138
 0.43431962 0.4308842  0.4270585  0.42299855 0.41930526 0.41597283
 0.41274986 0.4097387  0.40775272 0.40655363 0.405703   0.40520975
 0.40547132 0.40652746 0.4079951  0.40935332 0.4109461  0.4124953
 0.4139422  0.415251   0.41643402 0.41743305 0.41832262 0.4191381
 0.4198272  0.4204191  0.42083907 0.4211831  0.4213842  0.42157835
 0.42140335 0.4210628  0.4207912  0.4205283  0.42043045 0.4205076
 0.42056698 0.4204618  0.4201828  0.41978663 0.4196428  0.41958013
 0.41938716 0.41916546 0.41888812 0.41868865 0.4184686  0.418221
 0.41783664 0.4174616  0.41723952 0.4169485  0.41675186 0.41644347
 0.41622937 0.41612643 0.41599986 0.4158266  0.4155313  0.4150133
 0.41454485 0.4141643  0.4140843  0.41405392 0.41373676 0.41326046
 0.41287166 0.41260138 0.412546   0.41217962 0.41179568 0.41148475
 0.41128445 0.4111938  0.4109786  0.41072235 0.41076913 0.41100493
 0.41117343 0.41110358 0.411111   0.41117573 0.4113078  0.41129535
 0.41121116 0.41117564 0.41124457 0.41128832 0.4111343  0.41103107
 0.41096142 0.4109999  0.41116464 0.4112569  0.411079   0.4108221
 0.4106209  0.41039386 0.4101917  0.41004896 0.40994084 0.40978262
 0.4094771  0.4092635  0.40907604 0.40897468 0.40889168 0.4086214
 0.40832096 0.4080089  0.40776813 0.40747178 0.40696847 0.40634635
 0.40576544 0.4052123  0.4048096  0.4044052  0.4039947  0.4036077
 0.40307087 0.40231833 0.40140522 0.40037    0.3994016  0.39833522
 0.39706776 0.39562297 0.3942264  0.3926238  0.3907659  0.38848725
 0.38595143 0.38317224 0.38033894 0.37729877 0.3738766  0.3700036
 0.36610985 0.36231506 0.35849407 0.35432684 0.35004032 0.34562722
 0.3411841  0.33683524 0.33278015 0.32935345 0.32623816 0.32369816
 0.32203224 0.32111752 0.3210531  0.32186785 0.3231246  0.3257181
 0.32846904 0.331639   0.33484453 0.338533   0.34197468 0.34485638
 0.3476561  0.3502155  0.35244682 0.35445207 0.3562334  0.35808513
 0.3598313  0.36138296 0.36279407 0.36442322 0.36595783 0.36726862
 0.3684029  0.3691126  0.36968613 0.37027624 0.37068644 0.37097222
 0.37126875 0.37152568 0.37180838 0.37214428 0.37236944 0.3723747
 0.37243003 0.37243274 0.37241912 0.37228802 0.3720058  0.37172207
 0.37144634 0.371137   0.3708367  0.3705544  0.37017447 0.36976713
 0.36934376 0.36899203 0.36876997 0.36848345 0.36805704 0.3677294
 0.36744598 0.36723208 0.36696327 0.36650547 0.36604875 0.3657954
 0.36565018 0.36538348 0.36503372 0.36465904 0.36429584 0.36396462
 0.36354816 0.3630636  0.3627262  0.36256889 0.36233467 0.36204726
 0.36191517 0.3619754  0.3619894  0.36203006 0.36218005 0.36243117
 0.36266887 0.36289054 0.36297464 0.36321107 0.36352974 0.36386615
 0.36410734 0.36425957 0.36443806 0.3647055  0.36460194 0.36435005
 0.3642659  0.36436993 0.36449924 0.36452976 0.36454955 0.36454573
 0.36471143 0.36481965 0.36485907 0.36489022 0.36497226 0.36490932
 0.36481217 0.3646995  0.3647182  0.3647144  0.3645493  0.36418414
 0.36377233 0.36341262 0.36310226 0.3626452  0.36197907 0.3612607
 0.36048752 0.3595784  0.35843098 0.35719916 0.35583407 0.35439616
 0.35269228 0.3508213  0.34866157 0.3462857  0.3439507  0.34168455
 0.33928496 0.33653373 0.33348563 0.33037212 0.32716754 0.32383537
 0.3201673  0.31636268 0.31240815 0.3086282  0.30492708 0.30132112
 0.29782248 0.29484913 0.29222313 0.28958437 0.28721577 0.28590956
 0.28545904 0.28563568 0.2860131  0.28735274 0.28965497 0.29229292
 0.29490682 0.29755127 0.3002316  0.30278674 0.30542323 0.3080084
 0.3107101  0.31285766 0.31486714 0.3168834  0.31950533 0.3220498
 0.3238499  0.32496604 0.3263727  0.3282643  0.32929385 0.32874987]
