Args in experiment:
Namespace(is_training=1, model_id='traffic_336_336', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=130, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_336_336_FITS_custom_ftM_sl336_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11609
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=130, out_features=260, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3729356800.0
params:  34060.0
Trainable parameters:  34060
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 51.20808029174805
Epoch: 1, Steps: 90 | Train Loss: 1.3049900 Vali Loss: 1.3604100 Test Loss: 1.5989538
Validation loss decreased (inf --> 1.360410).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 51.18963980674744
Epoch: 2, Steps: 90 | Train Loss: 0.9453329 Vali Loss: 1.1692326 Test Loss: 1.3710656
Validation loss decreased (1.360410 --> 1.169233).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 51.135608196258545
Epoch: 3, Steps: 90 | Train Loss: 0.8254819 Vali Loss: 1.0892037 Test Loss: 1.2767180
Validation loss decreased (1.169233 --> 1.089204).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 51.66700458526611
Epoch: 4, Steps: 90 | Train Loss: 0.7565743 Vali Loss: 1.0269141 Test Loss: 1.2040205
Validation loss decreased (1.089204 --> 1.026914).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 50.13640594482422
Epoch: 5, Steps: 90 | Train Loss: 0.7009297 Vali Loss: 0.9720604 Test Loss: 1.1397798
Validation loss decreased (1.026914 --> 0.972060).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 49.756428956985474
Epoch: 6, Steps: 90 | Train Loss: 0.6523595 Vali Loss: 0.9237303 Test Loss: 1.0825818
Validation loss decreased (0.972060 --> 0.923730).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 51.81720781326294
Epoch: 7, Steps: 90 | Train Loss: 0.6096195 Vali Loss: 0.8793333 Test Loss: 1.0306313
Validation loss decreased (0.923730 --> 0.879333).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 50.17657542228699
Epoch: 8, Steps: 90 | Train Loss: 0.5715308 Vali Loss: 0.8407513 Test Loss: 0.9852105
Validation loss decreased (0.879333 --> 0.840751).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 52.373130321502686
Epoch: 9, Steps: 90 | Train Loss: 0.5376815 Vali Loss: 0.8040769 Test Loss: 0.9430599
Validation loss decreased (0.840751 --> 0.804077).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 52.1753032207489
Epoch: 10, Steps: 90 | Train Loss: 0.5073899 Vali Loss: 0.7721698 Test Loss: 0.9051139
Validation loss decreased (0.804077 --> 0.772170).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 48.53708481788635
Epoch: 11, Steps: 90 | Train Loss: 0.4802415 Vali Loss: 0.7412460 Test Loss: 0.8693905
Validation loss decreased (0.772170 --> 0.741246).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 50.35673522949219
Epoch: 12, Steps: 90 | Train Loss: 0.4558830 Vali Loss: 0.7156079 Test Loss: 0.8390985
Validation loss decreased (0.741246 --> 0.715608).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 53.35490012168884
Epoch: 13, Steps: 90 | Train Loss: 0.4338505 Vali Loss: 0.6916150 Test Loss: 0.8112951
Validation loss decreased (0.715608 --> 0.691615).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 49.927961111068726
Epoch: 14, Steps: 90 | Train Loss: 0.4139829 Vali Loss: 0.6705498 Test Loss: 0.7865204
Validation loss decreased (0.691615 --> 0.670550).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 53.981122970581055
Epoch: 15, Steps: 90 | Train Loss: 0.3959741 Vali Loss: 0.6503993 Test Loss: 0.7635590
Validation loss decreased (0.670550 --> 0.650399).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 52.861948013305664
Epoch: 16, Steps: 90 | Train Loss: 0.3796304 Vali Loss: 0.6325445 Test Loss: 0.7425649
Validation loss decreased (0.650399 --> 0.632545).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 50.84807324409485
Epoch: 17, Steps: 90 | Train Loss: 0.3647722 Vali Loss: 0.6152318 Test Loss: 0.7226925
Validation loss decreased (0.632545 --> 0.615232).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 51.60874032974243
Epoch: 18, Steps: 90 | Train Loss: 0.3512116 Vali Loss: 0.6004025 Test Loss: 0.7056271
Validation loss decreased (0.615232 --> 0.600403).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 48.91496658325195
Epoch: 19, Steps: 90 | Train Loss: 0.3388530 Vali Loss: 0.5866538 Test Loss: 0.6896818
Validation loss decreased (0.600403 --> 0.586654).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 53.43231415748596
Epoch: 20, Steps: 90 | Train Loss: 0.3275104 Vali Loss: 0.5741394 Test Loss: 0.6752892
Validation loss decreased (0.586654 --> 0.574139).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 49.981706380844116
Epoch: 21, Steps: 90 | Train Loss: 0.3171563 Vali Loss: 0.5622171 Test Loss: 0.6618372
Validation loss decreased (0.574139 --> 0.562217).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 49.53593444824219
Epoch: 22, Steps: 90 | Train Loss: 0.3076088 Vali Loss: 0.5524417 Test Loss: 0.6500909
Validation loss decreased (0.562217 --> 0.552442).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 52.32830333709717
Epoch: 23, Steps: 90 | Train Loss: 0.2988587 Vali Loss: 0.5420967 Test Loss: 0.6388258
Validation loss decreased (0.552442 --> 0.542097).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 51.95242404937744
Epoch: 24, Steps: 90 | Train Loss: 0.2907875 Vali Loss: 0.5331157 Test Loss: 0.6281473
Validation loss decreased (0.542097 --> 0.533116).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 48.475151777267456
Epoch: 25, Steps: 90 | Train Loss: 0.2833531 Vali Loss: 0.5243776 Test Loss: 0.6182390
Validation loss decreased (0.533116 --> 0.524378).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 53.927818298339844
Epoch: 26, Steps: 90 | Train Loss: 0.2764220 Vali Loss: 0.5169961 Test Loss: 0.6098519
Validation loss decreased (0.524378 --> 0.516996).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 53.358598709106445
Epoch: 27, Steps: 90 | Train Loss: 0.2700903 Vali Loss: 0.5098897 Test Loss: 0.6018497
Validation loss decreased (0.516996 --> 0.509890).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 53.0050106048584
Epoch: 28, Steps: 90 | Train Loss: 0.2642211 Vali Loss: 0.5031695 Test Loss: 0.5943151
Validation loss decreased (0.509890 --> 0.503170).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 52.800498247146606
Epoch: 29, Steps: 90 | Train Loss: 0.2587048 Vali Loss: 0.4969435 Test Loss: 0.5869085
Validation loss decreased (0.503170 --> 0.496943).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 53.624314069747925
Epoch: 30, Steps: 90 | Train Loss: 0.2536922 Vali Loss: 0.4913242 Test Loss: 0.5806449
Validation loss decreased (0.496943 --> 0.491324).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11609
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=130, out_features=260, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3729356800.0
params:  34060.0
Trainable parameters:  34060
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 52.223021507263184
Epoch: 1, Steps: 90 | Train Loss: 0.3360114 Vali Loss: 0.4039311 Test Loss: 0.4841580
Validation loss decreased (inf --> 0.403931).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 48.804362058639526
Epoch: 2, Steps: 90 | Train Loss: 0.2881795 Vali Loss: 0.3702314 Test Loss: 0.4503641
Validation loss decreased (0.403931 --> 0.370231).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 52.07297492027283
Epoch: 3, Steps: 90 | Train Loss: 0.2719109 Vali Loss: 0.3598508 Test Loss: 0.4414739
Validation loss decreased (0.370231 --> 0.359851).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 50.62834692001343
Epoch: 4, Steps: 90 | Train Loss: 0.2675534 Vali Loss: 0.3567556 Test Loss: 0.4400070
Validation loss decreased (0.359851 --> 0.356756).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 50.22684144973755
Epoch: 5, Steps: 90 | Train Loss: 0.2664071 Vali Loss: 0.3562170 Test Loss: 0.4394768
Validation loss decreased (0.356756 --> 0.356217).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 52.68045210838318
Epoch: 6, Steps: 90 | Train Loss: 0.2659579 Vali Loss: 0.3556986 Test Loss: 0.4390858
Validation loss decreased (0.356217 --> 0.355699).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 51.774908781051636
Epoch: 7, Steps: 90 | Train Loss: 0.2659100 Vali Loss: 0.3556435 Test Loss: 0.4394181
Validation loss decreased (0.355699 --> 0.355643).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 52.4380362033844
Epoch: 8, Steps: 90 | Train Loss: 0.2657987 Vali Loss: 0.3552440 Test Loss: 0.4393688
Validation loss decreased (0.355643 --> 0.355244).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 48.04322028160095
Epoch: 9, Steps: 90 | Train Loss: 0.2657306 Vali Loss: 0.3558069 Test Loss: 0.4394691
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 51.92138361930847
Epoch: 10, Steps: 90 | Train Loss: 0.2656482 Vali Loss: 0.3554018 Test Loss: 0.4392419
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 54.005648612976074
Epoch: 11, Steps: 90 | Train Loss: 0.2656505 Vali Loss: 0.3552130 Test Loss: 0.4390346
Validation loss decreased (0.355244 --> 0.355213).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 50.77083158493042
Epoch: 12, Steps: 90 | Train Loss: 0.2656069 Vali Loss: 0.3549494 Test Loss: 0.4391903
Validation loss decreased (0.355213 --> 0.354949).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 52.7135009765625
Epoch: 13, Steps: 90 | Train Loss: 0.2655838 Vali Loss: 0.3553087 Test Loss: 0.4389374
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 52.06681752204895
Epoch: 14, Steps: 90 | Train Loss: 0.2654652 Vali Loss: 0.3549200 Test Loss: 0.4389962
Validation loss decreased (0.354949 --> 0.354920).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 52.70917224884033
Epoch: 15, Steps: 90 | Train Loss: 0.2655137 Vali Loss: 0.3554199 Test Loss: 0.4390607
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 52.01163578033447
Epoch: 16, Steps: 90 | Train Loss: 0.2654433 Vali Loss: 0.3551383 Test Loss: 0.4391558
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 52.911659240722656
Epoch: 17, Steps: 90 | Train Loss: 0.2655816 Vali Loss: 0.3552339 Test Loss: 0.4390031
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 51.02964925765991
Epoch: 18, Steps: 90 | Train Loss: 0.2655268 Vali Loss: 0.3552082 Test Loss: 0.4390361
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 52.76265573501587
Epoch: 19, Steps: 90 | Train Loss: 0.2654298 Vali Loss: 0.3552085 Test Loss: 0.4389536
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_336_336_FITS_custom_ftM_sl336_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4365481734275818, mae:0.29141855239868164, rse:0.5430194139480591, corr:[0.27201024 0.28321716 0.28328127 0.28159747 0.28370875 0.28272617
 0.28341246 0.2842037  0.28314894 0.28370026 0.28339803 0.28274438
 0.283685   0.28311917 0.2826286  0.28319323 0.28258973 0.28275862
 0.2832124  0.28254148 0.2827033  0.2824268  0.28158092 0.28277034
 0.28391045 0.2836604  0.2842     0.28380772 0.28339854 0.284097
 0.28385234 0.28357843 0.28397518 0.2834219  0.28328955 0.2835719
 0.2827264  0.28274938 0.28337592 0.2829649  0.28317526 0.283597
 0.2832501  0.28329632 0.2832808  0.28294998 0.2830306  0.28302974
 0.28343737 0.28405598 0.28376788 0.28351426 0.28370145 0.28315717
 0.28277877 0.282881   0.2827537  0.28331253 0.283815   0.2832802
 0.28329724 0.28354588 0.2832329  0.2835453  0.28356823 0.28280887
 0.28309473 0.2832019  0.28256598 0.28273988 0.2829182  0.28280383
 0.28310606 0.28307673 0.283032   0.2831976  0.2826465  0.28234813
 0.28267613 0.28238386 0.28226453 0.28270122 0.28266978 0.2826724
 0.28277814 0.2826042  0.28286856 0.28285167 0.2822595  0.2824952
 0.28293917 0.28252736 0.28233027 0.2822809  0.2821401  0.2824893
 0.28267232 0.2828409  0.28323394 0.28287268 0.28243053 0.2826183
 0.28249213 0.28233513 0.2823872  0.28204292 0.28212008 0.28258708
 0.28245223 0.2823928  0.2825396  0.28217363 0.28179643 0.2815795
 0.28144735 0.281719   0.2820107  0.28184026 0.2817058  0.28177476
 0.2821481  0.28238928 0.28228012 0.28256607 0.2830478  0.28278527
 0.2826682  0.28289813 0.28253463 0.2823364  0.28259012 0.28247738
 0.28241575 0.28240687 0.2819865  0.28173453 0.2817468  0.28177768
 0.2819339  0.2818352  0.28169316 0.2818846  0.28192523 0.28239658
 0.28311285 0.28269598 0.28247428 0.28298253 0.2830103  0.28299335
 0.28320324 0.28278658 0.28262505 0.28290892 0.2827848  0.28303418
 0.28329173 0.282594   0.2822012  0.28226948 0.282173   0.2824365
 0.2822471  0.28162983 0.28199795 0.28246412 0.2826817  0.28452632
 0.2860162  0.28505102 0.28469393 0.28452796 0.2842497  0.28443205
 0.28445563 0.28413346 0.28399262 0.28358534 0.2833087  0.28359374
 0.28374305 0.28380474 0.28374368 0.28337166 0.28352123 0.2838492
 0.28370783 0.2838307  0.28374916 0.28329828 0.2834679  0.28378424
 0.28392953 0.2844126  0.28461933 0.28428385 0.2842649  0.2841025
 0.28399184 0.2840755  0.28349409 0.2831264  0.2835757  0.28353125
 0.28326136 0.28327304 0.28302    0.2830453  0.2831952  0.2828659
 0.28294286 0.28309372 0.28267494 0.2828237  0.28313804 0.28287268
 0.28289375 0.28317156 0.28343764 0.28363413 0.283197   0.2827591
 0.2828423  0.28246757 0.28217667 0.28273502 0.2829802  0.2828549
 0.2828913  0.28251973 0.28234023 0.28271356 0.28284928 0.28305063
 0.28314683 0.2826778  0.2827536  0.28315714 0.28323123 0.2834095
 0.28308743 0.2826465  0.28321028 0.28331012 0.28272417 0.2828475
 0.28288606 0.28272498 0.28286156 0.28239992 0.2820159  0.28223813
 0.2819378  0.28174984 0.28205726 0.28183448 0.28179428 0.28218955
 0.28211093 0.28201517 0.28193235 0.28172514 0.28207588 0.28227374
 0.28194007 0.282092   0.2822291  0.28206396 0.28218397 0.28193521
 0.28174248 0.28211764 0.28177285 0.281368   0.28180125 0.28167945
 0.28137192 0.28153005 0.2812788  0.28121334 0.2812868  0.28075236
 0.28080675 0.2810777  0.2806784  0.28090182 0.28128126 0.281159
 0.28170294 0.28195786 0.28181577 0.28235766 0.28229928 0.28181183
 0.2819407  0.28141156 0.28102013 0.28144807 0.28115156 0.2811654
 0.2816424  0.28095946 0.28070652 0.28105572 0.2807833  0.28129318
 0.2817367  0.2810187  0.28098473 0.2809844  0.28074044 0.2815741
 0.281466   0.28096253 0.2821354  0.2821698  0.2816891  0.28219578
 0.28133515 0.28094003 0.2817935  0.28157827 0.28218555 0.28264415
 0.28197023 0.2827989  0.28223103 0.28117013 0.2828004  0.28204438
 0.28139338 0.2821138  0.27893385 0.28064296 0.28033236 0.2852065 ]
