Args in experiment:
Namespace(is_training=1, model_id='ETTm1_720_336', model='FITS', data='ETTm1', root_path='./dataset/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=196, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4042754
	speed: 0.0818s/iter; left time: 632.2383s
	iters: 200, epoch: 1 | loss: 0.3071581
	speed: 0.0625s/iter; left time: 476.6552s
Epoch: 1 cost time: 18.47148871421814
Epoch: 1, Steps: 261 | Train Loss: 0.4052117 Vali Loss: 0.9160852 Test Loss: 0.5427957
Validation loss decreased (inf --> 0.916085).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2664435
	speed: 0.3399s/iter; left time: 2539.1100s
	iters: 200, epoch: 2 | loss: 0.2202385
	speed: 0.0756s/iter; left time: 557.3315s
Epoch: 2 cost time: 20.969659090042114
Epoch: 2, Steps: 261 | Train Loss: 0.2452451 Vali Loss: 0.8001165 Test Loss: 0.4676827
Validation loss decreased (0.916085 --> 0.800117).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1959015
	speed: 0.3024s/iter; left time: 2179.9556s
	iters: 200, epoch: 3 | loss: 0.1790755
	speed: 0.0638s/iter; left time: 453.3000s
Epoch: 3 cost time: 18.796257257461548
Epoch: 3, Steps: 261 | Train Loss: 0.1898615 Vali Loss: 0.7533655 Test Loss: 0.4303720
Validation loss decreased (0.800117 --> 0.753366).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1624797
	speed: 0.3040s/iter; left time: 2111.8551s
	iters: 200, epoch: 4 | loss: 0.1535425
	speed: 0.0684s/iter; left time: 468.1163s
Epoch: 4 cost time: 18.926145792007446
Epoch: 4, Steps: 261 | Train Loss: 0.1614818 Vali Loss: 0.7218526 Test Loss: 0.4087797
Validation loss decreased (0.753366 --> 0.721853).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1466157
	speed: 0.2945s/iter; left time: 1969.2849s
	iters: 200, epoch: 5 | loss: 0.1446029
	speed: 0.0674s/iter; left time: 443.8514s
Epoch: 5 cost time: 18.01923942565918
Epoch: 5, Steps: 261 | Train Loss: 0.1450885 Vali Loss: 0.7059584 Test Loss: 0.3945136
Validation loss decreased (0.721853 --> 0.705958).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1524565
	speed: 0.3382s/iter; left time: 2173.1447s
	iters: 200, epoch: 6 | loss: 0.1273478
	speed: 0.0746s/iter; left time: 472.0594s
Epoch: 6 cost time: 21.545871257781982
Epoch: 6, Steps: 261 | Train Loss: 0.1350453 Vali Loss: 0.6952593 Test Loss: 0.3848929
Validation loss decreased (0.705958 --> 0.695259).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1293449
	speed: 0.3083s/iter; left time: 1900.5959s
	iters: 200, epoch: 7 | loss: 0.1459162
	speed: 0.0559s/iter; left time: 338.9124s
Epoch: 7 cost time: 16.779189586639404
Epoch: 7, Steps: 261 | Train Loss: 0.1287452 Vali Loss: 0.6851333 Test Loss: 0.3783051
Validation loss decreased (0.695259 --> 0.685133).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1142617
	speed: 0.2855s/iter; left time: 1685.3723s
	iters: 200, epoch: 8 | loss: 0.1284886
	speed: 0.0647s/iter; left time: 375.3143s
Epoch: 8 cost time: 17.722800731658936
Epoch: 8, Steps: 261 | Train Loss: 0.1246300 Vali Loss: 0.6810217 Test Loss: 0.3739083
Validation loss decreased (0.685133 --> 0.681022).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1228316
	speed: 0.3085s/iter; left time: 1741.0234s
	iters: 200, epoch: 9 | loss: 0.1279678
	speed: 0.0758s/iter; left time: 420.2704s
Epoch: 9 cost time: 20.409293174743652
Epoch: 9, Steps: 261 | Train Loss: 0.1219194 Vali Loss: 0.6789587 Test Loss: 0.3717223
Validation loss decreased (0.681022 --> 0.678959).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1192432
	speed: 0.3185s/iter; left time: 1714.3854s
	iters: 200, epoch: 10 | loss: 0.1236943
	speed: 0.0655s/iter; left time: 346.0132s
Epoch: 10 cost time: 19.086799383163452
Epoch: 10, Steps: 261 | Train Loss: 0.1200248 Vali Loss: 0.6767588 Test Loss: 0.3704542
Validation loss decreased (0.678959 --> 0.676759).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1321281
	speed: 0.3070s/iter; left time: 1572.3476s
	iters: 200, epoch: 11 | loss: 0.1119043
	speed: 0.0645s/iter; left time: 324.0454s
Epoch: 11 cost time: 18.448315143585205
Epoch: 11, Steps: 261 | Train Loss: 0.1187302 Vali Loss: 0.6756114 Test Loss: 0.3685922
Validation loss decreased (0.676759 --> 0.675611).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1097204
	speed: 0.2834s/iter; left time: 1377.5353s
	iters: 200, epoch: 12 | loss: 0.1140996
	speed: 0.0717s/iter; left time: 341.2044s
Epoch: 12 cost time: 17.795703172683716
Epoch: 12, Steps: 261 | Train Loss: 0.1178198 Vali Loss: 0.6741520 Test Loss: 0.3686350
Validation loss decreased (0.675611 --> 0.674152).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1128642
	speed: 0.3130s/iter; left time: 1439.4984s
	iters: 200, epoch: 13 | loss: 0.1131150
	speed: 0.0743s/iter; left time: 334.1748s
Epoch: 13 cost time: 20.912639141082764
Epoch: 13, Steps: 261 | Train Loss: 0.1171807 Vali Loss: 0.6727939 Test Loss: 0.3686489
Validation loss decreased (0.674152 --> 0.672794).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1152310
	speed: 0.3272s/iter; left time: 1419.4509s
	iters: 200, epoch: 14 | loss: 0.1117894
	speed: 0.0594s/iter; left time: 251.8045s
Epoch: 14 cost time: 18.105563640594482
Epoch: 14, Steps: 261 | Train Loss: 0.1167509 Vali Loss: 0.6719347 Test Loss: 0.3684928
Validation loss decreased (0.672794 --> 0.671935).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1215593
	speed: 0.3032s/iter; left time: 1236.2587s
	iters: 200, epoch: 15 | loss: 0.1318775
	speed: 0.0729s/iter; left time: 289.7839s
Epoch: 15 cost time: 19.728328704833984
Epoch: 15, Steps: 261 | Train Loss: 0.1164552 Vali Loss: 0.6718296 Test Loss: 0.3688391
Validation loss decreased (0.671935 --> 0.671830).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1216324
	speed: 0.2957s/iter; left time: 1128.5713s
	iters: 200, epoch: 16 | loss: 0.1150799
	speed: 0.0663s/iter; left time: 246.5149s
Epoch: 16 cost time: 17.224064350128174
Epoch: 16, Steps: 261 | Train Loss: 0.1162822 Vali Loss: 0.6725556 Test Loss: 0.3691007
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1197394
	speed: 0.3184s/iter; left time: 1131.7657s
	iters: 200, epoch: 17 | loss: 0.1153182
	speed: 0.0767s/iter; left time: 264.9732s
Epoch: 17 cost time: 20.85220766067505
Epoch: 17, Steps: 261 | Train Loss: 0.1161324 Vali Loss: 0.6710116 Test Loss: 0.3696337
Validation loss decreased (0.671830 --> 0.671012).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1167751
	speed: 0.3154s/iter; left time: 1038.8772s
	iters: 200, epoch: 18 | loss: 0.1210917
	speed: 0.0616s/iter; left time: 196.8050s
Epoch: 18 cost time: 17.44951295852661
Epoch: 18, Steps: 261 | Train Loss: 0.1160335 Vali Loss: 0.6717661 Test Loss: 0.3698871
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1159301
	speed: 0.2920s/iter; left time: 885.6385s
	iters: 200, epoch: 19 | loss: 0.1222874
	speed: 0.0593s/iter; left time: 173.9073s
Epoch: 19 cost time: 17.594797134399414
Epoch: 19, Steps: 261 | Train Loss: 0.1160016 Vali Loss: 0.6726879 Test Loss: 0.3698914
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1114947
	speed: 0.3048s/iter; left time: 844.8475s
	iters: 200, epoch: 20 | loss: 0.1172156
	speed: 0.0706s/iter; left time: 188.7634s
Epoch: 20 cost time: 19.147937059402466
Epoch: 20, Steps: 261 | Train Loss: 0.1159607 Vali Loss: 0.6713696 Test Loss: 0.3704274
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1080595
	speed: 0.3232s/iter; left time: 811.5570s
	iters: 200, epoch: 21 | loss: 0.1264183
	speed: 0.0682s/iter; left time: 164.3835s
Epoch: 21 cost time: 20.07453942298889
Epoch: 21, Steps: 261 | Train Loss: 0.1159469 Vali Loss: 0.6724663 Test Loss: 0.3702551
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1153423
	speed: 0.3119s/iter; left time: 701.6744s
	iters: 200, epoch: 22 | loss: 0.1320290
	speed: 0.0677s/iter; left time: 145.6440s
Epoch: 22 cost time: 18.85834789276123
Epoch: 22, Steps: 261 | Train Loss: 0.1158895 Vali Loss: 0.6727073 Test Loss: 0.3709448
EarlyStopping counter: 5 out of 5
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3824286
	speed: 0.0709s/iter; left time: 548.3376s
	iters: 200, epoch: 1 | loss: 0.3251123
	speed: 0.0743s/iter; left time: 566.8022s
Epoch: 1 cost time: 18.286243200302124
Epoch: 1, Steps: 261 | Train Loss: 0.3386962 Vali Loss: 0.6587397 Test Loss: 0.3678136
Validation loss decreased (inf --> 0.658740).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3003258
	speed: 0.2968s/iter; left time: 2216.8754s
	iters: 200, epoch: 2 | loss: 0.3108638
	speed: 0.0750s/iter; left time: 552.6373s
Epoch: 2 cost time: 20.13995122909546
Epoch: 2, Steps: 261 | Train Loss: 0.3373433 Vali Loss: 0.6551259 Test Loss: 0.3672495
Validation loss decreased (0.658740 --> 0.655126).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3581932
	speed: 0.3280s/iter; left time: 2364.8244s
	iters: 200, epoch: 3 | loss: 0.3172767
	speed: 0.0651s/iter; left time: 462.7898s
Epoch: 3 cost time: 19.23750329017639
Epoch: 3, Steps: 261 | Train Loss: 0.3368074 Vali Loss: 0.6554170 Test Loss: 0.3671048
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3581706
	speed: 0.3002s/iter; left time: 2085.5361s
	iters: 200, epoch: 4 | loss: 0.3123760
	speed: 0.0690s/iter; left time: 472.8358s
Epoch: 4 cost time: 18.98584008216858
Epoch: 4, Steps: 261 | Train Loss: 0.3364966 Vali Loss: 0.6565070 Test Loss: 0.3672498
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3237403
	speed: 0.2876s/iter; left time: 1923.4886s
	iters: 200, epoch: 5 | loss: 0.3407586
	speed: 0.0575s/iter; left time: 378.4979s
Epoch: 5 cost time: 17.072208642959595
Epoch: 5, Steps: 261 | Train Loss: 0.3364012 Vali Loss: 0.6557491 Test Loss: 0.3669529
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3247098
	speed: 0.2942s/iter; left time: 1890.8287s
	iters: 200, epoch: 6 | loss: 0.3358437
	speed: 0.0757s/iter; left time: 479.0778s
Epoch: 6 cost time: 20.19538378715515
Epoch: 6, Steps: 261 | Train Loss: 0.3361213 Vali Loss: 0.6531309 Test Loss: 0.3670953
Validation loss decreased (0.655126 --> 0.653131).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3310620
	speed: 0.3344s/iter; left time: 2061.2771s
	iters: 200, epoch: 7 | loss: 0.3325234
	speed: 0.0668s/iter; left time: 405.0626s
Epoch: 7 cost time: 18.012847185134888
Epoch: 7, Steps: 261 | Train Loss: 0.3361517 Vali Loss: 0.6536632 Test Loss: 0.3672794
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3118677
	speed: 0.2589s/iter; left time: 1528.4732s
	iters: 200, epoch: 8 | loss: 0.3303940
	speed: 0.0543s/iter; left time: 314.8956s
Epoch: 8 cost time: 15.138550043106079
Epoch: 8, Steps: 261 | Train Loss: 0.3360806 Vali Loss: 0.6553438 Test Loss: 0.3670582
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3502649
	speed: 0.3017s/iter; left time: 1702.6057s
	iters: 200, epoch: 9 | loss: 0.3122521
	speed: 0.0708s/iter; left time: 392.5261s
Epoch: 9 cost time: 19.628052711486816
Epoch: 9, Steps: 261 | Train Loss: 0.3359595 Vali Loss: 0.6536880 Test Loss: 0.3670297
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3119791
	speed: 0.2860s/iter; left time: 1539.0709s
	iters: 200, epoch: 10 | loss: 0.3609668
	speed: 0.0721s/iter; left time: 381.0720s
Epoch: 10 cost time: 18.014877557754517
Epoch: 10, Steps: 261 | Train Loss: 0.3357546 Vali Loss: 0.6551156 Test Loss: 0.3666892
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3340828
	speed: 0.2984s/iter; left time: 1527.9188s
	iters: 200, epoch: 11 | loss: 0.3190125
	speed: 0.0763s/iter; left time: 383.2808s
Epoch: 11 cost time: 20.538199424743652
Epoch: 11, Steps: 261 | Train Loss: 0.3358029 Vali Loss: 0.6536444 Test Loss: 0.3667599
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3665642738342285, mae:0.38489800691604614, rse:0.5761337876319885, corr:[0.53842443 0.54650396 0.54838395 0.5531832  0.55597436 0.5558608
 0.55662066 0.5577384  0.5579688  0.55895436 0.56032807 0.5600498
 0.5594895  0.560183   0.5602628  0.5586785  0.5574523  0.55694956
 0.5558047  0.55441564 0.55363864 0.55254966 0.55047685 0.5486166
 0.5476845  0.5468653  0.5456299  0.54482275 0.54482675 0.54451656
 0.543785   0.54399896 0.54536015 0.54661393 0.54657114 0.5457415
 0.5448799  0.5449201  0.5454037  0.54507345 0.5442789  0.5443423
 0.54530865 0.54582936 0.5454408  0.544981   0.5448987  0.54462093
 0.5441959  0.5442349  0.54469323 0.5447477  0.5443455  0.5439569
 0.5441501  0.54458714 0.54454476 0.5442047  0.5444869  0.54504013
 0.54495686 0.5443078  0.5439834  0.5438008  0.54310423 0.5424022
 0.54283345 0.5440077  0.5443925  0.5437244  0.54320455 0.5432453
 0.54326314 0.5429705  0.5425389  0.5419204  0.5413449  0.54141366
 0.54195523 0.5421768  0.5420371  0.54228187 0.5428315  0.54275715
 0.5421338  0.5418025  0.5418467  0.54156953 0.541287   0.54167277
 0.5422395  0.542255   0.5421853  0.5426626  0.543069   0.54247904
 0.54142094 0.54109895 0.5414235  0.5414765  0.5409845  0.5406431
 0.5407322  0.5407565  0.5405965  0.54045105 0.54028416 0.5399663
 0.53951454 0.5392031  0.5389771  0.53872925 0.538664   0.53880125
 0.53882897 0.5386131  0.5383175  0.5379763  0.5374272  0.5368306
 0.5365592  0.53658146 0.5363748  0.5360232  0.53589535 0.5359182
 0.53577584 0.53563815 0.53570426 0.5355648  0.53476864 0.53394806
 0.5342779  0.53523105 0.5355054  0.53503025 0.5345856  0.5343396
 0.534316   0.53457516 0.5348022  0.5345674  0.5341137  0.5341474
 0.5345511  0.53451633 0.5341708  0.53433144 0.53506505 0.5353066
 0.5347397  0.5343151  0.5346857  0.53528816 0.5353405  0.534934
 0.53458905 0.53460187 0.5348458  0.534859   0.5347167  0.5344976
 0.5342933  0.53423834 0.534535   0.5350321  0.5353766  0.5355847
 0.5357889  0.53583586 0.53568393 0.53556436 0.53565216 0.5357798
 0.53557146 0.5351625  0.53507173 0.53547406 0.5358461  0.5358377
 0.5357294  0.53585255 0.5360119  0.5360013  0.5360717  0.5365952
 0.5373294  0.5376747  0.53746736 0.5372964  0.5376058  0.5380325
 0.5381321  0.53800714 0.53785515 0.53753495 0.5368277  0.5358187
 0.53497803 0.53451806 0.53404653 0.53317326 0.53210014 0.5314279
 0.5313163  0.5313087  0.5308284  0.5298032  0.5286816  0.52789354
 0.5275413  0.5274642  0.5272335  0.5267244  0.52598476 0.52515566
 0.5243448  0.5237214  0.52350515 0.52353483 0.5234983  0.5233523
 0.5232721  0.52324    0.52358985 0.52424484 0.52471095 0.5247168
 0.52444184 0.52432793 0.524455   0.5245123  0.5245188  0.52478695
 0.5250921  0.5248835  0.5240677  0.52380925 0.5242633  0.52457786
 0.52437115 0.52430165 0.52479273 0.5250279  0.52447385 0.5236346
 0.52339435 0.52375203 0.52417815 0.5244691  0.52428067 0.5236372
 0.5229913  0.5231191  0.5235438  0.52355313 0.5233411  0.52354497
 0.52393854 0.5238938  0.5235747  0.52376544 0.5245115  0.5251098
 0.52506447 0.5249882  0.5254327  0.52587944 0.52565587 0.52495486
 0.524435   0.5242981  0.52461636 0.5251276  0.525242   0.524822
 0.5241459  0.5240644  0.5245651  0.5251183  0.52527946 0.5250436
 0.52464867 0.5243299  0.52429277 0.52450275 0.52442753 0.5238398
 0.52313226 0.5226873  0.5223177  0.52171135 0.52079934 0.5197153
 0.51892275 0.51877534 0.5190803  0.5188015  0.5178235  0.5170269
 0.5169537  0.5170679  0.51662993 0.51571554 0.5150249  0.51476115
 0.51489055 0.5150307  0.5149459  0.5145258  0.5140326  0.5137552
 0.5135947  0.51334316 0.513142   0.5132856  0.513425   0.51329416
 0.5132193  0.51342046 0.5136392  0.51362705 0.5133893  0.5129828
 0.51267296 0.51289    0.5132181  0.5132266  0.5130814  0.51286006
 0.51248866 0.51254725 0.51292044 0.51228917 0.51209545 0.508137  ]
