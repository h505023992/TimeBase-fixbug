[2024-10-27 22:56:31,313] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-27 22:56:38,137] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-27 22:56:38,138] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-27 22:56:53,186] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.2, git-hash=unknown, git-branch=unknown
[2024-10-27 22:56:53,186] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1
[2024-10-27 22:56:56,220] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-27 22:56:56,222] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-10-27 22:56:56,222] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-10-27 22:56:56,225] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-10-27 22:56:56,225] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-10-27 22:56:56,225] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-10-27 22:56:56,225] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-10-27 22:56:56,225] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-10-27 22:56:56,226] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-10-27 22:56:56,226] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-10-27 22:56:56,551] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-10-27 22:56:56,552] [INFO] [utils.py:782:see_memory_usage] MA 0.57 GB         Max_MA 0.67 GB         CA 0.68 GB         Max_CA 1 GB 
[2024-10-27 22:56:56,553] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 186.86 GB, percent = 74.2%
[2024-10-27 22:56:56,684] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-10-27 22:56:56,686] [INFO] [utils.py:782:see_memory_usage] MA 0.57 GB         Max_MA 0.78 GB         CA 0.89 GB         Max_CA 1 GB 
[2024-10-27 22:56:56,686] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 186.88 GB, percent = 74.2%
[2024-10-27 22:56:56,686] [INFO] [stage_1_and_2.py:544:__init__] optimizer state initialized
[2024-10-27 22:56:56,816] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-10-27 22:56:56,817] [INFO] [utils.py:782:see_memory_usage] MA 0.57 GB         Max_MA 0.57 GB         CA 0.89 GB         Max_CA 1 GB 
[2024-10-27 22:56:56,817] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 186.89 GB, percent = 74.2%
[2024-10-27 22:56:56,818] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-10-27 22:56:56,819] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-10-27 22:56:56,819] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.OneCycleLR object at 0x1554a7a60f40>
[2024-10-27 22:56:56,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-10-27 22:56:56,820] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-27 22:56:56,821] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-27 22:56:56,821] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-27 22:56:56,821] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-27 22:56:56,821] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-27 22:56:56,821] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-27 22:56:56,821] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-27 22:56:56,821] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-27 22:56:56,821] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-27 22:56:56,821] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554a698f2e0>
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-27 22:56:56,822] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-27 22:56:56,823] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-27 22:56:56,824] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   train_batch_size ............. 32
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  32
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   world_size ................... 1
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2024-10-27 22:56:56,825] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-27 22:56:56,826] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 2
[2024-10-27 22:56:56,826] [INFO] [config.py:989:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.6849406
	speed: 0.4100s/iter; left time: 20377.7887s
	iters: 200, epoch: 1 | loss: 0.7777722
	speed: 0.3548s/iter; left time: 17599.6739s
	iters: 300, epoch: 1 | loss: 1.0164269
	speed: 0.3528s/iter; left time: 17462.2801s
	iters: 400, epoch: 1 | loss: 1.2101861
	speed: 0.3548s/iter; left time: 17529.2848s
	iters: 500, epoch: 1 | loss: 0.4604106
	speed: 0.3544s/iter; left time: 17471.3438s
	iters: 600, epoch: 1 | loss: 0.6314791
	speed: 0.3540s/iter; left time: 17417.7109s
	iters: 700, epoch: 1 | loss: 0.6822497
	speed: 0.3533s/iter; left time: 17347.0681s
	iters: 800, epoch: 1 | loss: 0.6003971
	speed: 0.3564s/iter; left time: 17463.4268s
	iters: 900, epoch: 1 | loss: 0.5926489
	speed: 0.3572s/iter; left time: 17466.8458s
	iters: 1000, epoch: 1 | loss: 0.7095150
	speed: 0.3559s/iter; left time: 17366.8074s
	iters: 1100, epoch: 1 | loss: 0.6018248
	speed: 0.3552s/iter; left time: 17299.2807s
	iters: 1200, epoch: 1 | loss: 1.0244991
	speed: 0.3554s/iter; left time: 17274.4567s
	iters: 1300, epoch: 1 | loss: 0.7612644
	speed: 0.3553s/iter; left time: 17234.6046s
	iters: 1400, epoch: 1 | loss: 0.7289110
	speed: 0.3585s/iter; left time: 17349.5824s
	iters: 1500, epoch: 1 | loss: 0.6291536
	speed: 0.3562s/iter; left time: 17204.1639s
	iters: 1600, epoch: 1 | loss: 0.7318429
	speed: 0.3595s/iter; left time: 17328.8095s
Epoch: 1 cost time: 592.2678551673889
Epoch: 1 | Train Loss: 0.8583791 Vali Loss: 0.6247971 Test Loss: 0.4257926 MAE Loss: 0.4594516
lr = 0.0001043204
Updating learning rate to 0.0001043204248628312
	iters: 100, epoch: 2 | loss: 0.8666587
	speed: 2.3969s/iter; left time: 115149.1183s
	iters: 200, epoch: 2 | loss: 0.7923694
	speed: 0.3392s/iter; left time: 16259.7741s
	iters: 300, epoch: 2 | loss: 0.7881361
	speed: 0.3433s/iter; left time: 16425.3980s
	iters: 400, epoch: 2 | loss: 1.0983790
	speed: 0.3416s/iter; left time: 16307.5905s
	iters: 500, epoch: 2 | loss: 0.7580456
	speed: 0.3422s/iter; left time: 16303.0660s
	iters: 600, epoch: 2 | loss: 0.4899436
	speed: 0.3407s/iter; left time: 16199.0333s
	iters: 700, epoch: 2 | loss: 0.2592630
	speed: 0.3424s/iter; left time: 16242.8148s
	iters: 800, epoch: 2 | loss: 0.5140008
	speed: 0.3452s/iter; left time: 16343.3461s
	iters: 900, epoch: 2 | loss: 0.8950656
	speed: 0.3433s/iter; left time: 16218.6675s
	iters: 1000, epoch: 2 | loss: 0.7823468
	speed: 0.3439s/iter; left time: 16210.7152s
	iters: 1100, epoch: 2 | loss: 0.4616712
	speed: 0.3454s/iter; left time: 16246.9187s
	iters: 1200, epoch: 2 | loss: 0.9663979
	speed: 0.3427s/iter; left time: 16085.3747s
	iters: 1300, epoch: 2 | loss: 0.7123867
	speed: 0.3421s/iter; left time: 16023.9922s
	iters: 1400, epoch: 2 | loss: 1.0682749
	speed: 0.3441s/iter; left time: 16084.4888s
	iters: 1500, epoch: 2 | loss: 0.7996988
	speed: 0.3410s/iter; left time: 15904.5705s
	iters: 1600, epoch: 2 | loss: 0.9784578
	speed: 0.3426s/iter; left time: 15943.6427s
Epoch: 2 cost time: 568.9998481273651
Epoch: 2 | Train Loss: 0.7878630 Vali Loss: 0.6441661 Test Loss: 0.4137238 MAE Loss: 0.4462164
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.21602124314156e-05
	iters: 100, epoch: 3 | loss: 0.8824270
	speed: 2.2135s/iter; left time: 102662.9289s
	iters: 200, epoch: 3 | loss: 0.9695466
	speed: 0.3380s/iter; left time: 15643.3475s
	iters: 300, epoch: 3 | loss: 0.7356892
	speed: 0.3414s/iter; left time: 15765.3310s
	iters: 400, epoch: 3 | loss: 0.7798852
	speed: 0.3393s/iter; left time: 15636.4956s
	iters: 500, epoch: 3 | loss: 0.9430505
	speed: 0.3417s/iter; left time: 15710.7063s
	iters: 600, epoch: 3 | loss: 0.7943390
	speed: 0.3352s/iter; left time: 15378.5345s
	iters: 700, epoch: 3 | loss: 0.6314381
	speed: 0.3357s/iter; left time: 15368.9795s
	iters: 800, epoch: 3 | loss: 0.7545057
	speed: 0.3338s/iter; left time: 15246.9365s
	iters: 900, epoch: 3 | loss: 1.1554331
	speed: 0.3338s/iter; left time: 15214.5383s
	iters: 1000, epoch: 3 | loss: 0.9432288
	speed: 0.3333s/iter; left time: 15159.4028s
	iters: 1100, epoch: 3 | loss: 0.8853051
	speed: 0.3341s/iter; left time: 15163.6389s
	iters: 1200, epoch: 3 | loss: 0.8712053
	speed: 0.3340s/iter; left time: 15124.2757s
	iters: 1300, epoch: 3 | loss: 1.0049294
	speed: 0.3356s/iter; left time: 15161.4588s
	iters: 1400, epoch: 3 | loss: 1.0746660
	speed: 0.3338s/iter; left time: 15047.5047s
	iters: 1500, epoch: 3 | loss: 0.7707030
	speed: 0.3382s/iter; left time: 15213.0037s
	iters: 1600, epoch: 3 | loss: 0.9118180
	speed: 0.3353s/iter; left time: 15048.7740s
Epoch: 3 cost time: 558.6025593280792
Epoch: 3 | Train Loss: 0.7924881 Vali Loss: 0.5999191 Test Loss: 0.4225115 MAE Loss: 0.4509747
Updating learning rate to 2.60801062157078e-05
	iters: 100, epoch: 4 | loss: 1.5758886
	speed: 2.2493s/iter; left time: 100590.0934s
	iters: 200, epoch: 4 | loss: 0.4397477
	speed: 0.3384s/iter; left time: 15099.0798s
	iters: 300, epoch: 4 | loss: 0.6787968
	speed: 0.3350s/iter; left time: 14912.8904s
	iters: 400, epoch: 4 | loss: 1.0537617
	speed: 0.3335s/iter; left time: 14815.5324s
	iters: 500, epoch: 4 | loss: 0.5584456
	speed: 0.3330s/iter; left time: 14757.9782s
	iters: 600, epoch: 4 | loss: 0.6063905
	speed: 0.3359s/iter; left time: 14853.1538s
	iters: 700, epoch: 4 | loss: 0.7057015
	speed: 0.3329s/iter; left time: 14688.3756s
	iters: 800, epoch: 4 | loss: 1.0705920
	speed: 0.3395s/iter; left time: 14943.6328s
	iters: 900, epoch: 4 | loss: 0.8974448
	speed: 0.3371s/iter; left time: 14805.8124s
	iters: 1000, epoch: 4 | loss: 0.5621161
	speed: 0.3359s/iter; left time: 14719.1162s
	iters: 1100, epoch: 4 | loss: 0.5781792
	speed: 0.3328s/iter; left time: 14551.7753s
	iters: 1200, epoch: 4 | loss: 0.8847985
	speed: 0.3342s/iter; left time: 14577.7211s
	iters: 1300, epoch: 4 | loss: 0.6363138
	speed: 0.3408s/iter; left time: 14833.3613s
	iters: 1400, epoch: 4 | loss: 1.2165490
	speed: 0.3397s/iter; left time: 14750.4693s
	iters: 1500, epoch: 4 | loss: 0.6918525
	speed: 0.3379s/iter; left time: 14636.2570s
	iters: 1600, epoch: 4 | loss: 0.7109133
	speed: 0.3362s/iter; left time: 14529.0612s
Epoch: 4 cost time: 558.3628141880035
Epoch: 4 | Train Loss: 0.8102066 Vali Loss: 0.6403014 Test Loss: 0.4099293 MAE Loss: 0.4402955
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.30400531078539e-05
	iters: 100, epoch: 5 | loss: 0.4824751
	speed: 2.1864s/iter; left time: 94150.0526s
	iters: 200, epoch: 5 | loss: 0.9625603
	speed: 0.3390s/iter; left time: 14565.6047s
	iters: 300, epoch: 5 | loss: 0.8659955
	speed: 0.3325s/iter; left time: 14251.5831s
	iters: 400, epoch: 5 | loss: 0.4598912
	speed: 0.3340s/iter; left time: 14280.1552s
	iters: 500, epoch: 5 | loss: 1.0124655
	speed: 0.3343s/iter; left time: 14263.6791s
	iters: 600, epoch: 5 | loss: 0.7165442
	speed: 0.3302s/iter; left time: 14052.2662s
	iters: 700, epoch: 5 | loss: 0.9286874
	speed: 0.3365s/iter; left time: 14288.7029s
	iters: 800, epoch: 5 | loss: 1.1933039
	speed: 0.3382s/iter; left time: 14325.0322s
	iters: 900, epoch: 5 | loss: 0.8388463
	speed: 0.3372s/iter; left time: 14248.5951s
	iters: 1000, epoch: 5 | loss: 1.5953060
	speed: 0.3362s/iter; left time: 14173.3435s
	iters: 1100, epoch: 5 | loss: 1.0382761
	speed: 0.3331s/iter; left time: 14009.3498s
	iters: 1200, epoch: 5 | loss: 1.1249874
	speed: 0.3376s/iter; left time: 14166.9044s
	iters: 1300, epoch: 5 | loss: 0.7814779
	speed: 0.3349s/iter; left time: 14020.6725s
	iters: 1400, epoch: 5 | loss: 0.3966458
	speed: 0.3341s/iter; left time: 13953.1225s
	iters: 1500, epoch: 5 | loss: 0.7072588
	speed: 0.3364s/iter; left time: 14013.0369s
	iters: 1600, epoch: 5 | loss: 0.8098140
	speed: 0.3339s/iter; left time: 13875.7435s
Epoch: 5 cost time: 557.1614923477173
Epoch: 5 | Train Loss: 0.8162426 Vali Loss: 0.6850600 Test Loss: 0.3987013 MAE Loss: 0.4296135
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.52002655392695e-06
	iters: 100, epoch: 6 | loss: 0.7835453
	speed: 2.1692s/iter; left time: 89807.3407s
	iters: 200, epoch: 6 | loss: 1.1379639
	speed: 0.3378s/iter; left time: 13952.7396s
	iters: 300, epoch: 6 | loss: 0.8679040
	speed: 0.3399s/iter; left time: 14005.1111s
	iters: 400, epoch: 6 | loss: 0.7206920
	speed: 0.3402s/iter; left time: 13984.1263s
	iters: 500, epoch: 6 | loss: 0.8842138
	speed: 0.3378s/iter; left time: 13848.7244s
	iters: 600, epoch: 6 | loss: 0.9372346
	speed: 0.3384s/iter; left time: 13838.9973s
	iters: 700, epoch: 6 | loss: 0.6573574
	speed: 0.3356s/iter; left time: 13692.5761s
	iters: 800, epoch: 6 | loss: 0.7376652
	speed: 0.3374s/iter; left time: 13733.4629s
	iters: 900, epoch: 6 | loss: 0.8717110
	speed: 0.3330s/iter; left time: 13521.2500s
	iters: 1000, epoch: 6 | loss: 0.6464038
	speed: 0.3371s/iter; left time: 13653.4573s
	iters: 1100, epoch: 6 | loss: 0.5679321
	speed: 0.3349s/iter; left time: 13529.8232s
	iters: 1200, epoch: 6 | loss: 1.1854106
	speed: 0.3320s/iter; left time: 13379.1475s
	iters: 1300, epoch: 6 | loss: 0.4424608
	speed: 0.3336s/iter; left time: 13410.1741s
	iters: 1400, epoch: 6 | loss: 0.6579100
	speed: 0.3354s/iter; left time: 13451.5824s
	iters: 1500, epoch: 6 | loss: 0.6003445
	speed: 0.3336s/iter; left time: 13345.9060s
	iters: 1600, epoch: 6 | loss: 1.3495076
	speed: 0.3352s/iter; left time: 13375.3880s
Epoch: 6 cost time: 558.3421761989594
Epoch: 6 | Train Loss: 0.8313103 Vali Loss: 0.6529270 Test Loss: 0.4110613 MAE Loss: 0.4422591
EarlyStopping counter: 3 out of 3
Early stopping
success delete checkpoints
