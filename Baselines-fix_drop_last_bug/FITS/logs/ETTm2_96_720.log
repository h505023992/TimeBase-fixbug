Args in experiment:
Namespace(is_training=1, model_id='ETTm2_96_720', model='FITS', data='ETTm2', root_path='./dataset/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=40, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_96_720_FITS_ETTm2_ftM_sl96_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=40, out_features=340, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12185600.0
params:  13940.0
Trainable parameters:  13940
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7606543
	speed: 0.0895s/iter; left time: 697.6796s
	iters: 200, epoch: 1 | loss: 0.5608841
	speed: 0.0826s/iter; left time: 635.5670s
Epoch: 1 cost time: 22.73466396331787
Epoch: 1, Steps: 263 | Train Loss: 0.6729801 Vali Loss: 0.3215367 Test Loss: 0.4490810
Validation loss decreased (inf --> 0.321537).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3741564
	speed: 0.3588s/iter; left time: 2700.7472s
	iters: 200, epoch: 2 | loss: 0.4599433
	speed: 0.0747s/iter; left time: 554.7402s
Epoch: 2 cost time: 21.014806032180786
Epoch: 2, Steps: 263 | Train Loss: 0.5369643 Vali Loss: 0.2932605 Test Loss: 0.4161073
Validation loss decreased (0.321537 --> 0.293260).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6189850
	speed: 0.3738s/iter; left time: 2715.4431s
	iters: 200, epoch: 3 | loss: 0.5199910
	speed: 0.0851s/iter; left time: 609.6083s
Epoch: 3 cost time: 23.38856816291809
Epoch: 3, Steps: 263 | Train Loss: 0.5228581 Vali Loss: 0.2896173 Test Loss: 0.4116271
Validation loss decreased (0.293260 --> 0.289617).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3777831
	speed: 0.3862s/iter; left time: 2703.8810s
	iters: 200, epoch: 4 | loss: 0.3956144
	speed: 0.0801s/iter; left time: 552.7814s
Epoch: 4 cost time: 22.608103036880493
Epoch: 4, Steps: 263 | Train Loss: 0.5201990 Vali Loss: 0.2884589 Test Loss: 0.4103598
Validation loss decreased (0.289617 --> 0.288459).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3979883
	speed: 0.3400s/iter; left time: 2291.0194s
	iters: 200, epoch: 5 | loss: 0.6030276
	speed: 0.0839s/iter; left time: 557.3268s
Epoch: 5 cost time: 24.033334493637085
Epoch: 5, Steps: 263 | Train Loss: 0.5192433 Vali Loss: 0.2881244 Test Loss: 0.4099109
Validation loss decreased (0.288459 --> 0.288124).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.7086149
	speed: 0.3862s/iter; left time: 2501.3432s
	iters: 200, epoch: 6 | loss: 0.4132804
	speed: 0.0849s/iter; left time: 541.0384s
Epoch: 6 cost time: 23.308780908584595
Epoch: 6, Steps: 263 | Train Loss: 0.5186775 Vali Loss: 0.2879156 Test Loss: 0.4097014
Validation loss decreased (0.288124 --> 0.287916).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4375191
	speed: 0.3744s/iter; left time: 2326.4179s
	iters: 200, epoch: 7 | loss: 0.5872985
	speed: 0.0913s/iter; left time: 557.9671s
Epoch: 7 cost time: 24.941972732543945
Epoch: 7, Steps: 263 | Train Loss: 0.5178035 Vali Loss: 0.2879944 Test Loss: 0.4095984
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4608369
	speed: 0.3895s/iter; left time: 2317.3419s
	iters: 200, epoch: 8 | loss: 0.5185228
	speed: 0.0798s/iter; left time: 467.0892s
Epoch: 8 cost time: 23.414421319961548
Epoch: 8, Steps: 263 | Train Loss: 0.5178806 Vali Loss: 0.2884659 Test Loss: 0.4097098
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5771033
	speed: 0.3921s/iter; left time: 2229.8742s
	iters: 200, epoch: 9 | loss: 0.3728362
	speed: 0.0855s/iter; left time: 477.6831s
Epoch: 9 cost time: 23.623005628585815
Epoch: 9, Steps: 263 | Train Loss: 0.5179534 Vali Loss: 0.2880740 Test Loss: 0.4096087
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5697411
	speed: 0.4089s/iter; left time: 2217.9868s
	iters: 200, epoch: 10 | loss: 0.4965274
	speed: 0.0917s/iter; left time: 487.9553s
Epoch: 10 cost time: 24.853280544281006
Epoch: 10, Steps: 263 | Train Loss: 0.5166889 Vali Loss: 0.2883080 Test Loss: 0.4096683
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4874923
	speed: 0.4116s/iter; left time: 2124.1804s
	iters: 200, epoch: 11 | loss: 0.5227554
	speed: 0.0908s/iter; left time: 459.3087s
Epoch: 11 cost time: 25.286898612976074
Epoch: 11, Steps: 263 | Train Loss: 0.5175473 Vali Loss: 0.2884484 Test Loss: 0.4098046
EarlyStopping counter: 5 out of 5
Early stopping
train 33745
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=40, out_features=340, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12185600.0
params:  13940.0
Trainable parameters:  13940
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5013471
	speed: 0.1012s/iter; left time: 788.6534s
	iters: 200, epoch: 1 | loss: 0.6000267
	speed: 0.0928s/iter; left time: 713.7228s
Epoch: 1 cost time: 24.843218088150024
Epoch: 1, Steps: 263 | Train Loss: 0.5858494 Vali Loss: 0.2882172 Test Loss: 0.4096930
Validation loss decreased (inf --> 0.288217).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7202789
	speed: 0.3946s/iter; left time: 2970.7936s
	iters: 200, epoch: 2 | loss: 0.7415937
	speed: 0.0789s/iter; left time: 585.7019s
Epoch: 2 cost time: 21.386783599853516
Epoch: 2, Steps: 263 | Train Loss: 0.5858896 Vali Loss: 0.2881438 Test Loss: 0.4095782
Validation loss decreased (0.288217 --> 0.288144).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6055832
	speed: 0.3578s/iter; left time: 2599.2536s
	iters: 200, epoch: 3 | loss: 0.5319358
	speed: 0.0850s/iter; left time: 609.3726s
Epoch: 3 cost time: 22.881460666656494
Epoch: 3, Steps: 263 | Train Loss: 0.5851136 Vali Loss: 0.2882439 Test Loss: 0.4093999
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4192477
	speed: 0.4030s/iter; left time: 2821.9167s
	iters: 200, epoch: 4 | loss: 0.5243956
	speed: 0.0904s/iter; left time: 624.0074s
Epoch: 4 cost time: 24.812529802322388
Epoch: 4, Steps: 263 | Train Loss: 0.5848239 Vali Loss: 0.2881948 Test Loss: 0.4094276
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5766412
	speed: 0.3924s/iter; left time: 2644.7078s
	iters: 200, epoch: 5 | loss: 0.4424258
	speed: 0.0938s/iter; left time: 622.9621s
Epoch: 5 cost time: 24.667834758758545
Epoch: 5, Steps: 263 | Train Loss: 0.5845826 Vali Loss: 0.2883668 Test Loss: 0.4094950
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.8121527
	speed: 0.3873s/iter; left time: 2508.2686s
	iters: 200, epoch: 6 | loss: 0.5382133
	speed: 0.0916s/iter; left time: 583.9802s
Epoch: 6 cost time: 24.1231746673584
Epoch: 6, Steps: 263 | Train Loss: 0.5851513 Vali Loss: 0.2885722 Test Loss: 0.4096250
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5177713
	speed: 0.3926s/iter; left time: 2439.4041s
	iters: 200, epoch: 7 | loss: 0.5056407
	speed: 0.0847s/iter; left time: 517.8218s
Epoch: 7 cost time: 24.148802042007446
Epoch: 7, Steps: 263 | Train Loss: 0.5854706 Vali Loss: 0.2882001 Test Loss: 0.4095539
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTm2_96_720_FITS_ETTm2_ftM_sl96_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.40718841552734375, mae:0.39751532673835754, rse:0.5129111409187317, corr:[0.55391306 0.54569536 0.5435181  0.54119235 0.5387246  0.53820384
 0.53578633 0.5357395  0.53348535 0.53292173 0.53159887 0.5304489
 0.529958   0.52847004 0.5284621  0.5276282  0.52671784 0.5260306
 0.52459174 0.5231771  0.5220647  0.52133936 0.5206969  0.5201214
 0.51944107 0.5189188  0.51820576 0.51727307 0.51636297 0.5154445
 0.5145596  0.51344746 0.51286465 0.512158   0.5116065  0.51144916
 0.51063085 0.51016796 0.50928915 0.5088253  0.50859356 0.50743574
 0.50681895 0.5064719  0.5063121  0.5056889  0.50484467 0.50431556
 0.5029035  0.50206363 0.501279   0.50024796 0.49993286 0.49928293
 0.49877265 0.4981086  0.4974509  0.49721134 0.49679613 0.49693576
 0.49661282 0.4961484  0.49621165 0.49577293 0.49581543 0.49582306
 0.49561438 0.49586433 0.49571437 0.4955799  0.49570316 0.49580914
 0.4959014  0.4958375  0.49571466 0.495664   0.49555987 0.495437
 0.49538144 0.4953089  0.49522513 0.49495226 0.49458778 0.49438098
 0.4941939  0.49389538 0.4934881  0.49329528 0.49302512 0.4925999
 0.4923465  0.49185392 0.49122256 0.49036893 0.48912483 0.48771247
 0.48571467 0.4841271  0.48313853 0.48126593 0.47990802 0.4789227
 0.47736368 0.47601798 0.47453335 0.47286952 0.47118682 0.46973613
 0.46860054 0.46729273 0.46571827 0.46392617 0.46287867 0.4620815
 0.4602225  0.4586172  0.45740935 0.4559423  0.45481572 0.45380607
 0.4527588  0.4514718  0.45006254 0.44882455 0.44773218 0.44678926
 0.4453471  0.44399768 0.44325978 0.44232532 0.44142124 0.4406908
 0.43972114 0.43863058 0.43752718 0.43670034 0.43622506 0.4358172
 0.4350935  0.43449706 0.4337687  0.43298766 0.432661   0.43213984
 0.43108225 0.4301416  0.42891106 0.42732093 0.42587256 0.42491457
 0.42475134 0.42438877 0.42349905 0.423366   0.42321193 0.42248562
 0.42226166 0.421994   0.4217551  0.42240345 0.4227656  0.42244467
 0.42219624 0.42192644 0.42160308 0.42103907 0.42121804 0.42165485
 0.42178917 0.4221396  0.42199725 0.42213175 0.42235819 0.4219221
 0.42219025 0.4222692  0.42266682 0.4231175  0.42251715 0.4228223
 0.4228573  0.42247397 0.42305717 0.422725   0.42232057 0.4220422
 0.42118144 0.4212964  0.42118132 0.42021418 0.41873577 0.41689906
 0.41558138 0.41409153 0.41272607 0.41141728 0.4098181  0.40838835
 0.40683502 0.40521848 0.40309882 0.4010678  0.39926708 0.39749157
 0.39653173 0.39486837 0.39311314 0.39204982 0.38994074 0.3883509
 0.3871363  0.38550594 0.38406244 0.38237858 0.38111407 0.38009197
 0.3786969  0.37732023 0.37614417 0.37452787 0.373305   0.37201267
 0.37030384 0.36901736 0.36788872 0.36695057 0.36626762 0.3654525
 0.36425057 0.36249644 0.3612308  0.3602922  0.35946208 0.35923788
 0.3589197  0.3588985  0.35861957 0.35857767 0.35888222 0.35755348
 0.3568321  0.35652295 0.3551559  0.35488763 0.35476944 0.35375163
 0.3534778  0.35385284 0.3544844  0.3545272  0.35489687 0.35566103
 0.35537    0.3550857  0.35548726 0.35604167 0.35685992 0.3573051
 0.35766163 0.35825953 0.35738277 0.35709038 0.35864532 0.35869506
 0.35846817 0.35855678 0.35911867 0.360384   0.36080703 0.36167014
 0.36206767 0.36149406 0.36203247 0.3625046  0.36281168 0.3629452
 0.36280924 0.36355734 0.36403826 0.36433998 0.36466905 0.3649396
 0.3655991  0.3658692  0.36605486 0.36610317 0.3651246  0.3639403
 0.36350706 0.3632822  0.3633377  0.3629854  0.36198428 0.36134738
 0.36081877 0.35970613 0.35910696 0.3582808  0.35638908 0.3556305
 0.3554117  0.3543358  0.35341594 0.35246453 0.35157275 0.35060698
 0.34934267 0.34842142 0.34712258 0.3459121  0.34561992 0.3453677
 0.34477907 0.34369066 0.34231308 0.34169266 0.34071738 0.33938533
 0.3391404  0.33859462 0.33803475 0.3379951  0.33652526 0.3355466
 0.33585837 0.33472267 0.3336809  0.3334141  0.3334122  0.33347246
 0.33283535 0.33283916 0.3335709  0.3337449  0.33364785 0.33365786
 0.33398822 0.33364815 0.3328385  0.33203688 0.33157325 0.3324442
 0.33256176 0.33194587 0.33242258 0.3331614  0.33381832 0.33390787
 0.3337318  0.3339042  0.3343563  0.33480024 0.33490467 0.33560127
 0.33604014 0.33618334 0.33670688 0.3362334  0.3362516  0.33725777
 0.33769095 0.33847713 0.33912706 0.33969206 0.3400266  0.34040675
 0.34166116 0.34212697 0.34205604 0.34227112 0.3424162  0.34305203
 0.34362248 0.34384376 0.34393317 0.344092   0.3443167  0.34457576
 0.344804   0.34435946 0.3445391  0.3442304  0.3425837  0.34215853
 0.34121135 0.34012827 0.33946788 0.33832714 0.33831125 0.33791417
 0.33703753 0.3359145  0.33438846 0.33374146 0.33205616 0.33045667
 0.32965747 0.32794532 0.32729205 0.32629544 0.3239152  0.3227581
 0.3218676  0.32018703 0.31863973 0.3177978  0.31668752 0.31535333
 0.31486857 0.3134697  0.31223264 0.31183222 0.31028277 0.3099079
 0.30957425 0.3078174  0.30683795 0.30593312 0.3056018  0.30497342
 0.30368808 0.30322748 0.30174997 0.30080688 0.30080044 0.3011681
 0.30176085 0.30084553 0.3003264  0.3001497  0.29910216 0.29836404
 0.29818296 0.29825628 0.29654434 0.29534274 0.29526445 0.29542157
 0.29614684 0.2946376  0.29389557 0.29404634 0.29343706 0.29445538
 0.29408947 0.29364416 0.29489306 0.29680094 0.29746005 0.29542068
 0.2958398  0.29665735 0.29542527 0.29569975 0.29580697 0.29499024
 0.29401088 0.29493544 0.29639006 0.2964112  0.2978136  0.2982874
 0.29847363 0.29891178 0.29835474 0.2986561  0.2979662  0.29740325
 0.29707935 0.29694155 0.29818237 0.29783037 0.2972214  0.2964018
 0.29641163 0.29731125 0.29582945 0.29546398 0.2949205  0.29262018
 0.29118952 0.28924242 0.28974423 0.28983638 0.28779423 0.2877084
 0.28704438 0.28579146 0.28413492 0.28272069 0.2821649  0.28094152
 0.2806283  0.27930063 0.27721512 0.27675995 0.2750203  0.27333024
 0.2719095  0.26960933 0.26874158 0.26724252 0.26538727 0.26534924
 0.26444086 0.26287812 0.26194456 0.260551   0.2591164  0.2584736
 0.2579172  0.25630337 0.2553336  0.25546324 0.25446433 0.25344598
 0.25249562 0.2515349  0.25077698 0.2489944  0.24864566 0.24904735
 0.24836649 0.24848585 0.24772395 0.24727835 0.24808529 0.24666625
 0.24582124 0.24483259 0.24269412 0.24357219 0.24417177 0.24327499
 0.24301738 0.24173601 0.24197389 0.24300966 0.2428506  0.24419646
 0.2439031  0.24295639 0.24407908 0.24409786 0.244667   0.24597517
 0.24611297 0.24639466 0.24587205 0.24510701 0.24542715 0.24639265
 0.24730092 0.24844024 0.24930525 0.24919653 0.24958715 0.24990895
 0.25052175 0.2517041  0.2508579  0.25051844 0.2514121  0.2514647
 0.25191677 0.25294244 0.25410536 0.25381315 0.25315857 0.25388047
 0.2538174  0.25402078 0.25435314 0.25319716 0.25190353 0.25033647
 0.24866591 0.24785604 0.24785662 0.24827257 0.24868251 0.24811618
 0.24695896 0.24624838 0.24514212 0.24316846 0.24171457 0.24118699
 0.24058178 0.23835203 0.23650625 0.23599572 0.23422211 0.23294437
 0.23208597 0.22955571 0.22827928 0.22779499 0.22653222 0.22588015
 0.22484986 0.22376394 0.22328329 0.2222067  0.22132146 0.22100091
 0.22012353 0.2182945  0.21676421 0.21606448 0.21528631 0.2152692
 0.21467142 0.21219285 0.21196927 0.2120894  0.2107625  0.21011622
 0.20919903 0.20923397 0.21025003 0.21004507 0.20963849 0.20860898
 0.20760415 0.20717748 0.20660613 0.20642713 0.20553637 0.20455718
 0.20464537 0.20521429 0.20608613 0.20559388 0.20530531 0.2073489
 0.20809345 0.20761998 0.20760381 0.2061085  0.20728633 0.21046843
 0.20985891 0.20889284 0.20970777 0.20972753 0.2105218  0.21195468
 0.21165884 0.21113028 0.2123469  0.21326278 0.21331389 0.21401624
 0.21455917 0.2150687  0.21649893 0.21726671 0.21737544 0.21903346
 0.2198402  0.21869288 0.21902251 0.2198144  0.22011852 0.22083265
 0.22132616 0.22165261 0.2213068  0.22058572 0.21934861 0.21757188
 0.21625121 0.21561201 0.21603723 0.21526302 0.21445023 0.21479452
 0.21349332 0.21232134 0.21173263 0.21041183 0.2093143  0.20807119
 0.20703252 0.20591372 0.20402563 0.2023804  0.20106663 0.1993934
 0.19774179 0.1964044  0.1949936  0.19390573 0.19233765 0.19074968
 0.18996385 0.18800375 0.18651822 0.18621434 0.18478724 0.1832176
 0.18136689 0.178279   0.17658323 0.17635067 0.1748737  0.17381999
 0.17181884 0.17007802 0.17038687 0.16852312 0.16839738 0.16793281
 0.1666506  0.16890015 0.1667443  0.17039438 0.17122465 0.17655058]
