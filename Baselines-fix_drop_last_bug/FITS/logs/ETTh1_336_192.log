Args in experiment:
Namespace(is_training=1, model_id='ETTh1_336_192', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=100, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_336_192_FITS_ETTh1_ftM_sl336_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8113
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=100, out_features=157, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14067200.0
params:  15857.0
Trainable parameters:  15857
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.333190441131592
Epoch: 1, Steps: 63 | Train Loss: 0.6450251 Vali Loss: 1.5988089 Test Loss: 0.8361980
Validation loss decreased (inf --> 1.598809).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.4392945766448975
Epoch: 2, Steps: 63 | Train Loss: 0.5022277 Vali Loss: 1.4204307 Test Loss: 0.7291591
Validation loss decreased (1.598809 --> 1.420431).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.368638515472412
Epoch: 3, Steps: 63 | Train Loss: 0.4301941 Vali Loss: 1.3382636 Test Loss: 0.6822518
Validation loss decreased (1.420431 --> 1.338264).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.7533204555511475
Epoch: 4, Steps: 63 | Train Loss: 0.3896504 Vali Loss: 1.2886755 Test Loss: 0.6537403
Validation loss decreased (1.338264 --> 1.288676).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.7326269149780273
Epoch: 5, Steps: 63 | Train Loss: 0.3629197 Vali Loss: 1.2577175 Test Loss: 0.6369183
Validation loss decreased (1.288676 --> 1.257717).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.6342673301696777
Epoch: 6, Steps: 63 | Train Loss: 0.3427190 Vali Loss: 1.2312729 Test Loss: 0.6211759
Validation loss decreased (1.257717 --> 1.231273).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.5651919841766357
Epoch: 7, Steps: 63 | Train Loss: 0.3260630 Vali Loss: 1.2110437 Test Loss: 0.6093816
Validation loss decreased (1.231273 --> 1.211044).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.062937021255493
Epoch: 8, Steps: 63 | Train Loss: 0.3122672 Vali Loss: 1.1939462 Test Loss: 0.5987517
Validation loss decreased (1.211044 --> 1.193946).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.7446789741516113
Epoch: 9, Steps: 63 | Train Loss: 0.3004498 Vali Loss: 1.1790817 Test Loss: 0.5892575
Validation loss decreased (1.193946 --> 1.179082).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.337334156036377
Epoch: 10, Steps: 63 | Train Loss: 0.2897936 Vali Loss: 1.1648552 Test Loss: 0.5796597
Validation loss decreased (1.179082 --> 1.164855).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.357382297515869
Epoch: 11, Steps: 63 | Train Loss: 0.2806251 Vali Loss: 1.1508987 Test Loss: 0.5708702
Validation loss decreased (1.164855 --> 1.150899).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3850314617156982
Epoch: 12, Steps: 63 | Train Loss: 0.2724359 Vali Loss: 1.1394935 Test Loss: 0.5628580
Validation loss decreased (1.150899 --> 1.139493).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.4807116985321045
Epoch: 13, Steps: 63 | Train Loss: 0.2649587 Vali Loss: 1.1289535 Test Loss: 0.5555082
Validation loss decreased (1.139493 --> 1.128953).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.8119466304779053
Epoch: 14, Steps: 63 | Train Loss: 0.2585908 Vali Loss: 1.1200708 Test Loss: 0.5494047
Validation loss decreased (1.128953 --> 1.120071).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.6764605045318604
Epoch: 15, Steps: 63 | Train Loss: 0.2527468 Vali Loss: 1.1110332 Test Loss: 0.5431793
Validation loss decreased (1.120071 --> 1.111033).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.715409278869629
Epoch: 16, Steps: 63 | Train Loss: 0.2474150 Vali Loss: 1.1023188 Test Loss: 0.5371666
Validation loss decreased (1.111033 --> 1.102319).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.5517446994781494
Epoch: 17, Steps: 63 | Train Loss: 0.2427006 Vali Loss: 1.0948106 Test Loss: 0.5322752
Validation loss decreased (1.102319 --> 1.094811).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.151228666305542
Epoch: 18, Steps: 63 | Train Loss: 0.2382265 Vali Loss: 1.0873648 Test Loss: 0.5267286
Validation loss decreased (1.094811 --> 1.087365).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.454369068145752
Epoch: 19, Steps: 63 | Train Loss: 0.2344383 Vali Loss: 1.0810442 Test Loss: 0.5222057
Validation loss decreased (1.087365 --> 1.081044).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.654186010360718
Epoch: 20, Steps: 63 | Train Loss: 0.2306541 Vali Loss: 1.0746355 Test Loss: 0.5181030
Validation loss decreased (1.081044 --> 1.074636).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.327146291732788
Epoch: 21, Steps: 63 | Train Loss: 0.2274172 Vali Loss: 1.0694506 Test Loss: 0.5142379
Validation loss decreased (1.074636 --> 1.069451).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.3172292709350586
Epoch: 22, Steps: 63 | Train Loss: 0.2243155 Vali Loss: 1.0635746 Test Loss: 0.5103760
Validation loss decreased (1.069451 --> 1.063575).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.403515338897705
Epoch: 23, Steps: 63 | Train Loss: 0.2215607 Vali Loss: 1.0590116 Test Loss: 0.5072368
Validation loss decreased (1.063575 --> 1.059012).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.279186487197876
Epoch: 24, Steps: 63 | Train Loss: 0.2189715 Vali Loss: 1.0541136 Test Loss: 0.5035967
Validation loss decreased (1.059012 --> 1.054114).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2742855548858643
Epoch: 25, Steps: 63 | Train Loss: 0.2166038 Vali Loss: 1.0492952 Test Loss: 0.5001212
Validation loss decreased (1.054114 --> 1.049295).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.358038902282715
Epoch: 26, Steps: 63 | Train Loss: 0.2144723 Vali Loss: 1.0454379 Test Loss: 0.4978659
Validation loss decreased (1.049295 --> 1.045438).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.270099401473999
Epoch: 27, Steps: 63 | Train Loss: 0.2123048 Vali Loss: 1.0416269 Test Loss: 0.4949368
Validation loss decreased (1.045438 --> 1.041627).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.8505699634552
Epoch: 28, Steps: 63 | Train Loss: 0.2105147 Vali Loss: 1.0381032 Test Loss: 0.4924867
Validation loss decreased (1.041627 --> 1.038103).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.028031587600708
Epoch: 29, Steps: 63 | Train Loss: 0.2087099 Vali Loss: 1.0356374 Test Loss: 0.4906259
Validation loss decreased (1.038103 --> 1.035637).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.9765353202819824
Epoch: 30, Steps: 63 | Train Loss: 0.2070424 Vali Loss: 1.0320059 Test Loss: 0.4883613
Validation loss decreased (1.035637 --> 1.032006).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 8113
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=100, out_features=157, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14067200.0
params:  15857.0
Trainable parameters:  15857
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.1203012466430664
Epoch: 1, Steps: 63 | Train Loss: 0.4288205 Vali Loss: 0.9644676 Test Loss: 0.4402641
Validation loss decreased (inf --> 0.964468).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.0264666080474854
Epoch: 2, Steps: 63 | Train Loss: 0.4080663 Vali Loss: 0.9386486 Test Loss: 0.4211920
Validation loss decreased (0.964468 --> 0.938649).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.0920650959014893
Epoch: 3, Steps: 63 | Train Loss: 0.3993730 Vali Loss: 0.9278101 Test Loss: 0.4146174
Validation loss decreased (0.938649 --> 0.927810).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.2150323390960693
Epoch: 4, Steps: 63 | Train Loss: 0.3958387 Vali Loss: 0.9240445 Test Loss: 0.4122423
Validation loss decreased (0.927810 --> 0.924045).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2044622898101807
Epoch: 5, Steps: 63 | Train Loss: 0.3944910 Vali Loss: 0.9221042 Test Loss: 0.4118181
Validation loss decreased (0.924045 --> 0.922104).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.1412477493286133
Epoch: 6, Steps: 63 | Train Loss: 0.3945632 Vali Loss: 0.9208072 Test Loss: 0.4112030
Validation loss decreased (0.922104 --> 0.920807).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1725144386291504
Epoch: 7, Steps: 63 | Train Loss: 0.3934449 Vali Loss: 0.9207584 Test Loss: 0.4116489
Validation loss decreased (0.920807 --> 0.920758).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.6048545837402344
Epoch: 8, Steps: 63 | Train Loss: 0.3935041 Vali Loss: 0.9199094 Test Loss: 0.4114127
Validation loss decreased (0.920758 --> 0.919909).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.6128721237182617
Epoch: 9, Steps: 63 | Train Loss: 0.3935519 Vali Loss: 0.9199187 Test Loss: 0.4114826
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.6695282459259033
Epoch: 10, Steps: 63 | Train Loss: 0.3932078 Vali Loss: 0.9204261 Test Loss: 0.4115438
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.251259803771973
Epoch: 11, Steps: 63 | Train Loss: 0.3932421 Vali Loss: 0.9202841 Test Loss: 0.4115794
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.6344895362854004
Epoch: 12, Steps: 63 | Train Loss: 0.3933257 Vali Loss: 0.9191792 Test Loss: 0.4115324
Validation loss decreased (0.919909 --> 0.919179).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.7497665882110596
Epoch: 13, Steps: 63 | Train Loss: 0.3928581 Vali Loss: 0.9192082 Test Loss: 0.4114655
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.8291165828704834
Epoch: 14, Steps: 63 | Train Loss: 0.3930730 Vali Loss: 0.9192563 Test Loss: 0.4114982
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.196606159210205
Epoch: 15, Steps: 63 | Train Loss: 0.3931474 Vali Loss: 0.9191572 Test Loss: 0.4113463
Validation loss decreased (0.919179 --> 0.919157).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.14467716217041
Epoch: 16, Steps: 63 | Train Loss: 0.3932781 Vali Loss: 0.9190689 Test Loss: 0.4113917
Validation loss decreased (0.919157 --> 0.919069).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.0547168254852295
Epoch: 17, Steps: 63 | Train Loss: 0.3930909 Vali Loss: 0.9190851 Test Loss: 0.4113759
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.018784999847412
Epoch: 18, Steps: 63 | Train Loss: 0.3924466 Vali Loss: 0.9192261 Test Loss: 0.4114923
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.601513385772705
Epoch: 19, Steps: 63 | Train Loss: 0.3927857 Vali Loss: 0.9187524 Test Loss: 0.4114491
Validation loss decreased (0.919069 --> 0.918752).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.4371907711029053
Epoch: 20, Steps: 63 | Train Loss: 0.3927973 Vali Loss: 0.9191114 Test Loss: 0.4113474
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.694611072540283
Epoch: 21, Steps: 63 | Train Loss: 0.3926862 Vali Loss: 0.9182515 Test Loss: 0.4113744
Validation loss decreased (0.918752 --> 0.918252).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.027924299240112
Epoch: 22, Steps: 63 | Train Loss: 0.3926515 Vali Loss: 0.9188638 Test Loss: 0.4114190
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.652406692504883
Epoch: 23, Steps: 63 | Train Loss: 0.3925383 Vali Loss: 0.9185262 Test Loss: 0.4114150
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.6695334911346436
Epoch: 24, Steps: 63 | Train Loss: 0.3926587 Vali Loss: 0.9185283 Test Loss: 0.4113740
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.708228588104248
Epoch: 25, Steps: 63 | Train Loss: 0.3923176 Vali Loss: 0.9185945 Test Loss: 0.4112307
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.2776222229003906
Epoch: 26, Steps: 63 | Train Loss: 0.3923657 Vali Loss: 0.9184290 Test Loss: 0.4113792
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_336_192_FITS_ETTh1_ftM_sl336_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4068075716495514, mae:0.41419434547424316, rse:0.6056920289993286, corr:[0.26359788 0.26979452 0.26965323 0.27104524 0.26844686 0.26548707
 0.26512524 0.2647325  0.26365718 0.26370654 0.26404557 0.26360264
 0.26347062 0.2635037  0.2631335  0.2630736  0.2632109  0.26305547
 0.26279938 0.26267263 0.26227415 0.2620223  0.26228756 0.26267728
 0.2623344  0.2621216  0.26204285 0.26169917 0.26132828 0.26121652
 0.26091152 0.2604474  0.2601949  0.26012394 0.26002762 0.259902
 0.2599722  0.2602048  0.2602946  0.26036057 0.26080683 0.26113173
 0.26122427 0.26118073 0.26112023 0.26103228 0.2612234  0.26140484
 0.26086718 0.25976765 0.25890103 0.25829467 0.2573116  0.25622883
 0.2555442  0.25523522 0.2549708  0.2548423  0.254808   0.2546456
 0.25459594 0.25465822 0.25463852 0.25459757 0.25474736 0.25490963
 0.25528303 0.25528824 0.25515744 0.25510493 0.2550785  0.25476414
 0.2540723  0.25304282 0.25224137 0.2518272  0.25140408 0.25087
 0.25054264 0.25011668 0.24955241 0.24927045 0.2492187  0.2490886
 0.24886465 0.24886657 0.24900411 0.24891329 0.24885456 0.24899557
 0.24892557 0.24863558 0.24852857 0.24867886 0.24875776 0.24930577
 0.25011104 0.25035295 0.25035515 0.25027764 0.25024226 0.25018683
 0.24993788 0.24966766 0.24958901 0.24946345 0.24919687 0.24902783
 0.24894674 0.24881044 0.24873531 0.24905914 0.24959402 0.2498994
 0.25003573 0.2499947  0.24974214 0.24950321 0.24941336 0.24945801
 0.24942082 0.24893679 0.24841398 0.2476135  0.2466768  0.24615611
 0.24592349 0.24574484 0.24556386 0.24529235 0.24489999 0.24460977
 0.24456018 0.24454418 0.24440618 0.24431095 0.24459118 0.24495602
 0.2451507  0.24508014 0.24472843 0.24448895 0.24449855 0.24463314
 0.24450506 0.24391405 0.24324189 0.24227826 0.24145569 0.24075897
 0.24057628 0.2405595  0.24055715 0.24054952 0.24073337 0.24077587
 0.24033742 0.24021228 0.24038844 0.24015573 0.24019483 0.24051255
 0.24074674 0.24068847 0.24068742 0.24076742 0.24078342 0.24107791
 0.24160004 0.24144511 0.24149701 0.2419893  0.2417056  0.24076837
 0.24062376 0.2411774  0.2407427  0.2405292  0.24101102 0.24072596
 0.23991983 0.24029155 0.24048503 0.2396881  0.23975402 0.24070372
 0.2404111  0.24036807 0.24140176 0.24094456 0.24168377 0.2405745 ]
