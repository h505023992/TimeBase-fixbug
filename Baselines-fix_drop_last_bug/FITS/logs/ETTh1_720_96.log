Args in experiment:
Namespace(is_training=1, model_id='ETTh1_720_96', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=96, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=196, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  38986752.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.524125099182129
Epoch: 1, Steps: 61 | Train Loss: 0.5976311 Vali Loss: 1.3598828 Test Loss: 0.7532833
Validation loss decreased (inf --> 1.359883).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.226803779602051
Epoch: 2, Steps: 61 | Train Loss: 0.4660741 Vali Loss: 1.2260488 Test Loss: 0.7019938
Validation loss decreased (1.359883 --> 1.226049).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.4779646396636963
Epoch: 3, Steps: 61 | Train Loss: 0.4047865 Vali Loss: 1.1866825 Test Loss: 0.6899001
Validation loss decreased (1.226049 --> 1.186682).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.6061973571777344
Epoch: 4, Steps: 61 | Train Loss: 0.3675742 Vali Loss: 1.1688939 Test Loss: 0.6866564
Validation loss decreased (1.186682 --> 1.168894).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.495645523071289
Epoch: 5, Steps: 61 | Train Loss: 0.3398817 Vali Loss: 1.1514994 Test Loss: 0.6801206
Validation loss decreased (1.168894 --> 1.151499).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.348201274871826
Epoch: 6, Steps: 61 | Train Loss: 0.3172677 Vali Loss: 1.1413733 Test Loss: 0.6739277
Validation loss decreased (1.151499 --> 1.141373).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.4068899154663086
Epoch: 7, Steps: 61 | Train Loss: 0.2980696 Vali Loss: 1.1235018 Test Loss: 0.6652153
Validation loss decreased (1.141373 --> 1.123502).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.3872151374816895
Epoch: 8, Steps: 61 | Train Loss: 0.2813033 Vali Loss: 1.1022537 Test Loss: 0.6549992
Validation loss decreased (1.123502 --> 1.102254).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.5438926219940186
Epoch: 9, Steps: 61 | Train Loss: 0.2665784 Vali Loss: 1.0872506 Test Loss: 0.6470901
Validation loss decreased (1.102254 --> 1.087251).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.955071449279785
Epoch: 10, Steps: 61 | Train Loss: 0.2534625 Vali Loss: 1.0648394 Test Loss: 0.6341623
Validation loss decreased (1.087251 --> 1.064839).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.7212345600128174
Epoch: 11, Steps: 61 | Train Loss: 0.2416633 Vali Loss: 1.0566291 Test Loss: 0.6267722
Validation loss decreased (1.064839 --> 1.056629).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.6192431449890137
Epoch: 12, Steps: 61 | Train Loss: 0.2312005 Vali Loss: 1.0378166 Test Loss: 0.6146964
Validation loss decreased (1.056629 --> 1.037817).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.120323419570923
Epoch: 13, Steps: 61 | Train Loss: 0.2216626 Vali Loss: 1.0215948 Test Loss: 0.6044858
Validation loss decreased (1.037817 --> 1.021595).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.4255735874176025
Epoch: 14, Steps: 61 | Train Loss: 0.2130355 Vali Loss: 1.0081351 Test Loss: 0.5962812
Validation loss decreased (1.021595 --> 1.008135).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.425868034362793
Epoch: 15, Steps: 61 | Train Loss: 0.2051603 Vali Loss: 0.9988575 Test Loss: 0.5883715
Validation loss decreased (1.008135 --> 0.998857).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.332686185836792
Epoch: 16, Steps: 61 | Train Loss: 0.1979788 Vali Loss: 0.9837348 Test Loss: 0.5792991
Validation loss decreased (0.998857 --> 0.983735).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.510239362716675
Epoch: 17, Steps: 61 | Train Loss: 0.1914788 Vali Loss: 0.9729885 Test Loss: 0.5722541
Validation loss decreased (0.983735 --> 0.972988).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.41782283782959
Epoch: 18, Steps: 61 | Train Loss: 0.1853879 Vali Loss: 0.9616024 Test Loss: 0.5661912
Validation loss decreased (0.972988 --> 0.961602).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.4597485065460205
Epoch: 19, Steps: 61 | Train Loss: 0.1799425 Vali Loss: 0.9513585 Test Loss: 0.5586407
Validation loss decreased (0.961602 --> 0.951358).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.495185375213623
Epoch: 20, Steps: 61 | Train Loss: 0.1748108 Vali Loss: 0.9445205 Test Loss: 0.5520732
Validation loss decreased (0.951358 --> 0.944520).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.5218448638916016
Epoch: 21, Steps: 61 | Train Loss: 0.1701417 Vali Loss: 0.9371953 Test Loss: 0.5467408
Validation loss decreased (0.944520 --> 0.937195).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.5855283737182617
Epoch: 22, Steps: 61 | Train Loss: 0.1658626 Vali Loss: 0.9217048 Test Loss: 0.5404999
Validation loss decreased (0.937195 --> 0.921705).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.465686559677124
Epoch: 23, Steps: 61 | Train Loss: 0.1618339 Vali Loss: 0.9174653 Test Loss: 0.5355641
Validation loss decreased (0.921705 --> 0.917465).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.3362886905670166
Epoch: 24, Steps: 61 | Train Loss: 0.1581752 Vali Loss: 0.9062350 Test Loss: 0.5306191
Validation loss decreased (0.917465 --> 0.906235).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.4272255897521973
Epoch: 25, Steps: 61 | Train Loss: 0.1547176 Vali Loss: 0.9067364 Test Loss: 0.5262709
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.968352794647217
Epoch: 26, Steps: 61 | Train Loss: 0.1515133 Vali Loss: 0.8993657 Test Loss: 0.5220424
Validation loss decreased (0.906235 --> 0.899366).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.628868341445923
Epoch: 27, Steps: 61 | Train Loss: 0.1484977 Vali Loss: 0.8942609 Test Loss: 0.5174403
Validation loss decreased (0.899366 --> 0.894261).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.833259344100952
Epoch: 28, Steps: 61 | Train Loss: 0.1457842 Vali Loss: 0.8873588 Test Loss: 0.5138788
Validation loss decreased (0.894261 --> 0.887359).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.0333690643310547
Epoch: 29, Steps: 61 | Train Loss: 0.1432184 Vali Loss: 0.8831928 Test Loss: 0.5114290
Validation loss decreased (0.887359 --> 0.883193).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.483988046646118
Epoch: 30, Steps: 61 | Train Loss: 0.1408058 Vali Loss: 0.8784192 Test Loss: 0.5074795
Validation loss decreased (0.883193 --> 0.878419).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  38986752.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.6286587715148926
Epoch: 1, Steps: 61 | Train Loss: 0.3716268 Vali Loss: 0.7174602 Test Loss: 0.3890544
Validation loss decreased (inf --> 0.717460).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.529792070388794
Epoch: 2, Steps: 61 | Train Loss: 0.3404395 Vali Loss: 0.6985450 Test Loss: 0.3808925
Validation loss decreased (0.717460 --> 0.698545).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.3284387588500977
Epoch: 3, Steps: 61 | Train Loss: 0.3360349 Vali Loss: 0.6996734 Test Loss: 0.3814163
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.288116931915283
Epoch: 4, Steps: 61 | Train Loss: 0.3346785 Vali Loss: 0.6956310 Test Loss: 0.3805164
Validation loss decreased (0.698545 --> 0.695631).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.361854314804077
Epoch: 5, Steps: 61 | Train Loss: 0.3337368 Vali Loss: 0.6970280 Test Loss: 0.3815191
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.341090202331543
Epoch: 6, Steps: 61 | Train Loss: 0.3330472 Vali Loss: 0.6951805 Test Loss: 0.3805664
Validation loss decreased (0.695631 --> 0.695181).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.2632298469543457
Epoch: 7, Steps: 61 | Train Loss: 0.3326101 Vali Loss: 0.6940932 Test Loss: 0.3802170
Validation loss decreased (0.695181 --> 0.694093).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.269458055496216
Epoch: 8, Steps: 61 | Train Loss: 0.3320448 Vali Loss: 0.6966723 Test Loss: 0.3805597
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.4949827194213867
Epoch: 9, Steps: 61 | Train Loss: 0.3319679 Vali Loss: 0.6942043 Test Loss: 0.3808230
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.248307704925537
Epoch: 10, Steps: 61 | Train Loss: 0.3317100 Vali Loss: 0.6938676 Test Loss: 0.3805538
Validation loss decreased (0.694093 --> 0.693868).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.845118284225464
Epoch: 11, Steps: 61 | Train Loss: 0.3313836 Vali Loss: 0.6954737 Test Loss: 0.3801262
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.627528429031372
Epoch: 12, Steps: 61 | Train Loss: 0.3312861 Vali Loss: 0.6941954 Test Loss: 0.3803110
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.6798036098480225
Epoch: 13, Steps: 61 | Train Loss: 0.3310678 Vali Loss: 0.6933099 Test Loss: 0.3804202
Validation loss decreased (0.693868 --> 0.693310).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.103634834289551
Epoch: 14, Steps: 61 | Train Loss: 0.3308876 Vali Loss: 0.6937833 Test Loss: 0.3806068
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.648911952972412
Epoch: 15, Steps: 61 | Train Loss: 0.3308142 Vali Loss: 0.6977570 Test Loss: 0.3806586
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.396907091140747
Epoch: 16, Steps: 61 | Train Loss: 0.3306468 Vali Loss: 0.6978791 Test Loss: 0.3805075
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.3832645416259766
Epoch: 17, Steps: 61 | Train Loss: 0.3304557 Vali Loss: 0.6940368 Test Loss: 0.3801596
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.3439903259277344
Epoch: 18, Steps: 61 | Train Loss: 0.3303775 Vali Loss: 0.6931272 Test Loss: 0.3801427
Validation loss decreased (0.693310 --> 0.693127).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.468158006668091
Epoch: 19, Steps: 61 | Train Loss: 0.3302659 Vali Loss: 0.6939760 Test Loss: 0.3802099
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.5009572505950928
Epoch: 20, Steps: 61 | Train Loss: 0.3302911 Vali Loss: 0.6946620 Test Loss: 0.3799274
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.3980343341827393
Epoch: 21, Steps: 61 | Train Loss: 0.3302714 Vali Loss: 0.6966750 Test Loss: 0.3803019
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.467444658279419
Epoch: 22, Steps: 61 | Train Loss: 0.3301417 Vali Loss: 0.6936494 Test Loss: 0.3801058
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.2915310859680176
Epoch: 23, Steps: 61 | Train Loss: 0.3299126 Vali Loss: 0.6920002 Test Loss: 0.3801268
Validation loss decreased (0.693127 --> 0.692000).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.3555898666381836
Epoch: 24, Steps: 61 | Train Loss: 0.3299542 Vali Loss: 0.6955179 Test Loss: 0.3802907
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.3552911281585693
Epoch: 25, Steps: 61 | Train Loss: 0.3295842 Vali Loss: 0.6926616 Test Loss: 0.3801983
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.640948534011841
Epoch: 26, Steps: 61 | Train Loss: 0.3297847 Vali Loss: 0.6932156 Test Loss: 0.3802181
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.747227191925049
Epoch: 27, Steps: 61 | Train Loss: 0.3296527 Vali Loss: 0.6908513 Test Loss: 0.3802018
Validation loss decreased (0.692000 --> 0.690851).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.5445661544799805
Epoch: 28, Steps: 61 | Train Loss: 0.3294556 Vali Loss: 0.6952547 Test Loss: 0.3803046
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.758922576904297
Epoch: 29, Steps: 61 | Train Loss: 0.3298171 Vali Loss: 0.6918328 Test Loss: 0.3802380
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.43855357170105
Epoch: 30, Steps: 61 | Train Loss: 0.3295835 Vali Loss: 0.6966101 Test Loss: 0.3800822
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00011296777049628277
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.37980711460113525, mae:0.40231090784072876, rse:0.5853820443153381, corr:[0.27035546 0.27818802 0.2786885  0.27837434 0.27626657 0.27325553
 0.27153826 0.27130324 0.27081844 0.27055323 0.2709381  0.27099606
 0.27059832 0.270324   0.2704065  0.27044567 0.27036032 0.27057552
 0.27049187 0.2696188  0.2687388  0.26867935 0.2687259  0.2688969
 0.26883826 0.2685309  0.26836726 0.26845565 0.2683191  0.26771468
 0.26725617 0.26702172 0.2668452  0.26659623 0.26638693 0.2665716
 0.2667502  0.26668987 0.26670668 0.26677302 0.26690826 0.26699615
 0.26708063 0.26703686 0.26671544 0.26650476 0.2669957  0.26768735
 0.26727661 0.2660369  0.2646722  0.2639686  0.2633799  0.26193863
 0.26053604 0.2599357  0.25984982 0.25981724 0.25920212 0.25890115
 0.2589161  0.25904265 0.25885376 0.25851378 0.2582419  0.2582261
 0.25871754 0.25882113 0.25832003 0.25797823 0.2582244  0.25821105
 0.25739193 0.25621286 0.2554274  0.2552734  0.25476745 0.2540498
 0.25364664 0.25303602 0.25196666 0.2513141  0.2510337  0.2503837
 0.2498919  0.25018692 0.25041065 0.24950393 0.24910787 0.24982263
 0.2494212  0.248285   0.2490315  0.24987371 0.25005665 0.25467068]
