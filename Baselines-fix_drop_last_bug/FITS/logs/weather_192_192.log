Args in experiment:
Namespace(is_training=1, model_id='weather_192_192', model='FITS', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=3, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=34, base_T=144, H_order=12)
Use GPU: cuda:0
>>>>>>>start training : weather_192_192_FITS_custom_ftM_sl192_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36504
val 5079
test 10348
Model(
  (freq_upsampler): Linear(in_features=34, out_features=68, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6214656.0
params:  2380.0
Trainable parameters:  2380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6282060
	speed: 0.1091s/iter; left time: 922.2624s
	iters: 200, epoch: 1 | loss: 0.4382271
	speed: 0.0944s/iter; left time: 788.1695s
Epoch: 1 cost time: 28.3911030292511
Epoch: 1, Steps: 285 | Train Loss: 0.5792015 Vali Loss: 0.6143816 Test Loss: 0.2623385
Validation loss decreased (inf --> 0.614382).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3769038
	speed: 0.3233s/iter; left time: 2640.1694s
	iters: 200, epoch: 2 | loss: 0.3310906
	speed: 0.0772s/iter; left time: 622.7813s
Epoch: 2 cost time: 22.675801992416382
Epoch: 2, Steps: 285 | Train Loss: 0.3832506 Vali Loss: 0.5618482 Test Loss: 0.2451932
Validation loss decreased (0.614382 --> 0.561848).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3270262
	speed: 0.3338s/iter; left time: 2630.5302s
	iters: 200, epoch: 3 | loss: 0.3269725
	speed: 0.0859s/iter; left time: 668.0160s
Epoch: 3 cost time: 25.641699075698853
Epoch: 3, Steps: 285 | Train Loss: 0.3390756 Vali Loss: 0.5464867 Test Loss: 0.2392026
Validation loss decreased (0.561848 --> 0.546487).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2478952
	speed: 0.3455s/iter; left time: 2624.5987s
	iters: 200, epoch: 4 | loss: 0.3695274
	speed: 0.0878s/iter; left time: 658.4944s
Epoch: 4 cost time: 25.958043575286865
Epoch: 4, Steps: 285 | Train Loss: 0.3237858 Vali Loss: 0.5398165 Test Loss: 0.2361101
Validation loss decreased (0.546487 --> 0.539816).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2670573
	speed: 0.3374s/iter; left time: 2466.4680s
	iters: 200, epoch: 5 | loss: 0.4153782
	speed: 0.0863s/iter; left time: 622.4709s
Epoch: 5 cost time: 26.111541271209717
Epoch: 5, Steps: 285 | Train Loss: 0.3180295 Vali Loss: 0.5371078 Test Loss: 0.2342725
Validation loss decreased (0.539816 --> 0.537108).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2479752
	speed: 0.3401s/iter; left time: 2389.4436s
	iters: 200, epoch: 6 | loss: 0.3211970
	speed: 0.0919s/iter; left time: 636.2537s
Epoch: 6 cost time: 25.156596183776855
Epoch: 6, Steps: 285 | Train Loss: 0.3154623 Vali Loss: 0.5341605 Test Loss: 0.2334414
Validation loss decreased (0.537108 --> 0.534160).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3349819
	speed: 0.3085s/iter; left time: 2079.4884s
	iters: 200, epoch: 7 | loss: 0.3804061
	speed: 0.0886s/iter; left time: 588.4151s
Epoch: 7 cost time: 25.171182870864868
Epoch: 7, Steps: 285 | Train Loss: 0.3143557 Vali Loss: 0.5340458 Test Loss: 0.2328048
Validation loss decreased (0.534160 --> 0.534046).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2396655
	speed: 0.3351s/iter; left time: 2163.5560s
	iters: 200, epoch: 8 | loss: 0.2574680
	speed: 0.0781s/iter; left time: 496.2612s
Epoch: 8 cost time: 24.28392195701599
Epoch: 8, Steps: 285 | Train Loss: 0.3139081 Vali Loss: 0.5334908 Test Loss: 0.2325149
Validation loss decreased (0.534046 --> 0.533491).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2668834
	speed: 0.3108s/iter; left time: 1918.0534s
	iters: 200, epoch: 9 | loss: 0.3876335
	speed: 0.0839s/iter; left time: 509.2539s
Epoch: 9 cost time: 25.43953847885132
Epoch: 9, Steps: 285 | Train Loss: 0.3135667 Vali Loss: 0.5326768 Test Loss: 0.2322177
Validation loss decreased (0.533491 --> 0.532677).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2918678
	speed: 0.3512s/iter; left time: 2067.1341s
	iters: 200, epoch: 10 | loss: 0.2588549
	speed: 0.0923s/iter; left time: 534.2394s
Epoch: 10 cost time: 26.595409154891968
Epoch: 10, Steps: 285 | Train Loss: 0.3134823 Vali Loss: 0.5311425 Test Loss: 0.2320963
Validation loss decreased (0.532677 --> 0.531143).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2826860
	speed: 0.3496s/iter; left time: 1958.3652s
	iters: 200, epoch: 11 | loss: 0.3637585
	speed: 0.0900s/iter; left time: 494.8751s
Epoch: 11 cost time: 26.182709217071533
Epoch: 11, Steps: 285 | Train Loss: 0.3132406 Vali Loss: 0.5319011 Test Loss: 0.2319789
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4011783
	speed: 0.3560s/iter; left time: 1892.7456s
	iters: 200, epoch: 12 | loss: 0.3189500
	speed: 0.0895s/iter; left time: 466.9273s
Epoch: 12 cost time: 26.80256152153015
Epoch: 12, Steps: 285 | Train Loss: 0.3133637 Vali Loss: 0.5334255 Test Loss: 0.2319771
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2374339
	speed: 0.3524s/iter; left time: 1772.9460s
	iters: 200, epoch: 13 | loss: 0.2571349
	speed: 0.0857s/iter; left time: 422.7460s
Epoch: 13 cost time: 26.494980335235596
Epoch: 13, Steps: 285 | Train Loss: 0.3133671 Vali Loss: 0.5298316 Test Loss: 0.2319073
Validation loss decreased (0.531143 --> 0.529832).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2849674
	speed: 0.3594s/iter; left time: 1705.5653s
	iters: 200, epoch: 14 | loss: 0.3389641
	speed: 0.0861s/iter; left time: 400.0478s
Epoch: 14 cost time: 26.498079776763916
Epoch: 14, Steps: 285 | Train Loss: 0.3133472 Vali Loss: 0.5345809 Test Loss: 0.2318183
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3345611
	speed: 0.3498s/iter; left time: 1560.4400s
	iters: 200, epoch: 15 | loss: 0.2683347
	speed: 0.0890s/iter; left time: 388.2332s
Epoch: 15 cost time: 26.21946620941162
Epoch: 15, Steps: 285 | Train Loss: 0.3133249 Vali Loss: 0.5336564 Test Loss: 0.2318504
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3934391
	speed: 0.3428s/iter; left time: 1431.4607s
	iters: 200, epoch: 16 | loss: 0.3842295
	speed: 0.0847s/iter; left time: 345.2028s
Epoch: 16 cost time: 25.94475769996643
Epoch: 16, Steps: 285 | Train Loss: 0.3133019 Vali Loss: 0.5303060 Test Loss: 0.2317069
EarlyStopping counter: 3 out of 3
Early stopping
train 36504
val 5079
test 10348
Model(
  (freq_upsampler): Linear(in_features=34, out_features=68, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6214656.0
params:  2380.0
Trainable parameters:  2380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4561391
	speed: 0.1023s/iter; left time: 864.3417s
	iters: 200, epoch: 1 | loss: 0.5579620
	speed: 0.0865s/iter; left time: 722.1742s
Epoch: 1 cost time: 26.201493501663208
Epoch: 1, Steps: 285 | Train Loss: 0.5430246 Vali Loss: 0.5257420 Test Loss: 0.2305018
Validation loss decreased (inf --> 0.525742).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4682097
	speed: 0.3425s/iter; left time: 2797.1353s
	iters: 200, epoch: 2 | loss: 0.5062769
	speed: 0.0822s/iter; left time: 662.9030s
Epoch: 2 cost time: 26.743456840515137
Epoch: 2, Steps: 285 | Train Loss: 0.5407007 Vali Loss: 0.5278134 Test Loss: 0.2307183
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5348194
	speed: 0.3567s/iter; left time: 2810.9216s
	iters: 200, epoch: 3 | loss: 0.5490354
	speed: 0.0764s/iter; left time: 594.7858s
Epoch: 3 cost time: 23.81298303604126
Epoch: 3, Steps: 285 | Train Loss: 0.5395954 Vali Loss: 0.5235778 Test Loss: 0.2298508
Validation loss decreased (0.525742 --> 0.523578).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4099812
	speed: 0.2769s/iter; left time: 2103.4845s
	iters: 200, epoch: 4 | loss: 0.4822517
	speed: 0.0492s/iter; left time: 369.1286s
Epoch: 4 cost time: 17.157222032546997
Epoch: 4, Steps: 285 | Train Loss: 0.5395132 Vali Loss: 0.5258059 Test Loss: 0.2299830
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6169840
	speed: 0.3002s/iter; left time: 2194.5478s
	iters: 200, epoch: 5 | loss: 0.5095703
	speed: 0.0897s/iter; left time: 646.8377s
Epoch: 5 cost time: 25.403581619262695
Epoch: 5, Steps: 285 | Train Loss: 0.5395846 Vali Loss: 0.5242727 Test Loss: 0.2298366
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6071306
	speed: 0.3302s/iter; left time: 2320.0135s
	iters: 200, epoch: 6 | loss: 0.6057473
	speed: 0.0839s/iter; left time: 581.3226s
Epoch: 6 cost time: 25.52848744392395
Epoch: 6, Steps: 285 | Train Loss: 0.5395169 Vali Loss: 0.5217382 Test Loss: 0.2298681
Validation loss decreased (0.523578 --> 0.521738).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4566031
	speed: 0.3476s/iter; left time: 2342.8932s
	iters: 200, epoch: 7 | loss: 0.6156825
	speed: 0.0863s/iter; left time: 573.0743s
Epoch: 7 cost time: 25.23822569847107
Epoch: 7, Steps: 285 | Train Loss: 0.5394564 Vali Loss: 0.5250489 Test Loss: 0.2297082
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4580155
	speed: 0.3461s/iter; left time: 2234.4200s
	iters: 200, epoch: 8 | loss: 0.4631816
	speed: 0.0893s/iter; left time: 567.8696s
Epoch: 8 cost time: 26.573843955993652
Epoch: 8, Steps: 285 | Train Loss: 0.5389168 Vali Loss: 0.5240954 Test Loss: 0.2296605
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.8163721
	speed: 0.3523s/iter; left time: 2174.2580s
	iters: 200, epoch: 9 | loss: 0.4629307
	speed: 0.0890s/iter; left time: 540.0758s
Epoch: 9 cost time: 25.5748929977417
Epoch: 9, Steps: 285 | Train Loss: 0.5388424 Vali Loss: 0.5253858 Test Loss: 0.2297684
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : weather_192_192_FITS_custom_ftM_sl192_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10348
mse:0.23008114099502563, mae:0.2662409245967865, rse:0.6314058899879456, corr:[0.48403525 0.48533335 0.48337927 0.48214915 0.48204803 0.48157427
 0.48014152 0.47839752 0.47704208 0.47615984 0.4753372  0.47433013
 0.473104   0.4719684  0.47097003 0.46994498 0.46870613 0.46722084
 0.46606147 0.46513042 0.46418643 0.4631652  0.46201426 0.4608266
 0.45975107 0.45861134 0.45733806 0.45571986 0.4540447  0.4524865
 0.45114356 0.44975775 0.44834754 0.4469384  0.44573498 0.44460174
 0.4435726  0.44243416 0.44133562 0.44030547 0.43948576 0.43888816
 0.43820387 0.43731952 0.436318   0.43536457 0.43473318 0.43422794
 0.43350628 0.43254307 0.4317518  0.4310581  0.43059516 0.4301773
 0.42974395 0.42916152 0.42871162 0.42820612 0.42788833 0.4276319
 0.427389   0.4272343  0.42716601 0.42703745 0.4268934  0.42675835
 0.42639166 0.42604753 0.42577812 0.42563215 0.4255564  0.42539683
 0.42529857 0.42536905 0.425484   0.42559135 0.4255333  0.42548525
 0.42550555 0.4255843  0.4258169  0.42605704 0.42609885 0.42603117
 0.42595008 0.42592424 0.4260074  0.42597735 0.42595118 0.42591587
 0.4259498  0.42609918 0.42630705 0.42656204 0.42679125 0.42695442
 0.42712966 0.42729318 0.42744595 0.42751876 0.42751166 0.42746794
 0.42746723 0.42748606 0.42745295 0.42733386 0.42711753 0.42695594
 0.42689994 0.4269034  0.42687798 0.42680183 0.42659575 0.4263734
 0.42612135 0.42582783 0.42552307 0.4252637  0.42504    0.42480278
 0.42453316 0.4242414  0.42392766 0.42363128 0.4233798  0.42314523
 0.4228551  0.42247158 0.42208624 0.42171928 0.42132807 0.42087993
 0.42035678 0.41978735 0.41932786 0.4189776  0.4187265  0.41844586
 0.41803402 0.41746378 0.41682464 0.41619122 0.41568294 0.41519538
 0.41466594 0.4140809  0.41341716 0.41270682 0.41192394 0.41098675
 0.40994966 0.40891227 0.40800825 0.40731242 0.40659374 0.4056541
 0.40460172 0.40352568 0.40250582 0.401625   0.40074942 0.39979973
 0.39886343 0.39794108 0.39714026 0.39650044 0.39566904 0.3944538
 0.39308372 0.3917009  0.39075777 0.3901197  0.38938048 0.38832262
 0.3870334  0.38605994 0.38556063 0.38537166 0.38497487 0.38414016
 0.38318714 0.38254604 0.38229483 0.38202152 0.38138786 0.38077068
 0.38058582 0.38081345 0.38100556 0.3806902  0.3796486  0.378628  ]
