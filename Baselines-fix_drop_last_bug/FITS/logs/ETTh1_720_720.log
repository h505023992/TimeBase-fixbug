Args in experiment:
Namespace(is_training=1, model_id='ETTh1_720_720', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=196, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.127561330795288
Epoch: 1, Steps: 56 | Train Loss: 0.8054126 Vali Loss: 2.0307627 Test Loss: 0.8378431
Validation loss decreased (inf --> 2.030763).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.976755380630493
Epoch: 2, Steps: 56 | Train Loss: 0.6231347 Vali Loss: 1.8413919 Test Loss: 0.7262427
Validation loss decreased (2.030763 --> 1.841392).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.020212411880493
Epoch: 3, Steps: 56 | Train Loss: 0.5488310 Vali Loss: 1.7670763 Test Loss: 0.6850778
Validation loss decreased (1.841392 --> 1.767076).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.4425735473632812
Epoch: 4, Steps: 56 | Train Loss: 0.5111858 Vali Loss: 1.7326943 Test Loss: 0.6637963
Validation loss decreased (1.767076 --> 1.732694).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.672427177429199
Epoch: 5, Steps: 56 | Train Loss: 0.4869982 Vali Loss: 1.7109001 Test Loss: 0.6488845
Validation loss decreased (1.732694 --> 1.710900).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.65712308883667
Epoch: 6, Steps: 56 | Train Loss: 0.4680378 Vali Loss: 1.6914943 Test Loss: 0.6374792
Validation loss decreased (1.710900 --> 1.691494).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.6006088256835938
Epoch: 7, Steps: 56 | Train Loss: 0.4527537 Vali Loss: 1.6767135 Test Loss: 0.6260486
Validation loss decreased (1.691494 --> 1.676713).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.691652536392212
Epoch: 8, Steps: 56 | Train Loss: 0.4396718 Vali Loss: 1.6542737 Test Loss: 0.6163942
Validation loss decreased (1.676713 --> 1.654274).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.7056524753570557
Epoch: 9, Steps: 56 | Train Loss: 0.4282870 Vali Loss: 1.6464224 Test Loss: 0.6074987
Validation loss decreased (1.654274 --> 1.646422).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.7186548709869385
Epoch: 10, Steps: 56 | Train Loss: 0.4183561 Vali Loss: 1.6457355 Test Loss: 0.5991874
Validation loss decreased (1.646422 --> 1.645736).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.5312533378601074
Epoch: 11, Steps: 56 | Train Loss: 0.4094821 Vali Loss: 1.6269327 Test Loss: 0.5910442
Validation loss decreased (1.645736 --> 1.626933).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.5890533924102783
Epoch: 12, Steps: 56 | Train Loss: 0.4017475 Vali Loss: 1.6165409 Test Loss: 0.5844679
Validation loss decreased (1.626933 --> 1.616541).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.676570177078247
Epoch: 13, Steps: 56 | Train Loss: 0.3948394 Vali Loss: 1.6134535 Test Loss: 0.5774056
Validation loss decreased (1.616541 --> 1.613454).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.689631462097168
Epoch: 14, Steps: 56 | Train Loss: 0.3886144 Vali Loss: 1.6047503 Test Loss: 0.5717059
Validation loss decreased (1.613454 --> 1.604750).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.1202878952026367
Epoch: 15, Steps: 56 | Train Loss: 0.3830883 Vali Loss: 1.5904493 Test Loss: 0.5660796
Validation loss decreased (1.604750 --> 1.590449).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5408434867858887
Epoch: 16, Steps: 56 | Train Loss: 0.3778667 Vali Loss: 1.5880873 Test Loss: 0.5610830
Validation loss decreased (1.590449 --> 1.588087).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.7092370986938477
Epoch: 17, Steps: 56 | Train Loss: 0.3733130 Vali Loss: 1.5827527 Test Loss: 0.5559000
Validation loss decreased (1.588087 --> 1.582753).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.3762331008911133
Epoch: 18, Steps: 56 | Train Loss: 0.3691765 Vali Loss: 1.5743659 Test Loss: 0.5514229
Validation loss decreased (1.582753 --> 1.574366).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.7126195430755615
Epoch: 19, Steps: 56 | Train Loss: 0.3651557 Vali Loss: 1.5744779 Test Loss: 0.5472782
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.4765126705169678
Epoch: 20, Steps: 56 | Train Loss: 0.3618162 Vali Loss: 1.5693662 Test Loss: 0.5435532
Validation loss decreased (1.574366 --> 1.569366).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.5850234031677246
Epoch: 21, Steps: 56 | Train Loss: 0.3585713 Vali Loss: 1.5644450 Test Loss: 0.5399736
Validation loss decreased (1.569366 --> 1.564445).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.472653388977051
Epoch: 22, Steps: 56 | Train Loss: 0.3559582 Vali Loss: 1.5523634 Test Loss: 0.5362995
Validation loss decreased (1.564445 --> 1.552363).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.4460155963897705
Epoch: 23, Steps: 56 | Train Loss: 0.3532851 Vali Loss: 1.5468307 Test Loss: 0.5332084
Validation loss decreased (1.552363 --> 1.546831).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.353092670440674
Epoch: 24, Steps: 56 | Train Loss: 0.3508700 Vali Loss: 1.5540617 Test Loss: 0.5302748
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.473318099975586
Epoch: 25, Steps: 56 | Train Loss: 0.3485248 Vali Loss: 1.5454502 Test Loss: 0.5276695
Validation loss decreased (1.546831 --> 1.545450).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.3940443992614746
Epoch: 26, Steps: 56 | Train Loss: 0.3465169 Vali Loss: 1.5444921 Test Loss: 0.5249720
Validation loss decreased (1.545450 --> 1.544492).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.5003044605255127
Epoch: 27, Steps: 56 | Train Loss: 0.3443010 Vali Loss: 1.5374089 Test Loss: 0.5228103
Validation loss decreased (1.544492 --> 1.537409).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.4916017055511475
Epoch: 28, Steps: 56 | Train Loss: 0.3427297 Vali Loss: 1.5375226 Test Loss: 0.5204437
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.5867550373077393
Epoch: 29, Steps: 56 | Train Loss: 0.3410915 Vali Loss: 1.5380446 Test Loss: 0.5182895
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.37837553024292
Epoch: 30, Steps: 56 | Train Loss: 0.3394004 Vali Loss: 1.5317074 Test Loss: 0.5165485
Validation loss decreased (1.537409 --> 1.531707).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.435823440551758
Epoch: 1, Steps: 56 | Train Loss: 0.5934207 Vali Loss: 1.4900999 Test Loss: 0.4814507
Validation loss decreased (inf --> 1.490100).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.4708878993988037
Epoch: 2, Steps: 56 | Train Loss: 0.5766857 Vali Loss: 1.4692216 Test Loss: 0.4597950
Validation loss decreased (1.490100 --> 1.469222).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.579601764678955
Epoch: 3, Steps: 56 | Train Loss: 0.5664828 Vali Loss: 1.4519826 Test Loss: 0.4470346
Validation loss decreased (1.469222 --> 1.451983).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.4325344562530518
Epoch: 4, Steps: 56 | Train Loss: 0.5604781 Vali Loss: 1.4405396 Test Loss: 0.4394189
Validation loss decreased (1.451983 --> 1.440540).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.740135669708252
Epoch: 5, Steps: 56 | Train Loss: 0.5565508 Vali Loss: 1.4356539 Test Loss: 0.4353104
Validation loss decreased (1.440540 --> 1.435654).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.752885341644287
Epoch: 6, Steps: 56 | Train Loss: 0.5539955 Vali Loss: 1.4343925 Test Loss: 0.4333987
Validation loss decreased (1.435654 --> 1.434392).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.747495651245117
Epoch: 7, Steps: 56 | Train Loss: 0.5528580 Vali Loss: 1.4339442 Test Loss: 0.4323670
Validation loss decreased (1.434392 --> 1.433944).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.7419190406799316
Epoch: 8, Steps: 56 | Train Loss: 0.5510850 Vali Loss: 1.4350808 Test Loss: 0.4328883
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.3711488246917725
Epoch: 9, Steps: 56 | Train Loss: 0.5505583 Vali Loss: 1.4310658 Test Loss: 0.4319445
Validation loss decreased (1.433944 --> 1.431066).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.4606072902679443
Epoch: 10, Steps: 56 | Train Loss: 0.5502792 Vali Loss: 1.4389120 Test Loss: 0.4320384
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.478283405303955
Epoch: 11, Steps: 56 | Train Loss: 0.5497534 Vali Loss: 1.4350469 Test Loss: 0.4322276
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3612442016601562
Epoch: 12, Steps: 56 | Train Loss: 0.5495986 Vali Loss: 1.4360073 Test Loss: 0.4323640
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.4072306156158447
Epoch: 13, Steps: 56 | Train Loss: 0.5490932 Vali Loss: 1.4325342 Test Loss: 0.4326256
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.475773572921753
Epoch: 14, Steps: 56 | Train Loss: 0.5490385 Vali Loss: 1.4400277 Test Loss: 0.4328937
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4330028851032257, mae:0.4574801881313324, rse:0.628480851650238, corr:[0.22441061 0.23182756 0.23112516 0.23347817 0.23322025 0.23002486
 0.22959737 0.231712   0.23177509 0.23070934 0.23069675 0.23143934
 0.23131926 0.2302779  0.22961508 0.22964604 0.22951365 0.22908616
 0.2289715  0.2292021  0.22932681 0.2292477  0.22923593 0.22987428
 0.23033006 0.22999118 0.22950113 0.22970353 0.23033065 0.23043135
 0.23022532 0.23018117 0.2300602  0.22925517 0.22807729 0.22758906
 0.22813909 0.22866654 0.22847041 0.22776835 0.22754459 0.22794421
 0.22838381 0.2283031  0.22823821 0.22892328 0.22996484 0.23039597
 0.23024587 0.23005717 0.22961973 0.22883992 0.22782652 0.22665545
 0.22587118 0.22543243 0.22526509 0.22505566 0.22461213 0.22455464
 0.22456686 0.22415966 0.22348222 0.22328891 0.22352192 0.22370794
 0.22383626 0.22395569 0.22386627 0.2235853  0.22340795 0.22366878
 0.22357287 0.2229659  0.22245233 0.22240748 0.22198153 0.22096923
 0.22048579 0.22082214 0.22101855 0.2205254  0.21977344 0.21943502
 0.21941842 0.21918559 0.21877141 0.21849157 0.21843942 0.21849056
 0.21833624 0.21825436 0.21852922 0.21900144 0.21919721 0.21960923
 0.220766   0.22204433 0.22254342 0.22253722 0.22260118 0.22282214
 0.22270456 0.22229919 0.22204742 0.22200234 0.22155574 0.22081023
 0.2205635  0.22083108 0.22111337 0.22109027 0.22091173 0.22077666
 0.2209528  0.2212565  0.2213847  0.22128078 0.2213545  0.22184809
 0.22206384 0.22127151 0.2200415  0.21940377 0.21900953 0.21862692
 0.21877368 0.21927913 0.2190312  0.21812405 0.2175772  0.21777023
 0.21804884 0.21782394 0.21753979 0.21761735 0.21775603 0.21755126
 0.2172063  0.21703076 0.21714517 0.21730822 0.21708658 0.21683711
 0.21696393 0.21709058 0.21673115 0.21575412 0.21494852 0.21480878
 0.21479604 0.21438465 0.2138664  0.21379526 0.21398745 0.21414186
 0.2142467  0.21433212 0.21429841 0.21403378 0.21374376 0.21375379
 0.21391352 0.2140206  0.21399622 0.21410017 0.21424924 0.21461248
 0.21484388 0.21487969 0.21506694 0.2156904  0.21600583 0.2155133
 0.21469079 0.21438532 0.21457379 0.2147308  0.21458171 0.21424577
 0.21395236 0.21396229 0.21434793 0.21471734 0.21483201 0.21496484
 0.21537542 0.21587056 0.21621794 0.21610929 0.21564044 0.2152612
 0.215204   0.21510759 0.21443716 0.21347974 0.21288683 0.21283443
 0.21286814 0.2127078  0.21236919 0.21197906 0.2116923  0.21165094
 0.21168733 0.21181692 0.21214382 0.21256366 0.21270181 0.2123779
 0.21193141 0.21180959 0.21208684 0.21238272 0.21241762 0.21206549
 0.21136297 0.21087709 0.21113569 0.21176195 0.21166947 0.21081461
 0.21050566 0.21085016 0.21072084 0.2099028  0.20927095 0.20919101
 0.2091683  0.2091261  0.20931894 0.20935428 0.20900512 0.20884195
 0.20905818 0.20899561 0.20842344 0.2081363  0.20879617 0.20964438
 0.20995884 0.20958312 0.20926961 0.20932697 0.20938452 0.2093194
 0.20936094 0.20939392 0.20920305 0.20880559 0.20836101 0.20798081
 0.20786053 0.20803842 0.20851198 0.20877579 0.20861001 0.20838025
 0.2083883  0.20830308 0.20808187 0.20785776 0.20782113 0.20818004
 0.2086342  0.20874003 0.2082427  0.20733356 0.20655131 0.206505
 0.2067461  0.2064581  0.20592028 0.20600252 0.20632207 0.20627902
 0.20605332 0.20618053 0.20640838 0.20641506 0.2064113  0.2067122
 0.20695499 0.2069255  0.20664483 0.20641555 0.20649469 0.20724446
 0.2081276  0.20852594 0.20846163 0.20840329 0.2085152  0.20884368
 0.2091763  0.20936817 0.20921376 0.2087872  0.2082998  0.20804639
 0.2080681  0.20818965 0.20827076 0.20824204 0.20837945 0.20890476
 0.20947206 0.20961101 0.20963186 0.20987228 0.20992476 0.20971729
 0.20966049 0.20992206 0.20992634 0.2093829  0.20889251 0.2090043
 0.20891152 0.20805454 0.20723699 0.20709671 0.20682897 0.20630151
 0.20648617 0.2074386  0.20794933 0.2079675  0.20802328 0.20810601
 0.20764491 0.20705944 0.2071059  0.2073684  0.20714788 0.20684052
 0.20698863 0.20710878 0.20696218 0.20675717 0.20639367 0.20576191
 0.2053174  0.20518616 0.20478265 0.20405105 0.20378311 0.20428364
 0.20481995 0.20505892 0.20503378 0.20458509 0.20379192 0.20393316
 0.20513453 0.20572127 0.20506112 0.20458163 0.20494403 0.20539764
 0.20521845 0.20482679 0.20466372 0.2044596  0.20401616 0.20337439
 0.2025028  0.20183864 0.20216836 0.20283137 0.20210427 0.20039546
 0.20010537 0.20139591 0.20171648 0.200311   0.19930299 0.19974644
 0.20018941 0.19996555 0.20013143 0.20101781 0.20127994 0.20106581
 0.20136708 0.20221883 0.20295057 0.20335738 0.20304765 0.20177981
 0.20062876 0.20025788 0.20007099 0.19957402 0.19929445 0.19989426
 0.20052364 0.2005936  0.20073584 0.20102584 0.20101465 0.2008987
 0.20105603 0.20119205 0.20098345 0.20096195 0.20143403 0.20206285
 0.20266825 0.20357835 0.20448455 0.2042998  0.20309518 0.20228872
 0.20265982 0.2031828  0.20317419 0.2026897  0.20183659 0.20103185
 0.20089953 0.20124456 0.20109026 0.20056851 0.2001241  0.19970137
 0.19954745 0.20042415 0.20158462 0.20206384 0.20173971 0.20205349
 0.20313784 0.20350778 0.20320971 0.20344885 0.20380875 0.20330165
 0.20258515 0.20243777 0.20190154 0.20082627 0.20055236 0.20147017
 0.20182344 0.20081669 0.19994126 0.20007741 0.2004439  0.20063765
 0.20095745 0.20111227 0.20086367 0.20071413 0.20108798 0.2018225
 0.20246312 0.20328775 0.20417392 0.20432933 0.20351242 0.20234701
 0.20169607 0.20173648 0.20197754 0.20163694 0.20070057 0.200768
 0.20218782 0.20300199 0.20206486 0.20107044 0.20155308 0.20250697
 0.20267184 0.20264857 0.20274939 0.20206785 0.20140807 0.20194462
 0.2028215  0.20225565 0.20099643 0.20104827 0.20161371 0.20091861
 0.1997632  0.19966976 0.19979554 0.19920397 0.19858547 0.19893946
 0.19958057 0.19949219 0.19898465 0.19851933 0.19834365 0.19881052
 0.19976377 0.2003102  0.20005861 0.19964822 0.19966671 0.20025685
 0.20114319 0.20192552 0.2021216  0.20207468 0.20240627 0.20279387
 0.20274898 0.20240815 0.20221332 0.20200229 0.2014339  0.20107768
 0.20138451 0.2018391  0.20220248 0.2024561  0.20257476 0.20211604
 0.20160559 0.20175341 0.20240219 0.20254673 0.2030136  0.20369595
 0.20434254 0.2042712  0.20389938 0.2034735  0.2023422  0.20126241
 0.20091182 0.2004636  0.1992648  0.19905576 0.20041744 0.20168212
 0.20185055 0.20198075 0.20270269 0.20281768 0.20226629 0.20191249
 0.20168209 0.20111799 0.20087473 0.201284   0.20104344 0.19983388
 0.19933492 0.20004314 0.19997142 0.19845936 0.19715317 0.19677703
 0.1965196  0.19616085 0.19568099 0.19439806 0.19258311 0.19193467
 0.19255532 0.19292688 0.19316728 0.19378467 0.19426377 0.19350843
 0.19286296 0.19350082 0.19463076 0.19465856 0.1939837  0.19339688
 0.19322723 0.19395277 0.19492404 0.19465654 0.19259572 0.19127221
 0.1922125  0.19248173 0.19088209 0.19001773 0.19100086 0.1911089
 0.1900979  0.19023821 0.19162637 0.19172093 0.19115321 0.19189449
 0.19243848 0.19072448 0.1890176  0.19008204 0.19177236 0.19105865
 0.18914674 0.18811017 0.18781221 0.18733446 0.18669264 0.185834
 0.18487407 0.18439108 0.18422861 0.18348588 0.18235973 0.18258144
 0.18421586 0.1851744  0.18468755 0.18379146 0.18315166 0.18286169
 0.1833888  0.18416575 0.18415098 0.18380542 0.18445657 0.18542287
 0.18538103 0.18501215 0.18534826 0.18546923 0.18424518 0.18296003
 0.1829279  0.18251766 0.1805788  0.17884059 0.1785135  0.17914745
 0.17981474 0.17999727 0.1794782  0.17872593 0.17885624 0.17962022
 0.17947507 0.17861797 0.17847782 0.17929153 0.18012333 0.1803636
 0.17972301 0.17850968 0.17813274 0.17959873 0.18046497 0.17896649
 0.17734466 0.17784217 0.17880328 0.1773717  0.17494543 0.17441024
 0.17582485 0.17727862 0.1777842  0.17671657 0.17473835 0.17381206
 0.17470498 0.17573848 0.17560647 0.17552519 0.17567159 0.17482795
 0.17302187 0.1717225  0.17061076 0.16935848 0.1692352  0.16943547
 0.16864179 0.16721031 0.16711679 0.16683221 0.16478151 0.16333804
 0.1644978  0.16586158 0.16563328 0.16545801 0.16623773 0.16596943
 0.16521773 0.16641048 0.16788867 0.1667006  0.16538048 0.16651991
 0.16695216 0.1653628  0.16477641 0.16589232 0.16393174 0.16029958
 0.16056299 0.16289431 0.16302034 0.16220331 0.16215836 0.16078609
 0.16007322 0.1623456  0.16331331 0.15939333 0.15849106 0.16264877
 0.16223575 0.15899362 0.16323008 0.1633753  0.15600036 0.17455491]
