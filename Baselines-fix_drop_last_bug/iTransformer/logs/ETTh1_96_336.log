Args in experiment:
Namespace(is_training=1, model_id='ETTh1_96_336', model='iTransformer', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=336, enc_in=7, dec_in=7, c_out=7, d_model=128, n_heads=8, e_layers=2, d_layers=1, d_ff=128, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=2, use_multi_gpu=False, devices='0,1,2,3', exp_name='MTSF', channel_independence=False, inverse=False, class_strategy='projection', target_root_path='./data/electricity/', target_data_path='electricity.csv', efficient_training=False, use_norm=True, partial_start_index=0)
Use GPU: cuda:2
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.
MACs: 3308032.0
Params: 255184.0
3.31M MACs
>>>>>>>start training : ETTh1_96_336_iTransformer_ETTh1_M_ft96_sl48_ll336_pl128_dm8_nh2_el1_dl128_df1_fctimeF_ebTrue_dtExp_projection_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8209
val 2545
test 2545
	iters: 100, epoch: 1 | loss: 0.6497779
	speed: 0.0475s/iter; left time: 116.8003s
	iters: 200, epoch: 1 | loss: 0.5709440
	speed: 0.0307s/iter; left time: 72.5751s
Max Memory (MB): 14.0625
Epoch: 1 cost time: 9.563161373138428
Epoch: 1, Steps: 256 | Train Loss: 0.5952852 Vali Loss: 1.3502920 Test Loss: 0.5307047
Validation loss decreased (inf --> 1.350292).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.5300944
	speed: 0.2748s/iter; left time: 605.9196s
	iters: 200, epoch: 2 | loss: 0.4964134
	speed: 0.0299s/iter; left time: 62.8706s
Max Memory (MB): 14.0625
Epoch: 2 cost time: 7.889219284057617
Epoch: 2, Steps: 256 | Train Loss: 0.5173216 Vali Loss: 1.2989128 Test Loss: 0.4994335
Validation loss decreased (1.350292 --> 1.298913).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.4815993
	speed: 0.2557s/iter; left time: 498.4290s
	iters: 200, epoch: 3 | loss: 0.5344657
	speed: 0.0279s/iter; left time: 51.6387s
Max Memory (MB): 14.0625
Epoch: 3 cost time: 7.935772657394409
Epoch: 3, Steps: 256 | Train Loss: 0.5025016 Vali Loss: 1.2859205 Test Loss: 0.4936304
Validation loss decreased (1.298913 --> 1.285921).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.3922454
	speed: 0.2372s/iter; left time: 401.6349s
	iters: 200, epoch: 4 | loss: 0.5525724
	speed: 0.0267s/iter; left time: 42.5585s
Max Memory (MB): 14.0625
Epoch: 4 cost time: 7.468110799789429
Epoch: 4, Steps: 256 | Train Loss: 0.4977989 Vali Loss: 1.2832409 Test Loss: 0.4931447
Validation loss decreased (1.285921 --> 1.283241).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5008733
	speed: 0.2383s/iter; left time: 342.4673s
	iters: 200, epoch: 5 | loss: 0.4685061
	speed: 0.0244s/iter; left time: 32.6397s
Max Memory (MB): 14.0625
Epoch: 5 cost time: 7.54430365562439
Epoch: 5, Steps: 256 | Train Loss: 0.4950077 Vali Loss: 1.2829376 Test Loss: 0.4935357
Validation loss decreased (1.283241 --> 1.282938).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5170589
	speed: 0.2243s/iter; left time: 264.9328s
	iters: 200, epoch: 6 | loss: 0.5438570
	speed: 0.0249s/iter; left time: 26.9531s
Max Memory (MB): 14.0625
Epoch: 6 cost time: 7.000323057174683
Epoch: 6, Steps: 256 | Train Loss: 0.4940910 Vali Loss: 1.2825780 Test Loss: 0.4936346
Validation loss decreased (1.282938 --> 1.282578).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.5309696
	speed: 0.2193s/iter; left time: 202.8939s
	iters: 200, epoch: 7 | loss: 0.5481212
	speed: 0.0245s/iter; left time: 20.1743s
Max Memory (MB): 14.0625
Epoch: 7 cost time: 6.965273857116699
Epoch: 7, Steps: 256 | Train Loss: 0.4935545 Vali Loss: 1.2827604 Test Loss: 0.4937473
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.4734587
	speed: 0.2201s/iter; left time: 147.2506s
	iters: 200, epoch: 8 | loss: 0.4663493
	speed: 0.0246s/iter; left time: 13.9970s
Max Memory (MB): 14.0625
Epoch: 8 cost time: 7.0717058181762695
Epoch: 8, Steps: 256 | Train Loss: 0.4930364 Vali Loss: 1.2809734 Test Loss: 0.4936855
Validation loss decreased (1.282578 --> 1.280973).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.4811635
	speed: 0.2182s/iter; left time: 90.1340s
	iters: 200, epoch: 9 | loss: 0.4946869
	speed: 0.0253s/iter; left time: 7.9164s
Max Memory (MB): 14.0625
Epoch: 9 cost time: 7.169868230819702
Epoch: 9, Steps: 256 | Train Loss: 0.4933121 Vali Loss: 1.2817771 Test Loss: 0.4936737
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.4934680
	speed: 0.2377s/iter; left time: 37.3191s
	iters: 200, epoch: 10 | loss: 0.4963856
	speed: 0.0293s/iter; left time: 1.6698s
Max Memory (MB): 14.0625
Epoch: 10 cost time: 8.426792621612549
Epoch: 10, Steps: 256 | Train Loss: 0.4930299 Vali Loss: 1.2798001 Test Loss: 0.4936737
Validation loss decreased (1.280973 --> 1.279800).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : ETTh1_96_336_iTransformer_ETTh1_M_ft96_sl48_ll336_pl128_dm8_nh2_el1_dl128_df1_fctimeF_ebTrue_dtExp_projection_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
test shape: (2545, 1, 336, 7) (2545, 1, 336, 7)
test shape: (2545, 336, 7) (2545, 336, 7)
mse:0.4936736226081848, mae:0.46673065423965454
