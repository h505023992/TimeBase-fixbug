Args in experiment:
Namespace(is_training=1, model_id='traffic_720_720', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=258, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_720_720_FITS_custom_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14688811008.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 73.70457172393799
Epoch: 1, Steps: 84 | Train Loss: 1.1817646 Vali Loss: 1.1714690 Test Loss: 1.3864007
Validation loss decreased (inf --> 1.171469).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 74.86659026145935
Epoch: 2, Steps: 84 | Train Loss: 0.8283699 Vali Loss: 1.0198107 Test Loss: 1.2020283
Validation loss decreased (1.171469 --> 1.019811).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 74.43690609931946
Epoch: 3, Steps: 84 | Train Loss: 0.7270631 Vali Loss: 0.9530807 Test Loss: 1.1204919
Validation loss decreased (1.019811 --> 0.953081).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 75.023273229599
Epoch: 4, Steps: 84 | Train Loss: 0.6634132 Vali Loss: 0.9007286 Test Loss: 1.0583315
Validation loss decreased (0.953081 --> 0.900729).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 73.75031924247742
Epoch: 5, Steps: 84 | Train Loss: 0.6123530 Vali Loss: 0.8563001 Test Loss: 1.0059936
Validation loss decreased (0.900729 --> 0.856300).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 71.79983472824097
Epoch: 6, Steps: 84 | Train Loss: 0.5694025 Vali Loss: 0.8178767 Test Loss: 0.9610309
Validation loss decreased (0.856300 --> 0.817877).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 73.2861819267273
Epoch: 7, Steps: 84 | Train Loss: 0.5324949 Vali Loss: 0.7845508 Test Loss: 0.9217913
Validation loss decreased (0.817877 --> 0.784551).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 72.59899353981018
Epoch: 8, Steps: 84 | Train Loss: 0.5004647 Vali Loss: 0.7554164 Test Loss: 0.8865381
Validation loss decreased (0.784551 --> 0.755416).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 73.8122923374176
Epoch: 9, Steps: 84 | Train Loss: 0.4723190 Vali Loss: 0.7285460 Test Loss: 0.8549761
Validation loss decreased (0.755416 --> 0.728546).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 73.10005569458008
Epoch: 10, Steps: 84 | Train Loss: 0.4675510 Vali Loss: 0.7042804 Test Loss: 0.8269640
Validation loss decreased (0.728546 --> 0.704280).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 74.55630111694336
Epoch: 11, Steps: 84 | Train Loss: 0.4254498 Vali Loss: 0.6829391 Test Loss: 0.8011091
Validation loss decreased (0.704280 --> 0.682939).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 75.72602653503418
Epoch: 12, Steps: 84 | Train Loss: 0.4058295 Vali Loss: 0.6626843 Test Loss: 0.7775453
Validation loss decreased (0.682939 --> 0.662684).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 74.37132263183594
Epoch: 13, Steps: 84 | Train Loss: 0.3881743 Vali Loss: 0.6455369 Test Loss: 0.7575845
Validation loss decreased (0.662684 --> 0.645537).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 74.3324179649353
Epoch: 14, Steps: 84 | Train Loss: 0.3722141 Vali Loss: 0.6289492 Test Loss: 0.7380196
Validation loss decreased (0.645537 --> 0.628949).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 74.82183527946472
Epoch: 15, Steps: 84 | Train Loss: 0.3578962 Vali Loss: 0.6140324 Test Loss: 0.7205900
Validation loss decreased (0.628949 --> 0.614032).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 74.57841968536377
Epoch: 16, Steps: 84 | Train Loss: 0.3448575 Vali Loss: 0.6009346 Test Loss: 0.7047339
Validation loss decreased (0.614032 --> 0.600935).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 74.31748604774475
Epoch: 17, Steps: 84 | Train Loss: 0.3330185 Vali Loss: 0.5889518 Test Loss: 0.6904403
Validation loss decreased (0.600935 --> 0.588952).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 74.2531270980835
Epoch: 18, Steps: 84 | Train Loss: 0.3222746 Vali Loss: 0.5767477 Test Loss: 0.6763149
Validation loss decreased (0.588952 --> 0.576748).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 71.59374928474426
Epoch: 19, Steps: 84 | Train Loss: 0.3124454 Vali Loss: 0.5671884 Test Loss: 0.6649779
Validation loss decreased (0.576748 --> 0.567188).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 71.5255913734436
Epoch: 20, Steps: 84 | Train Loss: 0.3034522 Vali Loss: 0.5580206 Test Loss: 0.6538696
Validation loss decreased (0.567188 --> 0.558021).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 75.08841013908386
Epoch: 21, Steps: 84 | Train Loss: 0.2951918 Vali Loss: 0.5492300 Test Loss: 0.6423823
Validation loss decreased (0.558021 --> 0.549230).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 72.77792429924011
Epoch: 22, Steps: 84 | Train Loss: 0.2876167 Vali Loss: 0.5408136 Test Loss: 0.6332036
Validation loss decreased (0.549230 --> 0.540814).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 71.5647246837616
Epoch: 23, Steps: 84 | Train Loss: 0.2806804 Vali Loss: 0.5333887 Test Loss: 0.6244789
Validation loss decreased (0.540814 --> 0.533389).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 72.23970651626587
Epoch: 24, Steps: 84 | Train Loss: 0.2742559 Vali Loss: 0.5269090 Test Loss: 0.6170709
Validation loss decreased (0.533389 --> 0.526909).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 70.56469655036926
Epoch: 25, Steps: 84 | Train Loss: 0.2683219 Vali Loss: 0.5204458 Test Loss: 0.6091044
Validation loss decreased (0.526909 --> 0.520446).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 69.20286750793457
Epoch: 26, Steps: 84 | Train Loss: 0.2628700 Vali Loss: 0.5139569 Test Loss: 0.6017058
Validation loss decreased (0.520446 --> 0.513957).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 68.41935920715332
Epoch: 27, Steps: 84 | Train Loss: 0.2577672 Vali Loss: 0.5093848 Test Loss: 0.5958042
Validation loss decreased (0.513957 --> 0.509385).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 72.33532810211182
Epoch: 28, Steps: 84 | Train Loss: 0.2530555 Vali Loss: 0.5035919 Test Loss: 0.5894975
Validation loss decreased (0.509385 --> 0.503592).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 68.19581651687622
Epoch: 29, Steps: 84 | Train Loss: 0.2487081 Vali Loss: 0.4987664 Test Loss: 0.5839031
Validation loss decreased (0.503592 --> 0.498766).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 67.27157306671143
Epoch: 30, Steps: 84 | Train Loss: 0.2446642 Vali Loss: 0.4946806 Test Loss: 0.5786744
Validation loss decreased (0.498766 --> 0.494681).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14688811008.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 72.70614290237427
Epoch: 1, Steps: 84 | Train Loss: 0.3344432 Vali Loss: 0.4243731 Test Loss: 0.4964399
Validation loss decreased (inf --> 0.424373).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 69.18479514122009
Epoch: 2, Steps: 84 | Train Loss: 0.2927899 Vali Loss: 0.3975914 Test Loss: 0.4644711
Validation loss decreased (0.424373 --> 0.397591).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 69.72136211395264
Epoch: 3, Steps: 84 | Train Loss: 0.2775199 Vali Loss: 0.3883764 Test Loss: 0.4639185
Validation loss decreased (0.397591 --> 0.388376).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 71.38645529747009
Epoch: 4, Steps: 84 | Train Loss: 0.2726752 Vali Loss: 0.3863314 Test Loss: 0.4609553
Validation loss decreased (0.388376 --> 0.386331).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 64.18800258636475
Epoch: 5, Steps: 84 | Train Loss: 0.2712727 Vali Loss: 0.3864985 Test Loss: 0.4605352
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 65.15779542922974
Epoch: 6, Steps: 84 | Train Loss: 0.2709784 Vali Loss: 0.3865067 Test Loss: 0.4608734
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 66.5164258480072
Epoch: 7, Steps: 84 | Train Loss: 0.2708439 Vali Loss: 0.3860044 Test Loss: 0.4607543
Validation loss decreased (0.386331 --> 0.386004).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 65.86083149909973
Epoch: 8, Steps: 84 | Train Loss: 0.2707641 Vali Loss: 0.3861311 Test Loss: 0.4610083
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 64.49138760566711
Epoch: 9, Steps: 84 | Train Loss: 0.2706735 Vali Loss: 0.3853999 Test Loss: 0.4599956
Validation loss decreased (0.386004 --> 0.385400).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 64.19661045074463
Epoch: 10, Steps: 84 | Train Loss: 0.2706596 Vali Loss: 0.3860053 Test Loss: 0.4602363
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 67.432870388031
Epoch: 11, Steps: 84 | Train Loss: 0.2705235 Vali Loss: 0.3864015 Test Loss: 0.4605906
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 62.94064784049988
Epoch: 12, Steps: 84 | Train Loss: 0.2705462 Vali Loss: 0.3865550 Test Loss: 0.4601172
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 64.76112675666809
Epoch: 13, Steps: 84 | Train Loss: 0.2704874 Vali Loss: 0.3859576 Test Loss: 0.4608478
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 67.5377984046936
Epoch: 14, Steps: 84 | Train Loss: 0.2704452 Vali Loss: 0.3854380 Test Loss: 0.4596901
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_720_720_FITS_custom_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4594582414627075, mae:0.31054590106010437, rse:0.5482081174850464, corr:[0.26086777 0.26482058 0.26756078 0.2657392  0.26754656 0.2683273
 0.26762795 0.26911855 0.2691724  0.26816416 0.26891944 0.2684492
 0.2671176  0.26760733 0.2675893  0.26679754 0.26724002 0.26731205
 0.26689252 0.2677297  0.26844215 0.26819637 0.26805067 0.26803717
 0.26876733 0.26915976 0.26907128 0.26861012 0.26901236 0.2693421
 0.26876545 0.26869023 0.26889774 0.26851925 0.2685541  0.2687535
 0.2681485  0.2679824  0.26849717 0.26838693 0.26828527 0.26863983
 0.2685503  0.26839027 0.26856905 0.26845127 0.26835194 0.2688056
 0.26890117 0.26840913 0.26863882 0.2687644  0.26840317 0.26846296
 0.26838192 0.26773772 0.26776704 0.26824373 0.26808044 0.2680948
 0.2684816  0.2680572  0.26757464 0.26770142 0.26757687 0.26751554
 0.26795906 0.26794854 0.26764962 0.2679285  0.26797533 0.26762393
 0.2675318  0.26757956 0.26765877 0.26812398 0.2682643  0.26781306
 0.26778045 0.26801407 0.2679168  0.26806268 0.26835024 0.2681445
 0.26776177 0.26766092 0.267784   0.2680042  0.26811498 0.2680261
 0.26828954 0.26837084 0.2680194  0.26799765 0.2682525  0.26803648
 0.2674261  0.26730487 0.26716545 0.26698586 0.26753423 0.2679337
 0.26758477 0.267395   0.26741374 0.26716012 0.26704934 0.2673236
 0.2674096  0.2674163  0.26756135 0.2674424  0.26720518 0.2672051
 0.26746112 0.26754397 0.2675866  0.26734048 0.26715243 0.26746348
 0.2676263  0.26759705 0.26782623 0.26795214 0.2675482  0.26722074
 0.26728025 0.26732606 0.26763022 0.2680051  0.26788884 0.2680519
 0.26858738 0.26842356 0.26790074 0.26791364 0.2680107  0.26776177
 0.26828703 0.26842725 0.26806337 0.2681493  0.2682905  0.26788804
 0.2677861  0.2678566  0.26773962 0.2679223  0.26831836 0.26807594
 0.2676861  0.2679021  0.2681255  0.268346   0.26894057 0.2691636
 0.26863697 0.2684393  0.26887572 0.2692532  0.26942462 0.2694114
 0.2692087  0.26891297 0.26881447 0.26913467 0.2693973  0.26912734
 0.2694214  0.26945683 0.26955226 0.2692649  0.2691383  0.26924384
 0.26906145 0.26918137 0.26965868 0.2694307  0.26897168 0.26934186
 0.2696279  0.26922095 0.26917922 0.26950216 0.26953006 0.26973
 0.2701414  0.27028862 0.27044863 0.27047476 0.26965272 0.2689351
 0.2694191  0.26949745 0.26955163 0.26989138 0.26985416 0.2694245
 0.26938647 0.26959392 0.2696824  0.27003896 0.27041352 0.27042195
 0.2704126  0.27019104 0.26961756 0.26945677 0.2696351  0.269435
 0.2694067  0.2698114  0.26970965 0.2693717  0.2695503  0.26963592
 0.26927534 0.26900536 0.2693112  0.26959732 0.26949582 0.26904005
 0.26860222 0.26863083 0.26890603 0.26910454 0.26936883 0.26949784
 0.26930702 0.26921052 0.2693779  0.2694449  0.26938865 0.26924765
 0.26899573 0.26890004 0.26902074 0.2691277  0.2694496  0.2696474
 0.2691764  0.26895276 0.26952472 0.26964706 0.26914543 0.26895407
 0.26880845 0.26837483 0.26847103 0.26898918 0.26885298 0.26859602
 0.26889864 0.26900885 0.2688416  0.26906383 0.26939064 0.26947436
 0.26956415 0.2692636  0.26865292 0.26853603 0.26870915 0.26856634
 0.2684047  0.26878935 0.26909348 0.26886442 0.2683877  0.2681469
 0.26814198 0.26799628 0.26771113 0.2678161  0.26815727 0.26848307
 0.26875514 0.26876724 0.2686371  0.26869506 0.268598   0.26822144
 0.26859516 0.26921755 0.26900676 0.26846734 0.26852047 0.26860523
 0.26827198 0.26841155 0.26879236 0.26885653 0.2688103  0.26858106
 0.26814282 0.2683651  0.26926777 0.2696181  0.26952887 0.2699063
 0.27008826 0.2696161  0.2693344  0.26925024 0.2690222  0.2691431
 0.2696985  0.2695255  0.2692717  0.2694413  0.2691796  0.2686206
 0.2687644  0.26907128 0.26914087 0.26943937 0.26974764 0.26940918
 0.26899052 0.269295   0.26975992 0.26962158 0.2694414  0.27001542
 0.2705817  0.27039602 0.27031183 0.27082318 0.27112773 0.2709718
 0.2710207  0.27092186 0.27051094 0.27031645 0.27011308 0.26977795
 0.2703109  0.2706538  0.27064282 0.27044126 0.27040514 0.27024826
 0.2699027  0.26983362 0.26984712 0.2698247  0.27031186 0.27099404
 0.27105874 0.27089807 0.27099615 0.27100188 0.27101764 0.27119553
 0.27111802 0.27082047 0.2707802  0.27097425 0.27084646 0.27058122
 0.27092788 0.27115563 0.27125546 0.27099696 0.27083105 0.27088544
 0.27056023 0.2701746  0.27031842 0.2706206  0.2709436  0.27161625
 0.2718526  0.2710758  0.27047917 0.27071542 0.27095157 0.27116793
 0.2716608  0.27153835 0.27068108 0.2702659  0.27032655 0.27035406
 0.2705204  0.27052346 0.27042627 0.2704997  0.27038252 0.26996872
 0.27002743 0.2704189  0.27038723 0.27036038 0.2707559  0.27092752
 0.2705876  0.2700338  0.26951978 0.26928326 0.26939213 0.26964217
 0.2700693  0.2704648  0.27020133 0.26979718 0.27003616 0.27015448
 0.26943633 0.2690585  0.2694049  0.2694394  0.2695794  0.27030578
 0.270622   0.27040386 0.27048498 0.27060992 0.27041745 0.27036792
 0.27018735 0.2695123  0.26906118 0.26903823 0.26900974 0.26920286
 0.26985952 0.27027142 0.27028954 0.2702276  0.26994872 0.26963145
 0.2693162  0.2692512  0.26927918 0.26918676 0.26923868 0.26961735
 0.26984727 0.26971558 0.2696258  0.26944634 0.26905692 0.2693192
 0.26999083 0.26975673 0.26919487 0.26941076 0.26941258 0.26876557
 0.268857   0.2692495  0.26906675 0.26881692 0.2688941  0.2688645
 0.2687136  0.26883328 0.26903442 0.26947913 0.26980993 0.2692996
 0.26868984 0.2690914  0.26988888 0.27022144 0.27031004 0.2702469
 0.27000532 0.26999685 0.27019063 0.27013397 0.26999998 0.27002746
 0.2700938  0.27018723 0.27030686 0.27020773 0.2700471  0.26994234
 0.26966393 0.2694523  0.26987544 0.2703567  0.27038643 0.2703616
 0.27045768 0.27041003 0.27018556 0.26995176 0.27017435 0.27083218
 0.2708452  0.27010718 0.26997736 0.27052552 0.2706068  0.27046317
 0.27060476 0.27042887 0.27017686 0.270467   0.27054498 0.27027255
 0.2707814  0.27097374 0.27092952 0.27109668 0.2711297  0.27066538
 0.2703342  0.27035737 0.27014259 0.27001578 0.27059084 0.27124998
 0.27125952 0.2709678  0.27091685 0.27102134 0.27095345 0.27063632
 0.27048308 0.27066258 0.27057913 0.2700297  0.26943356 0.26921988
 0.26956472 0.2696775  0.27003253 0.27035278 0.27028096 0.27022573
 0.27070424 0.27134344 0.2711893  0.2706264  0.2706983  0.2708467
 0.27018192 0.26944104 0.2690925  0.26879132 0.26900807 0.26968622
 0.26982322 0.26948652 0.26913342 0.2686738  0.26845104 0.26872325
 0.26860058 0.26802012 0.26829043 0.26885894 0.2688661  0.26897156
 0.26934427 0.26944256 0.26949257 0.26954007 0.2692531  0.26919997
 0.2694467  0.2689653  0.26828894 0.26857793 0.26899636 0.2689724
 0.26929352 0.26943597 0.268666   0.26821625 0.26856375 0.26867184
 0.2683617  0.26823533 0.2682063  0.2681209  0.26810306 0.26797536
 0.26791617 0.26807836 0.2680609  0.26791304 0.26796126 0.26791874
 0.26770455 0.26758733 0.2673892  0.26721525 0.26755804 0.26805454
 0.26824376 0.26827413 0.26777625 0.26682156 0.26654264 0.26687014
 0.26672953 0.2666596  0.26694506 0.2668768  0.26688203 0.26739404
 0.26772675 0.26788867 0.26813853 0.26786372 0.26721904 0.2669876
 0.26672518 0.2662008  0.26632702 0.26690644 0.26709938 0.26734483
 0.2678239  0.2678379  0.26763806 0.26764548 0.26757997 0.26751253
 0.26770183 0.26821917 0.26881474 0.26892686 0.26819098 0.26736256
 0.26700336 0.26688123 0.2672816  0.26800135 0.26821455 0.26845634
 0.268937   0.26838136 0.26739594 0.26736486 0.26713744 0.2663982
 0.26677948 0.2675873  0.2673123  0.26723593 0.26799795 0.26830706
 0.26847303 0.26896113 0.26896742 0.2685868  0.26859874 0.2682961
 0.26768515 0.26800662 0.26856154 0.26836547 0.268365   0.26871505
 0.26854515 0.26814276 0.26791552 0.26790705 0.2682621  0.26828322
 0.26793066 0.26831043 0.26862755 0.26786405 0.2673893  0.2680309
 0.26875255 0.26868933 0.2693329  0.2696102  0.26917297 0.26883185
 0.26809055 0.26748517 0.26815498 0.2686945  0.2687298  0.26977596
 0.27036193 0.2694721  0.26949474 0.2700571  0.26934123 0.26897994
 0.26965773 0.2698309  0.26995394 0.27018252 0.26958698 0.26952368
 0.27029178 0.26993546 0.2699291  0.27015424 0.2690398  0.26847145
 0.26927456 0.26868498 0.2681001  0.26959968 0.27006298 0.2697212
 0.27081108 0.2704205  0.26954833 0.27119827 0.27121192 0.2696604
 0.27054185 0.2688397  0.2665862  0.26898822 0.26387215 0.2708825 ]
