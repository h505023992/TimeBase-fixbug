Args in experiment:
Namespace(is_training=1, model_id='ETTh1_192_96', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=96, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=64, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_192_96_FITS_ETTh1_ftM_sl192_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8353
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=64, out_features=96, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5505024.0
params:  6240.0
Trainable parameters:  6240
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.4281673431396484
Epoch: 1, Steps: 65 | Train Loss: 0.5686645 Vali Loss: 1.3046577 Test Loss: 0.7565954
Validation loss decreased (inf --> 1.304658).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.553236722946167
Epoch: 2, Steps: 65 | Train Loss: 0.4509369 Vali Loss: 1.1582079 Test Loss: 0.6661612
Validation loss decreased (1.304658 --> 1.158208).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.5299198627471924
Epoch: 3, Steps: 65 | Train Loss: 0.3838008 Vali Loss: 1.0844008 Test Loss: 0.6182215
Validation loss decreased (1.158208 --> 1.084401).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.5923280715942383
Epoch: 4, Steps: 65 | Train Loss: 0.3431845 Vali Loss: 1.0336708 Test Loss: 0.5911736
Validation loss decreased (1.084401 --> 1.033671).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.423091173171997
Epoch: 5, Steps: 65 | Train Loss: 0.3155381 Vali Loss: 0.9977762 Test Loss: 0.5726957
Validation loss decreased (1.033671 --> 0.997776).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.4251952171325684
Epoch: 6, Steps: 65 | Train Loss: 0.2952015 Vali Loss: 0.9755518 Test Loss: 0.5583011
Validation loss decreased (0.997776 --> 0.975552).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.3654329776763916
Epoch: 7, Steps: 65 | Train Loss: 0.2790270 Vali Loss: 0.9559935 Test Loss: 0.5462024
Validation loss decreased (0.975552 --> 0.955994).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.9871487617492676
Epoch: 8, Steps: 65 | Train Loss: 0.2654031 Vali Loss: 0.9402660 Test Loss: 0.5356395
Validation loss decreased (0.955994 --> 0.940266).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.4412152767181396
Epoch: 9, Steps: 65 | Train Loss: 0.2537591 Vali Loss: 0.9206618 Test Loss: 0.5257956
Validation loss decreased (0.940266 --> 0.920662).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.222132682800293
Epoch: 10, Steps: 65 | Train Loss: 0.2438038 Vali Loss: 0.9065030 Test Loss: 0.5158562
Validation loss decreased (0.920662 --> 0.906503).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.9184391498565674
Epoch: 11, Steps: 65 | Train Loss: 0.2349987 Vali Loss: 0.8972203 Test Loss: 0.5079286
Validation loss decreased (0.906503 --> 0.897220).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3517916202545166
Epoch: 12, Steps: 65 | Train Loss: 0.2269294 Vali Loss: 0.8841463 Test Loss: 0.5007201
Validation loss decreased (0.897220 --> 0.884146).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.14313006401062
Epoch: 13, Steps: 65 | Train Loss: 0.2200142 Vali Loss: 0.8722921 Test Loss: 0.4931650
Validation loss decreased (0.884146 --> 0.872292).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.9407989978790283
Epoch: 14, Steps: 65 | Train Loss: 0.2139218 Vali Loss: 0.8631171 Test Loss: 0.4871865
Validation loss decreased (0.872292 --> 0.863117).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.6436336040496826
Epoch: 15, Steps: 65 | Train Loss: 0.2084109 Vali Loss: 0.8578675 Test Loss: 0.4815915
Validation loss decreased (0.863117 --> 0.857868).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9864680767059326
Epoch: 16, Steps: 65 | Train Loss: 0.2034286 Vali Loss: 0.8470033 Test Loss: 0.4762462
Validation loss decreased (0.857868 --> 0.847003).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.4978315830230713
Epoch: 17, Steps: 65 | Train Loss: 0.1988761 Vali Loss: 0.8440138 Test Loss: 0.4713254
Validation loss decreased (0.847003 --> 0.844014).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.4027085304260254
Epoch: 18, Steps: 65 | Train Loss: 0.1947773 Vali Loss: 0.8371717 Test Loss: 0.4670805
Validation loss decreased (0.844014 --> 0.837172).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.9885430335998535
Epoch: 19, Steps: 65 | Train Loss: 0.1911050 Vali Loss: 0.8307341 Test Loss: 0.4627165
Validation loss decreased (0.837172 --> 0.830734).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.674405813217163
Epoch: 20, Steps: 65 | Train Loss: 0.1878755 Vali Loss: 0.8233913 Test Loss: 0.4583020
Validation loss decreased (0.830734 --> 0.823391).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.285987377166748
Epoch: 21, Steps: 65 | Train Loss: 0.1847621 Vali Loss: 0.8163912 Test Loss: 0.4546935
Validation loss decreased (0.823391 --> 0.816391).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.084425210952759
Epoch: 22, Steps: 65 | Train Loss: 0.1818762 Vali Loss: 0.8143793 Test Loss: 0.4515426
Validation loss decreased (0.816391 --> 0.814379).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.4809844493865967
Epoch: 23, Steps: 65 | Train Loss: 0.1792322 Vali Loss: 0.8102413 Test Loss: 0.4485418
Validation loss decreased (0.814379 --> 0.810241).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.831725835800171
Epoch: 24, Steps: 65 | Train Loss: 0.1768594 Vali Loss: 0.8016014 Test Loss: 0.4454737
Validation loss decreased (0.810241 --> 0.801601).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.7566440105438232
Epoch: 25, Steps: 65 | Train Loss: 0.1747776 Vali Loss: 0.8049459 Test Loss: 0.4428551
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.6011595726013184
Epoch: 26, Steps: 65 | Train Loss: 0.1727601 Vali Loss: 0.7943922 Test Loss: 0.4403887
Validation loss decreased (0.801601 --> 0.794392).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.6987266540527344
Epoch: 27, Steps: 65 | Train Loss: 0.1708998 Vali Loss: 0.7969922 Test Loss: 0.4381512
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.710158348083496
Epoch: 28, Steps: 65 | Train Loss: 0.1692204 Vali Loss: 0.7889418 Test Loss: 0.4356478
Validation loss decreased (0.794392 --> 0.788942).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.7057769298553467
Epoch: 29, Steps: 65 | Train Loss: 0.1676005 Vali Loss: 0.7881690 Test Loss: 0.4338782
Validation loss decreased (0.788942 --> 0.788169).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.5974762439727783
Epoch: 30, Steps: 65 | Train Loss: 0.1661864 Vali Loss: 0.7844840 Test Loss: 0.4319599
Validation loss decreased (0.788169 --> 0.784484).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 8353
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=64, out_features=96, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5505024.0
params:  6240.0
Trainable parameters:  6240
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.4854979515075684
Epoch: 1, Steps: 65 | Train Loss: 0.3700135 Vali Loss: 0.7314860 Test Loss: 0.3917637
Validation loss decreased (inf --> 0.731486).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.51633882522583
Epoch: 2, Steps: 65 | Train Loss: 0.3528384 Vali Loss: 0.7110361 Test Loss: 0.3820823
Validation loss decreased (0.731486 --> 0.711036).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.038788557052612
Epoch: 3, Steps: 65 | Train Loss: 0.3481028 Vali Loss: 0.7084250 Test Loss: 0.3812028
Validation loss decreased (0.711036 --> 0.708425).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.7605268955230713
Epoch: 4, Steps: 65 | Train Loss: 0.3467856 Vali Loss: 0.7009743 Test Loss: 0.3814633
Validation loss decreased (0.708425 --> 0.700974).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.52932071685791
Epoch: 5, Steps: 65 | Train Loss: 0.3463455 Vali Loss: 0.7017260 Test Loss: 0.3814849
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.5429041385650635
Epoch: 6, Steps: 65 | Train Loss: 0.3461597 Vali Loss: 0.7052649 Test Loss: 0.3818431
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.5449531078338623
Epoch: 7, Steps: 65 | Train Loss: 0.3460912 Vali Loss: 0.6993796 Test Loss: 0.3819119
Validation loss decreased (0.700974 --> 0.699380).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.9689624309539795
Epoch: 8, Steps: 65 | Train Loss: 0.3459033 Vali Loss: 0.7030618 Test Loss: 0.3816614
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.809034585952759
Epoch: 9, Steps: 65 | Train Loss: 0.3454247 Vali Loss: 0.6977542 Test Loss: 0.3818779
Validation loss decreased (0.699380 --> 0.697754).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.0560495853424072
Epoch: 10, Steps: 65 | Train Loss: 0.3457025 Vali Loss: 0.7016511 Test Loss: 0.3817560
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.2631609439849854
Epoch: 11, Steps: 65 | Train Loss: 0.3456867 Vali Loss: 0.7001540 Test Loss: 0.3814830
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.438444137573242
Epoch: 12, Steps: 65 | Train Loss: 0.3452263 Vali Loss: 0.7017958 Test Loss: 0.3816012
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.10417366027832
Epoch: 13, Steps: 65 | Train Loss: 0.3454850 Vali Loss: 0.6995942 Test Loss: 0.3817438
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.864600419998169
Epoch: 14, Steps: 65 | Train Loss: 0.3453619 Vali Loss: 0.7007228 Test Loss: 0.3814965
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_192_96_FITS_ETTh1_ftM_sl192_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.3811255097389221, mae:0.39559993147850037, rse:0.586397111415863, corr:[0.27259532 0.27716747 0.2764088  0.2775942  0.2747919  0.27242428
 0.27210665 0.27140206 0.27086577 0.2710632  0.27058607 0.2704201
 0.27097797 0.2706081  0.2701883  0.27047604 0.27034587 0.27019238
 0.27001688 0.26961103 0.26933786 0.269302   0.26927584 0.26963013
 0.2692043  0.2684555  0.26801422 0.2676086  0.2671158  0.266905
 0.2664382  0.26573196 0.2655632  0.2653973  0.26513425 0.2651315
 0.26528752 0.26525232 0.26520556 0.26534268 0.2655503  0.26574266
 0.26587656 0.26605096 0.2660361  0.2656888  0.2659314  0.26675797
 0.26650253 0.2653317  0.2641141  0.26302016 0.26190633 0.26060718
 0.25982884 0.2593359  0.25882706 0.25866228 0.25843236 0.2583661
 0.2585631  0.25868532 0.25855204 0.25872204 0.2588886  0.25888243
 0.25923708 0.25939053 0.2591276  0.25918818 0.2593384  0.25903925
 0.25817016 0.25698143 0.25581163 0.2552001  0.2549571  0.25451565
 0.25409597 0.25347802 0.25291017 0.25242373 0.2523939  0.25250098
 0.2519958  0.25172505 0.25233647 0.25223032 0.25176692 0.2519615
 0.25169554 0.2510698  0.25127703 0.25074598 0.24991396 0.2523387 ]
