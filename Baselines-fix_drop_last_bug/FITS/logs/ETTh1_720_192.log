Args in experiment:
Namespace(is_training=1, model_id='ETTh1_720_192', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=196, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.3028855323791504
Epoch: 1, Steps: 60 | Train Loss: 0.6598449 Vali Loss: 1.5482574 Test Loss: 0.8041350
Validation loss decreased (inf --> 1.548257).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.4467034339904785
Epoch: 2, Steps: 60 | Train Loss: 0.5176424 Vali Loss: 1.4237411 Test Loss: 0.7505627
Validation loss decreased (1.548257 --> 1.423741).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.5436196327209473
Epoch: 3, Steps: 60 | Train Loss: 0.4528498 Vali Loss: 1.3699681 Test Loss: 0.7276840
Validation loss decreased (1.423741 --> 1.369968).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.495339870452881
Epoch: 4, Steps: 60 | Train Loss: 0.4134938 Vali Loss: 1.3438240 Test Loss: 0.7174740
Validation loss decreased (1.369968 --> 1.343824).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.298793077468872
Epoch: 5, Steps: 60 | Train Loss: 0.3846772 Vali Loss: 1.3248748 Test Loss: 0.7081063
Validation loss decreased (1.343824 --> 1.324875).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.5120508670806885
Epoch: 6, Steps: 60 | Train Loss: 0.3616336 Vali Loss: 1.3071592 Test Loss: 0.6977769
Validation loss decreased (1.324875 --> 1.307159).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.643854856491089
Epoch: 7, Steps: 60 | Train Loss: 0.3419366 Vali Loss: 1.2916881 Test Loss: 0.6880000
Validation loss decreased (1.307159 --> 1.291688).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.7310407161712646
Epoch: 8, Steps: 60 | Train Loss: 0.3248492 Vali Loss: 1.2770282 Test Loss: 0.6784803
Validation loss decreased (1.291688 --> 1.277028).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.848348617553711
Epoch: 9, Steps: 60 | Train Loss: 0.3097529 Vali Loss: 1.2611976 Test Loss: 0.6675230
Validation loss decreased (1.277028 --> 1.261198).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.9864962100982666
Epoch: 10, Steps: 60 | Train Loss: 0.2965432 Vali Loss: 1.2495356 Test Loss: 0.6595277
Validation loss decreased (1.261198 --> 1.249536).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.7892072200775146
Epoch: 11, Steps: 60 | Train Loss: 0.2847422 Vali Loss: 1.2393682 Test Loss: 0.6518036
Validation loss decreased (1.249536 --> 1.239368).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.8456547260284424
Epoch: 12, Steps: 60 | Train Loss: 0.2742775 Vali Loss: 1.2259605 Test Loss: 0.6420738
Validation loss decreased (1.239368 --> 1.225960).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.6234095096588135
Epoch: 13, Steps: 60 | Train Loss: 0.2644837 Vali Loss: 1.2153094 Test Loss: 0.6339843
Validation loss decreased (1.225960 --> 1.215309).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.6998531818389893
Epoch: 14, Steps: 60 | Train Loss: 0.2561266 Vali Loss: 1.2056535 Test Loss: 0.6266645
Validation loss decreased (1.215309 --> 1.205654).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.6014721393585205
Epoch: 15, Steps: 60 | Train Loss: 0.2482160 Vali Loss: 1.1963421 Test Loss: 0.6195928
Validation loss decreased (1.205654 --> 1.196342).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.538835287094116
Epoch: 16, Steps: 60 | Train Loss: 0.2410612 Vali Loss: 1.1860783 Test Loss: 0.6118237
Validation loss decreased (1.196342 --> 1.186078).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.6209771633148193
Epoch: 17, Steps: 60 | Train Loss: 0.2345848 Vali Loss: 1.1789167 Test Loss: 0.6060736
Validation loss decreased (1.186078 --> 1.178917).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.4913482666015625
Epoch: 18, Steps: 60 | Train Loss: 0.2285214 Vali Loss: 1.1712927 Test Loss: 0.6000925
Validation loss decreased (1.178917 --> 1.171293).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.787118434906006
Epoch: 19, Steps: 60 | Train Loss: 0.2231790 Vali Loss: 1.1637770 Test Loss: 0.5939733
Validation loss decreased (1.171293 --> 1.163777).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.291393280029297
Epoch: 20, Steps: 60 | Train Loss: 0.2181318 Vali Loss: 1.1568617 Test Loss: 0.5887899
Validation loss decreased (1.163777 --> 1.156862).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.4897139072418213
Epoch: 21, Steps: 60 | Train Loss: 0.2135550 Vali Loss: 1.1502146 Test Loss: 0.5833813
Validation loss decreased (1.156862 --> 1.150215).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.578415870666504
Epoch: 22, Steps: 60 | Train Loss: 0.2092901 Vali Loss: 1.1443186 Test Loss: 0.5785306
Validation loss decreased (1.150215 --> 1.144319).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.284214973449707
Epoch: 23, Steps: 60 | Train Loss: 0.2054657 Vali Loss: 1.1391070 Test Loss: 0.5747894
Validation loss decreased (1.144319 --> 1.139107).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.905768632888794
Epoch: 24, Steps: 60 | Train Loss: 0.2017231 Vali Loss: 1.1333365 Test Loss: 0.5697923
Validation loss decreased (1.139107 --> 1.133337).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.7488155364990234
Epoch: 25, Steps: 60 | Train Loss: 0.1983695 Vali Loss: 1.1287804 Test Loss: 0.5662550
Validation loss decreased (1.133337 --> 1.128780).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.008978843688965
Epoch: 26, Steps: 60 | Train Loss: 0.1952795 Vali Loss: 1.1244750 Test Loss: 0.5630499
Validation loss decreased (1.128780 --> 1.124475).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.6239943504333496
Epoch: 27, Steps: 60 | Train Loss: 0.1924769 Vali Loss: 1.1195139 Test Loss: 0.5586801
Validation loss decreased (1.124475 --> 1.119514).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.563870668411255
Epoch: 28, Steps: 60 | Train Loss: 0.1898458 Vali Loss: 1.1160311 Test Loss: 0.5558091
Validation loss decreased (1.119514 --> 1.116031).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.5630674362182617
Epoch: 29, Steps: 60 | Train Loss: 0.1873240 Vali Loss: 1.1122559 Test Loss: 0.5526034
Validation loss decreased (1.116031 --> 1.112256).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.3995845317840576
Epoch: 30, Steps: 60 | Train Loss: 0.1848878 Vali Loss: 1.1075680 Test Loss: 0.5492367
Validation loss decreased (1.112256 --> 1.107568).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.4739999771118164
Epoch: 1, Steps: 60 | Train Loss: 0.4334964 Vali Loss: 0.9907185 Test Loss: 0.4522003
Validation loss decreased (inf --> 0.990719).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.4122262001037598
Epoch: 2, Steps: 60 | Train Loss: 0.3963748 Vali Loss: 0.9578001 Test Loss: 0.4235512
Validation loss decreased (0.990719 --> 0.957800).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.480761766433716
Epoch: 3, Steps: 60 | Train Loss: 0.3854175 Vali Loss: 0.9506773 Test Loss: 0.4176044
Validation loss decreased (0.957800 --> 0.950677).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.3516180515289307
Epoch: 4, Steps: 60 | Train Loss: 0.3822907 Vali Loss: 0.9524750 Test Loss: 0.4177780
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.512819290161133
Epoch: 5, Steps: 60 | Train Loss: 0.3809188 Vali Loss: 0.9546639 Test Loss: 0.4180949
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.4511492252349854
Epoch: 6, Steps: 60 | Train Loss: 0.3801332 Vali Loss: 0.9554839 Test Loss: 0.4180201
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.554075241088867
Epoch: 7, Steps: 60 | Train Loss: 0.3798203 Vali Loss: 0.9563401 Test Loss: 0.4181533
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.3056743144989014
Epoch: 8, Steps: 60 | Train Loss: 0.3794210 Vali Loss: 0.9582038 Test Loss: 0.4186319
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4149291217327118, mae:0.42400506138801575, rse:0.6117081642150879, corr:[0.26547077 0.2721172  0.2696306  0.27043974 0.26909852 0.26561895
 0.26403323 0.26478985 0.26499224 0.26433218 0.2642043  0.26445124
 0.26433456 0.26365355 0.26319656 0.26319253 0.26300484 0.2624918
 0.26158324 0.2608163  0.26047644 0.2603758  0.26027563 0.26054373
 0.2609429  0.2608566  0.26031175 0.25995666 0.26007038 0.26005122
 0.25959197 0.25896958 0.2589536  0.25907987 0.2585567  0.2579614
 0.25805756 0.2586851  0.2589092  0.25854066 0.2583284  0.2585227
 0.25886154 0.25898796 0.2588875  0.25897622 0.25959855 0.26020262
 0.2600511  0.25946462 0.25887018 0.2582927  0.25709873 0.25549084
 0.25472555 0.25471497 0.25435993 0.25357255 0.2528087  0.25286672
 0.25308207 0.25317013 0.2532441  0.25329706 0.25320426 0.25318435
 0.25327155 0.25289735 0.25249717 0.25270396 0.25294706 0.2527313
 0.25220838 0.25167727 0.2512621  0.2512457  0.25123683 0.25103116
 0.25031218 0.24931964 0.24887048 0.2490257  0.24891602 0.24843669
 0.24808797 0.24777405 0.24736966 0.24705914 0.24703234 0.24698332
 0.2466351  0.24642801 0.2465529  0.24651593 0.24634191 0.24672574
 0.24788417 0.24877161 0.24855302 0.24803714 0.2480076  0.24790552
 0.24735548 0.24677134 0.24666713 0.2470332  0.24712954 0.24671331
 0.24625306 0.24612585 0.24650098 0.24705304 0.24725662 0.24713232
 0.24690455 0.24651821 0.24626529 0.24656202 0.24711935 0.24716578
 0.24617188 0.24450868 0.24338073 0.24330875 0.24312131 0.24197666
 0.24124974 0.2417463  0.24206965 0.2413431  0.24013248 0.23946707
 0.2391242  0.23868936 0.23857464 0.23924875 0.24003643 0.23963039
 0.23850359 0.23799421 0.23864093 0.23948842 0.23935437 0.23831466
 0.23758468 0.23728229 0.23683275 0.23593797 0.23493078 0.2338441
 0.23303083 0.2326911  0.2328223  0.23310533 0.23271637 0.23202693
 0.23186392 0.23241933 0.23279737 0.23208463 0.2310644  0.23095006
 0.23148102 0.23172373 0.23160644 0.23176631 0.232262   0.23186323
 0.2308183  0.23042238 0.2304912  0.22992282 0.22834995 0.22731458
 0.22740129 0.22771576 0.22707868 0.2265826  0.22659732 0.22659622
 0.2260389  0.2252795  0.2252049  0.22595884 0.2261376  0.22452208
 0.2234308  0.22429532 0.2227669  0.21661608 0.21730393 0.22438934]
