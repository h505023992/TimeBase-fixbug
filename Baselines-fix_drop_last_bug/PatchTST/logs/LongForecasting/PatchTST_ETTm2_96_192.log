Args in experiment:
Namespace(random_seed=2021, is_training=1, model_id='96_192', model='PatchTST', data='ETTm2', root_path='./dataset/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=192, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', loss='mse', lradj='TST', pct_start=0.4, use_amp=False, use_gpu=True, gpu=1, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)
Use GPU: cuda:1
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.
MACs: 35524608.0
Params: 694720.0
35.52M MACs
>>>>>>>start training : 96_192_PatchTST_ETTm2_ftM_sl96_ll48_pl192_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34273
val 11329
test 11329
	iters: 100, epoch: 1 | loss: 0.3528967
	speed: 0.1037s/iter; left time: 823.6206s
	iters: 200, epoch: 1 | loss: 0.4284083
	speed: 0.0897s/iter; left time: 703.1184s
Epoch: 1 cost time: 25.90164279937744
Epoch: 1, Steps: 268 | Train Loss: 0.4011362 Vali Loss: 0.1950600 Test Loss: 0.2730645
Validation loss decreased (inf --> 0.195060).  Saving model ...
Updating learning rate to 5.6365721298040346e-06
	iters: 100, epoch: 2 | loss: 0.3250659
	speed: 0.3971s/iter; left time: 3046.8383s
	iters: 200, epoch: 2 | loss: 0.3492172
	speed: 0.0945s/iter; left time: 715.5868s
Epoch: 2 cost time: 26.160255432128906
Epoch: 2, Steps: 268 | Train Loss: 0.3671636 Vali Loss: 0.1817088 Test Loss: 0.2559060
Validation loss decreased (0.195060 --> 0.181709).  Saving model ...
Updating learning rate to 1.043468983854729e-05
	iters: 100, epoch: 3 | loss: 0.3422984
	speed: 0.4066s/iter; left time: 3010.5631s
	iters: 200, epoch: 3 | loss: 0.4399010
	speed: 0.0933s/iter; left time: 681.5767s
Epoch: 3 cost time: 26.189930200576782
Epoch: 3, Steps: 268 | Train Loss: 0.3482754 Vali Loss: 0.1761525 Test Loss: 0.2470239
Validation loss decreased (0.181709 --> 0.176153).  Saving model ...
Updating learning rate to 1.806716705466112e-05
	iters: 100, epoch: 4 | loss: 0.3358703
	speed: 0.3993s/iter; left time: 2849.8192s
	iters: 200, epoch: 4 | loss: 0.3020194
	speed: 0.0889s/iter; left time: 625.7196s
Epoch: 4 cost time: 24.987256050109863
Epoch: 4, Steps: 268 | Train Loss: 0.3369758 Vali Loss: 0.1740235 Test Loss: 0.2434284
Validation loss decreased (0.176153 --> 0.174024).  Saving model ...
Updating learning rate to 2.8013541299259828e-05
	iters: 100, epoch: 5 | loss: 0.3531240
	speed: 0.4010s/iter; left time: 2754.5910s
	iters: 200, epoch: 5 | loss: 0.4372534
	speed: 0.0879s/iter; left time: 594.7332s
Epoch: 5 cost time: 24.989223957061768
Epoch: 5, Steps: 268 | Train Loss: 0.3311963 Vali Loss: 0.1726999 Test Loss: 0.2406792
Validation loss decreased (0.174024 --> 0.172700).  Saving model ...
Updating learning rate to 3.9595564285622165e-05
	iters: 100, epoch: 6 | loss: 0.4199653
	speed: 0.4016s/iter; left time: 2651.1209s
	iters: 200, epoch: 6 | loss: 0.3601191
	speed: 0.0928s/iter; left time: 603.3121s
Epoch: 6 cost time: 25.178669929504395
Epoch: 6, Steps: 268 | Train Loss: 0.3273527 Vali Loss: 0.1720647 Test Loss: 0.2402530
Validation loss decreased (0.172700 --> 0.172065).  Saving model ...
Updating learning rate to 5.202345201265517e-05
	iters: 100, epoch: 7 | loss: 0.3060531
	speed: 0.4013s/iter; left time: 2541.2369s
	iters: 200, epoch: 7 | loss: 0.2116176
	speed: 0.0937s/iter; left time: 583.7712s
Epoch: 7 cost time: 25.722582817077637
Epoch: 7, Steps: 268 | Train Loss: 0.3233325 Vali Loss: 0.1723519 Test Loss: 0.2401521
EarlyStopping counter: 1 out of 5
Updating learning rate to 6.444974053509239e-05
	iters: 100, epoch: 8 | loss: 0.2913706
	speed: 0.3990s/iter; left time: 2419.9240s
	iters: 200, epoch: 8 | loss: 0.3493400
	speed: 0.0946s/iter; left time: 564.0253s
Epoch: 8 cost time: 25.89210867881775
Epoch: 8, Steps: 268 | Train Loss: 0.3206409 Vali Loss: 0.1724771 Test Loss: 0.2428841
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.602707495823693e-05
	iters: 100, epoch: 9 | loss: 0.2507054
	speed: 0.3824s/iter; left time: 2216.8623s
	iters: 200, epoch: 9 | loss: 0.3408308
	speed: 0.0874s/iter; left time: 498.0675s
Epoch: 9 cost time: 24.192119359970093
Epoch: 9, Steps: 268 | Train Loss: 0.3176335 Vali Loss: 0.1726303 Test Loss: 0.2417422
EarlyStopping counter: 3 out of 5
Updating learning rate to 8.596599099649138e-05
	iters: 100, epoch: 10 | loss: 0.4226284
	speed: 0.4014s/iter; left time: 2219.2571s
	iters: 200, epoch: 10 | loss: 0.2790436
	speed: 0.0955s/iter; left time: 518.2430s
Epoch: 10 cost time: 25.474286317825317
Epoch: 10, Steps: 268 | Train Loss: 0.3145215 Vali Loss: 0.1727768 Test Loss: 0.2456863
EarlyStopping counter: 4 out of 5
Updating learning rate to 9.35887489419945e-05
	iters: 100, epoch: 11 | loss: 0.2491915
	speed: 0.4009s/iter; left time: 2109.1153s
	iters: 200, epoch: 11 | loss: 0.5108427
	speed: 0.0947s/iter; left time: 488.8259s
Epoch: 11 cost time: 26.11031198501587
Epoch: 11, Steps: 268 | Train Loss: 0.3126287 Vali Loss: 0.1728356 Test Loss: 0.2442133
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : 96_192_PatchTST_ETTm2_ftM_sl96_ll48_pl192_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.2409925013780594, mae:0.3012104034423828, rse:0.39737164974212646
