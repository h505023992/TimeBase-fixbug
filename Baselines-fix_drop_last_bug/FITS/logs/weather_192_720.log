Args in experiment:
Namespace(is_training=1, model_id='weather_192_720', model='FITS', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=3, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=34, base_T=144, H_order=12)
Use GPU: cuda:0
>>>>>>>start training : weather_192_720_FITS_custom_ftM_sl192_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35976
val 4551
test 9820
Model(
  (freq_upsampler): Linear(in_features=34, out_features=161, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14714112.0
params:  5635.0
Trainable parameters:  5635
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8772660
	speed: 0.1069s/iter; left time: 890.2608s
	iters: 200, epoch: 1 | loss: 0.7356174
	speed: 0.0971s/iter; left time: 799.4249s
Epoch: 1 cost time: 27.329141855239868
Epoch: 1, Steps: 281 | Train Loss: 0.8832134 Vali Loss: 0.8034592 Test Loss: 0.3765563
Validation loss decreased (inf --> 0.803459).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5636223
	speed: 0.3523s/iter; left time: 2835.9662s
	iters: 200, epoch: 2 | loss: 0.5363635
	speed: 0.0838s/iter; left time: 666.4979s
Epoch: 2 cost time: 24.269167184829712
Epoch: 2, Steps: 281 | Train Loss: 0.5978226 Vali Loss: 0.7341101 Test Loss: 0.3562145
Validation loss decreased (0.803459 --> 0.734110).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5806952
	speed: 0.3156s/iter; left time: 2452.0862s
	iters: 200, epoch: 3 | loss: 0.5948454
	speed: 0.0827s/iter; left time: 634.3895s
Epoch: 3 cost time: 25.41359567642212
Epoch: 3, Steps: 281 | Train Loss: 0.5619095 Vali Loss: 0.7252232 Test Loss: 0.3537178
Validation loss decreased (0.734110 --> 0.725223).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5731273
	speed: 0.3648s/iter; left time: 2731.6364s
	iters: 200, epoch: 4 | loss: 0.5688194
	speed: 0.0936s/iter; left time: 691.7352s
Epoch: 4 cost time: 28.522488832473755
Epoch: 4, Steps: 281 | Train Loss: 0.5552200 Vali Loss: 0.7236384 Test Loss: 0.3531292
Validation loss decreased (0.725223 --> 0.723638).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6134087
	speed: 0.3645s/iter; left time: 2627.0003s
	iters: 200, epoch: 5 | loss: 0.4610589
	speed: 0.0905s/iter; left time: 643.1041s
Epoch: 5 cost time: 26.936367750167847
Epoch: 5, Steps: 281 | Train Loss: 0.5530516 Vali Loss: 0.7223570 Test Loss: 0.3528788
Validation loss decreased (0.723638 --> 0.722357).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5552900
	speed: 0.3789s/iter; left time: 2624.0827s
	iters: 200, epoch: 6 | loss: 0.5323624
	speed: 0.0945s/iter; left time: 645.3495s
Epoch: 6 cost time: 28.009613275527954
Epoch: 6, Steps: 281 | Train Loss: 0.5522465 Vali Loss: 0.7222868 Test Loss: 0.3525879
Validation loss decreased (0.722357 --> 0.722287).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4872553
	speed: 0.3631s/iter; left time: 2413.0185s
	iters: 200, epoch: 7 | loss: 0.4985291
	speed: 0.0931s/iter; left time: 609.6298s
Epoch: 7 cost time: 27.114058256149292
Epoch: 7, Steps: 281 | Train Loss: 0.5517177 Vali Loss: 0.7214158 Test Loss: 0.3522694
Validation loss decreased (0.722287 --> 0.721416).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5211205
	speed: 0.3719s/iter; left time: 2367.0418s
	iters: 200, epoch: 8 | loss: 0.4769625
	speed: 0.0969s/iter; left time: 607.0218s
Epoch: 8 cost time: 27.54429268836975
Epoch: 8, Steps: 281 | Train Loss: 0.5513980 Vali Loss: 0.7208109 Test Loss: 0.3520409
Validation loss decreased (0.721416 --> 0.720811).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4939960
	speed: 0.3619s/iter; left time: 2201.5905s
	iters: 200, epoch: 9 | loss: 0.5795636
	speed: 0.0874s/iter; left time: 523.0387s
Epoch: 9 cost time: 27.64381170272827
Epoch: 9, Steps: 281 | Train Loss: 0.5511358 Vali Loss: 0.7217242 Test Loss: 0.3518970
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6085595
	speed: 0.3653s/iter; left time: 2119.2118s
	iters: 200, epoch: 10 | loss: 0.5153168
	speed: 0.0883s/iter; left time: 503.5159s
Epoch: 10 cost time: 26.643938302993774
Epoch: 10, Steps: 281 | Train Loss: 0.5509390 Vali Loss: 0.7228419 Test Loss: 0.3517126
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5394216
	speed: 0.3541s/iter; left time: 1954.8861s
	iters: 200, epoch: 11 | loss: 0.5274608
	speed: 0.0937s/iter; left time: 507.9951s
Epoch: 11 cost time: 27.449371576309204
Epoch: 11, Steps: 281 | Train Loss: 0.5508468 Vali Loss: 0.7209020 Test Loss: 0.3515415
EarlyStopping counter: 3 out of 3
Early stopping
train 35976
val 4551
test 9820
Model(
  (freq_upsampler): Linear(in_features=34, out_features=161, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14714112.0
params:  5635.0
Trainable parameters:  5635
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6200328
	speed: 0.0989s/iter; left time: 823.6766s
	iters: 200, epoch: 1 | loss: 0.5901664
	speed: 0.0763s/iter; left time: 627.7185s
Epoch: 1 cost time: 23.885778188705444
Epoch: 1, Steps: 281 | Train Loss: 0.6763286 Vali Loss: 0.7206295 Test Loss: 0.3512194
Validation loss decreased (inf --> 0.720629).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7662666
	speed: 0.3311s/iter; left time: 2665.4792s
	iters: 200, epoch: 2 | loss: 0.6291855
	speed: 0.0920s/iter; left time: 731.1820s
Epoch: 2 cost time: 26.63604164123535
Epoch: 2, Steps: 281 | Train Loss: 0.6755400 Vali Loss: 0.7201533 Test Loss: 0.3510410
Validation loss decreased (0.720629 --> 0.720153).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6274727
	speed: 0.3552s/iter; left time: 2759.4344s
	iters: 200, epoch: 3 | loss: 0.7493988
	speed: 0.0842s/iter; left time: 645.9750s
Epoch: 3 cost time: 25.97412133216858
Epoch: 3, Steps: 281 | Train Loss: 0.6751313 Vali Loss: 0.7187214 Test Loss: 0.3508630
Validation loss decreased (0.720153 --> 0.718721).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.7371973
	speed: 0.3628s/iter; left time: 2716.3238s
	iters: 200, epoch: 4 | loss: 0.6473334
	speed: 0.1008s/iter; left time: 745.0533s
Epoch: 4 cost time: 28.421396255493164
Epoch: 4, Steps: 281 | Train Loss: 0.6750430 Vali Loss: 0.7184987 Test Loss: 0.3509835
Validation loss decreased (0.718721 --> 0.718499).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.7169313
	speed: 0.3610s/iter; left time: 2601.8976s
	iters: 200, epoch: 5 | loss: 0.6270788
	speed: 0.0895s/iter; left time: 636.3250s
Epoch: 5 cost time: 27.16273522377014
Epoch: 5, Steps: 281 | Train Loss: 0.6750043 Vali Loss: 0.7172599 Test Loss: 0.3506161
Validation loss decreased (0.718499 --> 0.717260).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.7090700
	speed: 0.3780s/iter; left time: 2618.0823s
	iters: 200, epoch: 6 | loss: 0.7298894
	speed: 0.0937s/iter; left time: 639.6157s
Epoch: 6 cost time: 28.13589072227478
Epoch: 6, Steps: 281 | Train Loss: 0.6748628 Vali Loss: 0.7183159 Test Loss: 0.3505052
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.7754137
	speed: 0.3605s/iter; left time: 2395.8010s
	iters: 200, epoch: 7 | loss: 0.6464254
	speed: 0.0965s/iter; left time: 631.7453s
Epoch: 7 cost time: 27.413150310516357
Epoch: 7, Steps: 281 | Train Loss: 0.6748650 Vali Loss: 0.7171097 Test Loss: 0.3504060
Validation loss decreased (0.717260 --> 0.717110).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5578563
	speed: 0.3650s/iter; left time: 2323.1176s
	iters: 200, epoch: 8 | loss: 0.6281939
	speed: 0.0924s/iter; left time: 578.6260s
Epoch: 8 cost time: 27.86409068107605
Epoch: 8, Steps: 281 | Train Loss: 0.6747919 Vali Loss: 0.7167164 Test Loss: 0.3505472
Validation loss decreased (0.717110 --> 0.716716).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6722186
	speed: 0.3625s/iter; left time: 2205.2302s
	iters: 200, epoch: 9 | loss: 0.6891512
	speed: 0.0968s/iter; left time: 579.2920s
Epoch: 9 cost time: 27.390808582305908
Epoch: 9, Steps: 281 | Train Loss: 0.6746596 Vali Loss: 0.7165889 Test Loss: 0.3503380
Validation loss decreased (0.716716 --> 0.716589).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5974582
	speed: 0.3735s/iter; left time: 2166.8731s
	iters: 200, epoch: 10 | loss: 0.6858301
	speed: 0.0910s/iter; left time: 518.9403s
Epoch: 10 cost time: 27.61706042289734
Epoch: 10, Steps: 281 | Train Loss: 0.6747099 Vali Loss: 0.7167969 Test Loss: 0.3503480
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6299647
	speed: 0.3504s/iter; left time: 1934.7138s
	iters: 200, epoch: 11 | loss: 0.7798896
	speed: 0.0681s/iter; left time: 369.3234s
Epoch: 11 cost time: 22.47719931602478
Epoch: 11, Steps: 281 | Train Loss: 0.6747151 Vali Loss: 0.7182105 Test Loss: 0.3502770
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6401678
	speed: 0.3351s/iter; left time: 1755.8707s
	iters: 200, epoch: 12 | loss: 0.6773627
	speed: 0.0854s/iter; left time: 439.0788s
Epoch: 12 cost time: 25.652135133743286
Epoch: 12, Steps: 281 | Train Loss: 0.6746156 Vali Loss: 0.7172841 Test Loss: 0.3502998
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : weather_192_720_FITS_custom_ftM_sl192_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.34906789660453796, mae:0.34759077429771423, rse:0.7774760723114014, corr:[0.48136243 0.4807928  0.47878742 0.4779456  0.4778253  0.47714865
 0.47574088 0.47419366 0.47295597 0.47192457 0.47092044 0.46994436
 0.46881217 0.46773928 0.46664655 0.4655154  0.46437904 0.46314391
 0.4621525  0.4612226  0.4601354  0.45903805 0.45802948 0.4570523
 0.45605513 0.45481396 0.45342264 0.45187485 0.45044303 0.44908318
 0.4477301  0.44619554 0.44466284 0.44328335 0.44217536 0.4410899
 0.44002944 0.43886694 0.4378045  0.4368143  0.43596968 0.43527105
 0.4344564  0.43350878 0.43253914 0.43158737 0.43084434 0.43014568
 0.42926592 0.42826584 0.4275253  0.42683935 0.42628214 0.42572418
 0.42522424 0.42470258 0.4243802  0.42393985 0.42357132 0.42322874
 0.4229616  0.42286932 0.42287275 0.42273417 0.42249775 0.42225054
 0.42185172 0.42157766 0.42139858 0.42129368 0.4212347  0.42107996
 0.42100483 0.4210889  0.42117164 0.4212114  0.4211212  0.42107788
 0.42112705 0.42123413 0.42145696 0.42163745 0.42162284 0.42153502
 0.4214741  0.42145512 0.42147672 0.42135185 0.42128387 0.42133236
 0.42152178 0.4217576  0.4219318  0.42207366 0.42221975 0.42240632
 0.42267895 0.42290387 0.42299324 0.4229325  0.4228351  0.42278373
 0.4228239  0.42288542 0.4228411  0.42269233 0.4224656  0.42233077
 0.42231235 0.42230523 0.42220148 0.42203146 0.42179096 0.42162135
 0.4214746  0.42126456 0.42094183 0.42056787 0.42019293 0.41985095
 0.4195396  0.41926524 0.41897967 0.4187053  0.41846597 0.4182584
 0.4180065  0.41762805 0.41719353 0.4167293  0.41623554 0.41574082
 0.41524914 0.4147591  0.4143839  0.4140678  0.41378912 0.41346785
 0.41304293 0.41252407 0.41197073 0.41138297 0.4108386  0.41027546
 0.4097127  0.4091797  0.4086406  0.40805522 0.40736282 0.40647057
 0.40546724 0.4044571  0.4035097  0.40264055 0.40166503 0.40051004
 0.39940715 0.39846218 0.39758608 0.39673063 0.39577022 0.39471164
 0.39373323 0.39280584 0.3919512  0.39122048 0.39040175 0.38943267
 0.388467   0.38730827 0.3861407  0.38502863 0.3840048  0.3830857
 0.38218418 0.3813556  0.38045236 0.37957847 0.3787967  0.3781078
 0.37754157 0.3769938  0.37637112 0.37559712 0.3747592  0.37423474
 0.37401316 0.373886   0.37370437 0.37341607 0.37285888 0.37229824
 0.37194297 0.37171793 0.37153482 0.37145963 0.37134784 0.37120587
 0.37117058 0.37107375 0.37095752 0.3709223  0.37083918 0.37066242
 0.37060457 0.37067413 0.37075123 0.37077364 0.37068138 0.37067452
 0.37064803 0.3706321  0.37056354 0.37047508 0.3704224  0.3705826
 0.37070403 0.37080318 0.37081745 0.37085348 0.370821   0.3708272
 0.3708776  0.37101316 0.37117228 0.37126976 0.3715755  0.3719004
 0.3722718  0.3725973  0.37300566 0.373359   0.37363982 0.37385428
 0.37403473 0.3740641  0.37409273 0.37424147 0.37431204 0.37428176
 0.37421662 0.37424263 0.37425834 0.3742785  0.37433472 0.37434936
 0.3742931  0.37417322 0.37395898 0.3738139  0.37371993 0.37366745
 0.3735723  0.37336716 0.3730465  0.37266466 0.37229136 0.37198368
 0.37176847 0.3715922  0.37138057 0.37113845 0.37086418 0.37059608
 0.3703443  0.37009013 0.3698287  0.36953905 0.36927897 0.36901164
 0.36875877 0.3684646  0.36814982 0.36779723 0.36743122 0.36705783
 0.3666278  0.36610892 0.36557192 0.36507142 0.36462337 0.36423716
 0.36379045 0.36320537 0.36249736 0.36176544 0.36101022 0.36023802
 0.35939142 0.35849965 0.35761902 0.35671905 0.35595757 0.35529926
 0.3546225  0.35383108 0.35295317 0.35206023 0.3511931  0.3504222
 0.34950867 0.34849778 0.34739727 0.34620038 0.34521708 0.3442635
 0.34332988 0.34236273 0.34155273 0.3407641  0.34006146 0.33922234
 0.33834326 0.33743462 0.33663675 0.33613166 0.33568072 0.33529887
 0.33474883 0.33408472 0.33327633 0.33262768 0.3320918  0.33139113
 0.3304998  0.32952678 0.32857016 0.32790864 0.32746002 0.32722723
 0.3269956  0.32660955 0.32621628 0.32586887 0.3256936  0.3256335
 0.3254436  0.32498258 0.3247099  0.32431355 0.32406205 0.32402095
 0.32404694 0.3241101  0.32403135 0.32390544 0.32365364 0.32341412
 0.32327437 0.32307202 0.3228233  0.32241362 0.32237506 0.32240275
 0.32234183 0.3223576  0.32235628 0.32243988 0.32262778 0.32298896
 0.3232426  0.32348797 0.32369372 0.32399052 0.32426813 0.32474202
 0.32531795 0.32585117 0.32628238 0.32646587 0.32679754 0.3272637
 0.32775322 0.32813275 0.32829562 0.32836983 0.32850736 0.3286234
 0.32872614 0.3288032  0.3288561  0.32902458 0.3292397  0.32938865
 0.32947052 0.32950175 0.32953224 0.32954177 0.32958782 0.32963297
 0.32964313 0.3295716  0.32939577 0.329155   0.32897452 0.32885295
 0.32876372 0.3286695  0.32863656 0.32857296 0.3285106  0.32837093
 0.32819977 0.32799345 0.32782403 0.32769793 0.32760078 0.32740387
 0.32707873 0.3266848  0.3264035  0.32636002 0.3265455  0.3267508
 0.32682526 0.32670674 0.32649177 0.32631853 0.3261745  0.3259909
 0.3256188  0.32509196 0.32447788 0.32399485 0.3234858  0.322856
 0.32204998 0.32114702 0.32030785 0.31956124 0.31886324 0.31815574
 0.31735167 0.31646731 0.31575006 0.3152694  0.31476763 0.31410787
 0.31320918 0.3122081  0.31120625 0.31032997 0.3093724  0.30818942
 0.30674982 0.30527058 0.30380148 0.30263394 0.30155134 0.30060902
 0.29963034 0.29867458 0.2977539  0.29686677 0.29592007 0.29473805
 0.29361907 0.29263443 0.29172936 0.29111287 0.29035798 0.28971395
 0.28884444 0.28811738 0.28746843 0.28684914 0.28616294 0.2854723
 0.28480196 0.28423968 0.2837037  0.28316244 0.2827308  0.28230086
 0.28190613 0.28152227 0.2811388  0.2809745  0.2808671  0.28080758
 0.2806999  0.280547   0.2803962  0.28037632 0.28047606 0.28064638
 0.2805829  0.28027672 0.2799485  0.2798224  0.2800539  0.2803323
 0.28061783 0.2806786  0.28056479 0.28053457 0.28044584 0.28039527
 0.28062123 0.2810655  0.28150597 0.28195423 0.28233096 0.28267384
 0.28290325 0.28321528 0.28354135 0.28383827 0.28429767 0.28459877
 0.2847723  0.28483778 0.28507835 0.28524247 0.28536257 0.28561288
 0.2858936  0.28616345 0.28645474 0.2866892  0.28702718 0.28727788
 0.28757372 0.28787735 0.28816158 0.28845125 0.28866938 0.28871801
 0.28861576 0.28848344 0.28826115 0.28813517 0.28806958 0.28796747
 0.28781322 0.287542   0.28723642 0.28692883 0.28663412 0.28637746
 0.28616777 0.2860436  0.2860292  0.2860665  0.28610036 0.28604057
 0.28586024 0.28557536 0.28529063 0.28507382 0.28496608 0.28492898
 0.28487143 0.2847033  0.2843909  0.2839895  0.2836484  0.2834355
 0.28335255 0.28328654 0.28306326 0.2826119  0.28194773 0.28127396
 0.2807436  0.28033635 0.27997857 0.2795944  0.27926135 0.27904648
 0.27891728 0.27874428 0.27837446 0.277706   0.27694666 0.27621308
 0.27552792 0.27485794 0.27409884 0.27323115 0.2723817  0.27167666
 0.27106875 0.27042076 0.26959604 0.2686083  0.26752755 0.26660946
 0.2658279  0.26506102 0.26417404 0.2633853  0.2627745  0.2624143
 0.26206324 0.26141775 0.26060954 0.2599346  0.25948447 0.25916415
 0.25880298 0.25818425 0.25729653 0.2562977  0.25540257 0.25476694
 0.25413004 0.2533324  0.25246903 0.251589   0.2509164  0.2503422
 0.24989502 0.2494487  0.24894607 0.24823692 0.24770822 0.24724881
 0.2469743  0.24697311 0.24694969 0.2468958  0.24686252 0.24687992
 0.24686061 0.24683204 0.24664585 0.24627192 0.24596614 0.24573596
 0.2456464  0.24558176 0.24559645 0.24559519 0.24547397 0.24524134
 0.24508205 0.24509631 0.24532108 0.2454981  0.24556774 0.24561945
 0.2458294  0.24597372 0.24613112 0.2462975  0.24655072 0.24678634
 0.24725696 0.24741633 0.24749805 0.24755569 0.24772626 0.2480046
 0.2484572  0.24885215 0.24917074 0.24931589 0.24929217 0.24921365
 0.24915838 0.24933526 0.24957998 0.24986501 0.25002596 0.25013417
 0.2501291  0.2501468  0.25020894 0.25036743 0.2506134  0.2507378
 0.25081274 0.25075367 0.25063944 0.25057682 0.2505527  0.2506092
 0.2505427  0.2502902  0.24985944 0.2493811  0.24897958 0.24864928
 0.2483798  0.248151   0.2480255  0.24803822 0.24813735 0.24815424
 0.24796462 0.24762967 0.24735284 0.2472676  0.24728078 0.24719281
 0.24695595 0.24669015 0.2465723  0.24664158 0.24669665 0.24652925
 0.24620718 0.24608088 0.24633804 0.24684724 0.24707565 0.24685276
 0.24650553 0.2464943  0.2469288  0.2473144  0.24702124 0.24638261
 0.24622703 0.24695823 0.24751182 0.24656032 0.2446604  0.24529947]
