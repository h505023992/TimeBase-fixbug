Args in experiment:
Namespace(is_training=1, model_id='traffic_336_96', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=130, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_336_96_FITS_custom_ftM_sl336_ll48_pl96_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11849
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=130, out_features=167, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2395394560.0
params:  21877.0
Trainable parameters:  21877
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 39.49480056762695
Epoch: 1, Steps: 92 | Train Loss: 1.1228681 Vali Loss: 1.2171004 Test Loss: 1.3984985
Validation loss decreased (inf --> 1.217100).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 39.83836245536804
Epoch: 2, Steps: 92 | Train Loss: 0.8445590 Vali Loss: 1.0463511 Test Loss: 1.2069789
Validation loss decreased (1.217100 --> 1.046351).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 40.64951038360596
Epoch: 3, Steps: 92 | Train Loss: 0.7303123 Vali Loss: 0.9654703 Test Loss: 1.1148419
Validation loss decreased (1.046351 --> 0.965470).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 45.573617458343506
Epoch: 4, Steps: 92 | Train Loss: 0.6560866 Vali Loss: 0.9054744 Test Loss: 1.0457782
Validation loss decreased (0.965470 --> 0.905474).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 40.02756190299988
Epoch: 5, Steps: 92 | Train Loss: 0.5954424 Vali Loss: 0.8524979 Test Loss: 0.9868859
Validation loss decreased (0.905474 --> 0.852498).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 39.734270095825195
Epoch: 6, Steps: 92 | Train Loss: 0.5435025 Vali Loss: 0.8072980 Test Loss: 0.9337443
Validation loss decreased (0.852498 --> 0.807298).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 35.90795087814331
Epoch: 7, Steps: 92 | Train Loss: 0.4984108 Vali Loss: 0.7675608 Test Loss: 0.8903484
Validation loss decreased (0.807298 --> 0.767561).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 38.19448184967041
Epoch: 8, Steps: 92 | Train Loss: 0.4589600 Vali Loss: 0.7310559 Test Loss: 0.8491487
Validation loss decreased (0.767561 --> 0.731056).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 36.03183150291443
Epoch: 9, Steps: 92 | Train Loss: 0.4242692 Vali Loss: 0.7012225 Test Loss: 0.8127844
Validation loss decreased (0.731056 --> 0.701222).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 34.45419883728027
Epoch: 10, Steps: 92 | Train Loss: 0.3936376 Vali Loss: 0.6718471 Test Loss: 0.7808376
Validation loss decreased (0.701222 --> 0.671847).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 36.645254611968994
Epoch: 11, Steps: 92 | Train Loss: 0.3663721 Vali Loss: 0.6472815 Test Loss: 0.7527093
Validation loss decreased (0.671847 --> 0.647281).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 40.1429443359375
Epoch: 12, Steps: 92 | Train Loss: 0.3422456 Vali Loss: 0.6242571 Test Loss: 0.7260726
Validation loss decreased (0.647281 --> 0.624257).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 40.17189431190491
Epoch: 13, Steps: 92 | Train Loss: 0.3206651 Vali Loss: 0.6023585 Test Loss: 0.7028771
Validation loss decreased (0.624257 --> 0.602359).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 40.75165033340454
Epoch: 14, Steps: 92 | Train Loss: 0.3014317 Vali Loss: 0.5852838 Test Loss: 0.6816240
Validation loss decreased (0.602359 --> 0.585284).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 36.959134340286255
Epoch: 15, Steps: 92 | Train Loss: 0.2841530 Vali Loss: 0.5687178 Test Loss: 0.6620060
Validation loss decreased (0.585284 --> 0.568718).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 37.87009119987488
Epoch: 16, Steps: 92 | Train Loss: 0.2685834 Vali Loss: 0.5534698 Test Loss: 0.6468484
Validation loss decreased (0.568718 --> 0.553470).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 38.92104411125183
Epoch: 17, Steps: 92 | Train Loss: 0.2545587 Vali Loss: 0.5387124 Test Loss: 0.6303231
Validation loss decreased (0.553470 --> 0.538712).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 38.52281141281128
Epoch: 18, Steps: 92 | Train Loss: 0.2419179 Vali Loss: 0.5273234 Test Loss: 0.6167474
Validation loss decreased (0.538712 --> 0.527323).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 38.71530365943909
Epoch: 19, Steps: 92 | Train Loss: 0.2304658 Vali Loss: 0.5164278 Test Loss: 0.6035895
Validation loss decreased (0.527323 --> 0.516428).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 36.84046459197998
Epoch: 20, Steps: 92 | Train Loss: 0.2200552 Vali Loss: 0.5069705 Test Loss: 0.5923524
Validation loss decreased (0.516428 --> 0.506970).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 39.082003355026245
Epoch: 21, Steps: 92 | Train Loss: 0.2106062 Vali Loss: 0.4972405 Test Loss: 0.5814221
Validation loss decreased (0.506970 --> 0.497241).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 39.6677143573761
Epoch: 22, Steps: 92 | Train Loss: 0.2019921 Vali Loss: 0.4864056 Test Loss: 0.5712329
Validation loss decreased (0.497241 --> 0.486406).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 38.85017490386963
Epoch: 23, Steps: 92 | Train Loss: 0.1940719 Vali Loss: 0.4802631 Test Loss: 0.5626316
Validation loss decreased (0.486406 --> 0.480263).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 39.93485617637634
Epoch: 24, Steps: 92 | Train Loss: 0.1868997 Vali Loss: 0.4721876 Test Loss: 0.5543425
Validation loss decreased (0.480263 --> 0.472188).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 39.45664954185486
Epoch: 25, Steps: 92 | Train Loss: 0.1802999 Vali Loss: 0.4656357 Test Loss: 0.5467917
Validation loss decreased (0.472188 --> 0.465636).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 36.99943709373474
Epoch: 26, Steps: 92 | Train Loss: 0.1742144 Vali Loss: 0.4588461 Test Loss: 0.5400987
Validation loss decreased (0.465636 --> 0.458846).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 39.43975257873535
Epoch: 27, Steps: 92 | Train Loss: 0.1686706 Vali Loss: 0.4524957 Test Loss: 0.5328805
Validation loss decreased (0.458846 --> 0.452496).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 37.89540767669678
Epoch: 28, Steps: 92 | Train Loss: 0.1635523 Vali Loss: 0.4492993 Test Loss: 0.5276458
Validation loss decreased (0.452496 --> 0.449299).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 37.64881706237793
Epoch: 29, Steps: 92 | Train Loss: 0.1588154 Vali Loss: 0.4429157 Test Loss: 0.5216302
Validation loss decreased (0.449299 --> 0.442916).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 38.7117280960083
Epoch: 30, Steps: 92 | Train Loss: 0.1544471 Vali Loss: 0.4377808 Test Loss: 0.5172051
Validation loss decreased (0.442916 --> 0.437781).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11849
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=130, out_features=167, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2395394560.0
params:  21877.0
Trainable parameters:  21877
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 39.42973279953003
Epoch: 1, Steps: 92 | Train Loss: 0.2760605 Vali Loss: 0.3416984 Test Loss: 0.4157707
Validation loss decreased (inf --> 0.341698).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 38.79861521720886
Epoch: 2, Steps: 92 | Train Loss: 0.2492667 Vali Loss: 0.3389545 Test Loss: 0.4142754
Validation loss decreased (0.341698 --> 0.338955).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 39.48400807380676
Epoch: 3, Steps: 92 | Train Loss: 0.2483145 Vali Loss: 0.3380620 Test Loss: 0.4141280
Validation loss decreased (0.338955 --> 0.338062).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 39.56011629104614
Epoch: 4, Steps: 92 | Train Loss: 0.2482675 Vali Loss: 0.3389349 Test Loss: 0.4137968
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 38.68627381324768
Epoch: 5, Steps: 92 | Train Loss: 0.2479300 Vali Loss: 0.3368161 Test Loss: 0.4137404
Validation loss decreased (0.338062 --> 0.336816).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 35.85034966468811
Epoch: 6, Steps: 92 | Train Loss: 0.2479044 Vali Loss: 0.3367186 Test Loss: 0.4132684
Validation loss decreased (0.336816 --> 0.336719).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 38.93734431266785
Epoch: 7, Steps: 92 | Train Loss: 0.2479461 Vali Loss: 0.3384930 Test Loss: 0.4133481
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 40.40269064903259
Epoch: 8, Steps: 92 | Train Loss: 0.2478143 Vali Loss: 0.3380371 Test Loss: 0.4134725
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 40.62041115760803
Epoch: 9, Steps: 92 | Train Loss: 0.2477126 Vali Loss: 0.3376186 Test Loss: 0.4131776
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 37.09056830406189
Epoch: 10, Steps: 92 | Train Loss: 0.2477757 Vali Loss: 0.3379772 Test Loss: 0.4132616
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 38.9044144153595
Epoch: 11, Steps: 92 | Train Loss: 0.2477909 Vali Loss: 0.3377424 Test Loss: 0.4132095
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_336_96_FITS_custom_ftM_sl336_ll48_pl96_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.4116695523262024, mae:0.2820005416870117, rse:0.5312853455543518, corr:[0.27920172 0.29181483 0.29215702 0.29106706 0.29212654 0.2919652
 0.29198343 0.29255536 0.29243034 0.29233113 0.29213962 0.29170588
 0.29168558 0.29129636 0.29087195 0.29135117 0.29174694 0.29175034
 0.29155764 0.29103687 0.29090253 0.29056507 0.28987342 0.29084095
 0.29240793 0.29240748 0.2923653  0.2921974  0.29214782 0.2921931
 0.29189605 0.29179913 0.2918854  0.29184744 0.2918018  0.29125944
 0.29040512 0.2901772  0.29064536 0.29104066 0.29115406 0.29118374
 0.29116073 0.2914096  0.291677   0.29132092 0.29077557 0.29081902
 0.29128027 0.29124865 0.2910857  0.29146862 0.29165322 0.29139066
 0.29139188 0.29110867 0.2910242  0.29149014 0.29119053 0.2905595
 0.29042897 0.2905988  0.29135504 0.29179314 0.29154745 0.29185978
 0.2922965  0.2920622  0.2918664  0.2917325  0.2912201  0.2910988
 0.2909734  0.29034826 0.2903647  0.29044005 0.29003122 0.29025367
 0.2903648  0.29045093 0.29097387 0.29044425 0.2901089  0.29061365
 0.29019812 0.28998575 0.29011402 0.28986958 0.29023024 0.29042944
 0.29062936 0.29030722 0.28890404 0.28882504 0.28769    0.29023787]
