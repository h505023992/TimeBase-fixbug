Args in experiment:
Namespace(is_training=1, model_id='ETTh2_720_96', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=96, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=196, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  38986752.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.7366418838500977
Epoch: 1, Steps: 61 | Train Loss: 0.5519803 Vali Loss: 0.3934826 Test Loss: 0.3883989
Validation loss decreased (inf --> 0.393483).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.1787853240966797
Epoch: 2, Steps: 61 | Train Loss: 0.4239087 Vali Loss: 0.3398600 Test Loss: 0.3496014
Validation loss decreased (0.393483 --> 0.339860).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.614593029022217
Epoch: 3, Steps: 61 | Train Loss: 0.3608664 Vali Loss: 0.3215057 Test Loss: 0.3365227
Validation loss decreased (0.339860 --> 0.321506).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.278489351272583
Epoch: 4, Steps: 61 | Train Loss: 0.3228644 Vali Loss: 0.3168986 Test Loss: 0.3322966
Validation loss decreased (0.321506 --> 0.316899).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.7095837593078613
Epoch: 5, Steps: 61 | Train Loss: 0.2958535 Vali Loss: 0.3134468 Test Loss: 0.3296292
Validation loss decreased (0.316899 --> 0.313447).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.3609001636505127
Epoch: 6, Steps: 61 | Train Loss: 0.2743553 Vali Loss: 0.3102456 Test Loss: 0.3280142
Validation loss decreased (0.313447 --> 0.310246).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.209812641143799
Epoch: 7, Steps: 61 | Train Loss: 0.2570530 Vali Loss: 0.3077917 Test Loss: 0.3261996
Validation loss decreased (0.310246 --> 0.307792).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.3369016647338867
Epoch: 8, Steps: 61 | Train Loss: 0.2417466 Vali Loss: 0.3058114 Test Loss: 0.3244205
Validation loss decreased (0.307792 --> 0.305811).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.4277474880218506
Epoch: 9, Steps: 61 | Train Loss: 0.2287429 Vali Loss: 0.3023170 Test Loss: 0.3227237
Validation loss decreased (0.305811 --> 0.302317).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.353616237640381
Epoch: 10, Steps: 61 | Train Loss: 0.2173227 Vali Loss: 0.2998021 Test Loss: 0.3207986
Validation loss decreased (0.302317 --> 0.299802).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.338008165359497
Epoch: 11, Steps: 61 | Train Loss: 0.2068674 Vali Loss: 0.2971934 Test Loss: 0.3188727
Validation loss decreased (0.299802 --> 0.297193).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.376483917236328
Epoch: 12, Steps: 61 | Train Loss: 0.1980021 Vali Loss: 0.2941425 Test Loss: 0.3168963
Validation loss decreased (0.297193 --> 0.294142).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.040071964263916
Epoch: 13, Steps: 61 | Train Loss: 0.1897737 Vali Loss: 0.2921363 Test Loss: 0.3151725
Validation loss decreased (0.294142 --> 0.292136).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.2250559329986572
Epoch: 14, Steps: 61 | Train Loss: 0.1823999 Vali Loss: 0.2896652 Test Loss: 0.3133029
Validation loss decreased (0.292136 --> 0.289665).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.2900967597961426
Epoch: 15, Steps: 61 | Train Loss: 0.1757859 Vali Loss: 0.2874363 Test Loss: 0.3116947
Validation loss decreased (0.289665 --> 0.287436).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.202191114425659
Epoch: 16, Steps: 61 | Train Loss: 0.1695442 Vali Loss: 0.2836775 Test Loss: 0.3101744
Validation loss decreased (0.287436 --> 0.283677).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.224503517150879
Epoch: 17, Steps: 61 | Train Loss: 0.1639959 Vali Loss: 0.2817485 Test Loss: 0.3085977
Validation loss decreased (0.283677 --> 0.281749).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.2727930545806885
Epoch: 18, Steps: 61 | Train Loss: 0.1588988 Vali Loss: 0.2804170 Test Loss: 0.3072749
Validation loss decreased (0.281749 --> 0.280417).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.8306756019592285
Epoch: 19, Steps: 61 | Train Loss: 0.1546092 Vali Loss: 0.2781437 Test Loss: 0.3056791
Validation loss decreased (0.280417 --> 0.278144).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.976854085922241
Epoch: 20, Steps: 61 | Train Loss: 0.1501363 Vali Loss: 0.2767414 Test Loss: 0.3044976
Validation loss decreased (0.278144 --> 0.276741).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.0056674480438232
Epoch: 21, Steps: 61 | Train Loss: 0.1464128 Vali Loss: 0.2753083 Test Loss: 0.3034845
Validation loss decreased (0.276741 --> 0.275308).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.465017557144165
Epoch: 22, Steps: 61 | Train Loss: 0.1427499 Vali Loss: 0.2725851 Test Loss: 0.3023434
Validation loss decreased (0.275308 --> 0.272585).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.171053171157837
Epoch: 23, Steps: 61 | Train Loss: 0.1396158 Vali Loss: 0.2710926 Test Loss: 0.3011942
Validation loss decreased (0.272585 --> 0.271093).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.5291574001312256
Epoch: 24, Steps: 61 | Train Loss: 0.1366224 Vali Loss: 0.2695412 Test Loss: 0.3003402
Validation loss decreased (0.271093 --> 0.269541).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.820236921310425
Epoch: 25, Steps: 61 | Train Loss: 0.1337784 Vali Loss: 0.2695516 Test Loss: 0.2994314
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.769941806793213
Epoch: 26, Steps: 61 | Train Loss: 0.1311603 Vali Loss: 0.2682586 Test Loss: 0.2985658
Validation loss decreased (0.269541 --> 0.268259).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.02189302444458
Epoch: 27, Steps: 61 | Train Loss: 0.1287137 Vali Loss: 0.2670416 Test Loss: 0.2977870
Validation loss decreased (0.268259 --> 0.267042).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.084012031555176
Epoch: 28, Steps: 61 | Train Loss: 0.1264802 Vali Loss: 0.2643517 Test Loss: 0.2970273
Validation loss decreased (0.267042 --> 0.264352).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.0714759826660156
Epoch: 29, Steps: 61 | Train Loss: 0.1245032 Vali Loss: 0.2646340 Test Loss: 0.2963310
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.615060806274414
Epoch: 30, Steps: 61 | Train Loss: 0.1224868 Vali Loss: 0.2633407 Test Loss: 0.2956895
Validation loss decreased (0.264352 --> 0.263341).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  38986752.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.10269832611084
Epoch: 1, Steps: 61 | Train Loss: 0.4374692 Vali Loss: 0.2301917 Test Loss: 0.2749988
Validation loss decreased (inf --> 0.230192).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.134232997894287
Epoch: 2, Steps: 61 | Train Loss: 0.4152816 Vali Loss: 0.2231824 Test Loss: 0.2734817
Validation loss decreased (0.230192 --> 0.223182).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.9410974979400635
Epoch: 3, Steps: 61 | Train Loss: 0.4100678 Vali Loss: 0.2204474 Test Loss: 0.2731014
Validation loss decreased (0.223182 --> 0.220447).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.6625001430511475
Epoch: 4, Steps: 61 | Train Loss: 0.4067571 Vali Loss: 0.2192438 Test Loss: 0.2728740
Validation loss decreased (0.220447 --> 0.219244).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.9820501804351807
Epoch: 5, Steps: 61 | Train Loss: 0.4048871 Vali Loss: 0.2157916 Test Loss: 0.2726745
Validation loss decreased (0.219244 --> 0.215792).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.036503791809082
Epoch: 6, Steps: 61 | Train Loss: 0.4043065 Vali Loss: 0.2170115 Test Loss: 0.2725594
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.911607265472412
Epoch: 7, Steps: 61 | Train Loss: 0.4033601 Vali Loss: 0.2154119 Test Loss: 0.2721603
Validation loss decreased (0.215792 --> 0.215412).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.7810206413269043
Epoch: 8, Steps: 61 | Train Loss: 0.4014614 Vali Loss: 0.2148513 Test Loss: 0.2723121
Validation loss decreased (0.215412 --> 0.214851).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.8159584999084473
Epoch: 9, Steps: 61 | Train Loss: 0.4022379 Vali Loss: 0.2154124 Test Loss: 0.2721241
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.755387783050537
Epoch: 10, Steps: 61 | Train Loss: 0.4021025 Vali Loss: 0.2145566 Test Loss: 0.2722867
Validation loss decreased (0.214851 --> 0.214557).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.652738332748413
Epoch: 11, Steps: 61 | Train Loss: 0.4013162 Vali Loss: 0.2139542 Test Loss: 0.2721464
Validation loss decreased (0.214557 --> 0.213954).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.0134215354919434
Epoch: 12, Steps: 61 | Train Loss: 0.3997645 Vali Loss: 0.2136025 Test Loss: 0.2718273
Validation loss decreased (0.213954 --> 0.213603).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.3131980895996094
Epoch: 13, Steps: 61 | Train Loss: 0.4008477 Vali Loss: 0.2131113 Test Loss: 0.2718224
Validation loss decreased (0.213603 --> 0.213111).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.0082499980926514
Epoch: 14, Steps: 61 | Train Loss: 0.4004862 Vali Loss: 0.2130068 Test Loss: 0.2719014
Validation loss decreased (0.213111 --> 0.213007).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.9920287132263184
Epoch: 15, Steps: 61 | Train Loss: 0.3999496 Vali Loss: 0.2122741 Test Loss: 0.2715303
Validation loss decreased (0.213007 --> 0.212274).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.940504789352417
Epoch: 16, Steps: 61 | Train Loss: 0.3996314 Vali Loss: 0.2138634 Test Loss: 0.2715480
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.3935489654541016
Epoch: 17, Steps: 61 | Train Loss: 0.3995665 Vali Loss: 0.2135820 Test Loss: 0.2716478
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.7522730827331543
Epoch: 18, Steps: 61 | Train Loss: 0.3997466 Vali Loss: 0.2140873 Test Loss: 0.2717992
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.099940061569214
Epoch: 19, Steps: 61 | Train Loss: 0.3985451 Vali Loss: 0.2121765 Test Loss: 0.2715533
Validation loss decreased (0.212274 --> 0.212176).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.723994255065918
Epoch: 20, Steps: 61 | Train Loss: 0.3992964 Vali Loss: 0.2118941 Test Loss: 0.2716882
Validation loss decreased (0.212176 --> 0.211894).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.6535098552703857
Epoch: 21, Steps: 61 | Train Loss: 0.3993017 Vali Loss: 0.2119439 Test Loss: 0.2715059
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.9747350215911865
Epoch: 22, Steps: 61 | Train Loss: 0.3993224 Vali Loss: 0.2125956 Test Loss: 0.2715558
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.9991679191589355
Epoch: 23, Steps: 61 | Train Loss: 0.3988784 Vali Loss: 0.2121772 Test Loss: 0.2715578
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.9611754417419434
Epoch: 24, Steps: 61 | Train Loss: 0.3989550 Vali Loss: 0.2128824 Test Loss: 0.2715648
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.5480129718780518
Epoch: 25, Steps: 61 | Train Loss: 0.3982859 Vali Loss: 0.2117440 Test Loss: 0.2715501
Validation loss decreased (0.211894 --> 0.211744).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.9162557125091553
Epoch: 26, Steps: 61 | Train Loss: 0.3986122 Vali Loss: 0.2118004 Test Loss: 0.2715737
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.729452610015869
Epoch: 27, Steps: 61 | Train Loss: 0.3985280 Vali Loss: 0.2130409 Test Loss: 0.2714069
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.8941893577575684
Epoch: 28, Steps: 61 | Train Loss: 0.3974867 Vali Loss: 0.2126124 Test Loss: 0.2714042
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.08383846282959
Epoch: 29, Steps: 61 | Train Loss: 0.3981328 Vali Loss: 0.2119529 Test Loss: 0.2714774
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.0632894039154053
Epoch: 30, Steps: 61 | Train Loss: 0.3984529 Vali Loss: 0.2122791 Test Loss: 0.2714556
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.27124568819999695, mae:0.3362613916397095, rse:0.41972294449806213, corr:[0.27383816 0.27629945 0.27511036 0.27522486 0.27473372 0.27343687
 0.27281624 0.27240917 0.27146846 0.2702326  0.26928043 0.26796085
 0.26620936 0.26482454 0.26424405 0.2639041  0.26321927 0.26272377
 0.26216793 0.260787   0.2590987  0.25819927 0.2575361  0.25581378
 0.25351506 0.25185713 0.25050673 0.24868236 0.24723262 0.24653089
 0.24537775 0.24343126 0.24190053 0.24123137 0.23990795 0.23807622
 0.23719624 0.23692062 0.23589306 0.23432985 0.23344482 0.23295583
 0.23236148 0.23176433 0.23121822 0.23025781 0.22922574 0.22796169
 0.22593425 0.22355379 0.22204287 0.22082599 0.21976069 0.21840028
 0.21668527 0.21516173 0.21335562 0.21150695 0.21014364 0.20890215
 0.2078535  0.2075983  0.20804064 0.20800048 0.20669739 0.20611307
 0.20594586 0.20543836 0.20524435 0.20525037 0.20437303 0.20318237
 0.20250344 0.20196964 0.20097795 0.19922754 0.19835785 0.19808966
 0.19807516 0.19736178 0.19707216 0.19691026 0.19643447 0.19585282
 0.1950986  0.19562994 0.19665523 0.19622014 0.19427231 0.19464467
 0.19516128 0.192713   0.1922173  0.1940196  0.19161294 0.19140093]
