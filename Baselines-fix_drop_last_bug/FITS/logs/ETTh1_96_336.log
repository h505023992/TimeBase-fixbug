Args in experiment:
Namespace(is_training=1, model_id='ETTh1_96_336', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=40, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_96_336_FITS_ETTh1_ftM_sl96_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8209
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=40, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6451200.0
params:  7380.0
Trainable parameters:  7380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.64525580406189
Epoch: 1, Steps: 64 | Train Loss: 1.0519556 Vali Loss: 2.2985880 Test Loss: 1.2598932
Validation loss decreased (inf --> 2.298588).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.4040093421936035
Epoch: 2, Steps: 64 | Train Loss: 0.8145304 Vali Loss: 1.9888817 Test Loss: 1.0029699
Validation loss decreased (2.298588 --> 1.988882).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.4195377826690674
Epoch: 3, Steps: 64 | Train Loss: 0.6811316 Vali Loss: 1.8149073 Test Loss: 0.8608102
Validation loss decreased (1.988882 --> 1.814907).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.4508676528930664
Epoch: 4, Steps: 64 | Train Loss: 0.6057060 Vali Loss: 1.7137213 Test Loss: 0.7775370
Validation loss decreased (1.814907 --> 1.713721).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.8657209873199463
Epoch: 5, Steps: 64 | Train Loss: 0.5612932 Vali Loss: 1.6512836 Test Loss: 0.7278411
Validation loss decreased (1.713721 --> 1.651284).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.3193869590759277
Epoch: 6, Steps: 64 | Train Loss: 0.5342331 Vali Loss: 1.6051919 Test Loss: 0.6960922
Validation loss decreased (1.651284 --> 1.605192).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.061793565750122
Epoch: 7, Steps: 64 | Train Loss: 0.5156959 Vali Loss: 1.5793391 Test Loss: 0.6743604
Validation loss decreased (1.605192 --> 1.579339).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.9837732315063477
Epoch: 8, Steps: 64 | Train Loss: 0.5029882 Vali Loss: 1.5614582 Test Loss: 0.6581071
Validation loss decreased (1.579339 --> 1.561458).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.966635227203369
Epoch: 9, Steps: 64 | Train Loss: 0.4934543 Vali Loss: 1.5406312 Test Loss: 0.6449671
Validation loss decreased (1.561458 --> 1.540631).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.131687641143799
Epoch: 10, Steps: 64 | Train Loss: 0.4856916 Vali Loss: 1.5318791 Test Loss: 0.6343041
Validation loss decreased (1.540631 --> 1.531879).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.569702386856079
Epoch: 11, Steps: 64 | Train Loss: 0.4789580 Vali Loss: 1.5135189 Test Loss: 0.6243842
Validation loss decreased (1.531879 --> 1.513519).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.5941333770751953
Epoch: 12, Steps: 64 | Train Loss: 0.4733982 Vali Loss: 1.4993706 Test Loss: 0.6158085
Validation loss decreased (1.513519 --> 1.499371).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.52402925491333
Epoch: 13, Steps: 64 | Train Loss: 0.4683464 Vali Loss: 1.4929640 Test Loss: 0.6081408
Validation loss decreased (1.499371 --> 1.492964).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.519395589828491
Epoch: 14, Steps: 64 | Train Loss: 0.4637615 Vali Loss: 1.4905310 Test Loss: 0.6011720
Validation loss decreased (1.492964 --> 1.490531).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.0020933151245117
Epoch: 15, Steps: 64 | Train Loss: 0.4596551 Vali Loss: 1.4761957 Test Loss: 0.5946916
Validation loss decreased (1.490531 --> 1.476196).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.2415945529937744
Epoch: 16, Steps: 64 | Train Loss: 0.4556973 Vali Loss: 1.4738635 Test Loss: 0.5888532
Validation loss decreased (1.476196 --> 1.473863).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.237628698348999
Epoch: 17, Steps: 64 | Train Loss: 0.4525566 Vali Loss: 1.4658930 Test Loss: 0.5834517
Validation loss decreased (1.473863 --> 1.465893).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.178093194961548
Epoch: 18, Steps: 64 | Train Loss: 0.4493859 Vali Loss: 1.4622331 Test Loss: 0.5783265
Validation loss decreased (1.465893 --> 1.462233).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.1924760341644287
Epoch: 19, Steps: 64 | Train Loss: 0.4463301 Vali Loss: 1.4427614 Test Loss: 0.5737758
Validation loss decreased (1.462233 --> 1.442761).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.223480463027954
Epoch: 20, Steps: 64 | Train Loss: 0.4435321 Vali Loss: 1.4392046 Test Loss: 0.5695207
Validation loss decreased (1.442761 --> 1.439205).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.8416059017181396
Epoch: 21, Steps: 64 | Train Loss: 0.4411463 Vali Loss: 1.4345206 Test Loss: 0.5655553
Validation loss decreased (1.439205 --> 1.434521).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.625042676925659
Epoch: 22, Steps: 64 | Train Loss: 0.4387910 Vali Loss: 1.4287004 Test Loss: 0.5620340
Validation loss decreased (1.434521 --> 1.428700).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.4756455421447754
Epoch: 23, Steps: 64 | Train Loss: 0.4367269 Vali Loss: 1.4230977 Test Loss: 0.5585482
Validation loss decreased (1.428700 --> 1.423098).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.9779884815216064
Epoch: 24, Steps: 64 | Train Loss: 0.4348945 Vali Loss: 1.4219186 Test Loss: 0.5554868
Validation loss decreased (1.423098 --> 1.421919).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.3044166564941406
Epoch: 25, Steps: 64 | Train Loss: 0.4330042 Vali Loss: 1.4200786 Test Loss: 0.5525467
Validation loss decreased (1.421919 --> 1.420079).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.4105875492095947
Epoch: 26, Steps: 64 | Train Loss: 0.4312475 Vali Loss: 1.4095360 Test Loss: 0.5499032
Validation loss decreased (1.420079 --> 1.409536).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.0154151916503906
Epoch: 27, Steps: 64 | Train Loss: 0.4298318 Vali Loss: 1.4100103 Test Loss: 0.5473546
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.0816502571105957
Epoch: 28, Steps: 64 | Train Loss: 0.4281426 Vali Loss: 1.4086529 Test Loss: 0.5449509
Validation loss decreased (1.409536 --> 1.408653).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.0580646991729736
Epoch: 29, Steps: 64 | Train Loss: 0.4266403 Vali Loss: 1.4083744 Test Loss: 0.5427510
Validation loss decreased (1.408653 --> 1.408374).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.440781354904175
Epoch: 30, Steps: 64 | Train Loss: 0.4252811 Vali Loss: 1.4023044 Test Loss: 0.5406750
Validation loss decreased (1.408374 --> 1.402304).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 8209
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=40, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6451200.0
params:  7380.0
Trainable parameters:  7380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.3758926391601562
Epoch: 1, Steps: 64 | Train Loss: 0.5207936 Vali Loss: 1.3672252 Test Loss: 0.5125911
Validation loss decreased (inf --> 1.367225).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.420553207397461
Epoch: 2, Steps: 64 | Train Loss: 0.5071344 Vali Loss: 1.3406202 Test Loss: 0.4962265
Validation loss decreased (1.367225 --> 1.340620).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.36661958694458
Epoch: 3, Steps: 64 | Train Loss: 0.4984194 Vali Loss: 1.3171326 Test Loss: 0.4863896
Validation loss decreased (1.340620 --> 1.317133).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.9345192909240723
Epoch: 4, Steps: 64 | Train Loss: 0.4927577 Vali Loss: 1.3146677 Test Loss: 0.4805458
Validation loss decreased (1.317133 --> 1.314668).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.097445249557495
Epoch: 5, Steps: 64 | Train Loss: 0.4884250 Vali Loss: 1.3015966 Test Loss: 0.4772480
Validation loss decreased (1.314668 --> 1.301597).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.7298078536987305
Epoch: 6, Steps: 64 | Train Loss: 0.4860869 Vali Loss: 1.2900662 Test Loss: 0.4755217
Validation loss decreased (1.301597 --> 1.290066).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.477426528930664
Epoch: 7, Steps: 64 | Train Loss: 0.4847542 Vali Loss: 1.2878577 Test Loss: 0.4748355
Validation loss decreased (1.290066 --> 1.287858).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.3550469875335693
Epoch: 8, Steps: 64 | Train Loss: 0.4836282 Vali Loss: 1.2991773 Test Loss: 0.4746504
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.495542526245117
Epoch: 9, Steps: 64 | Train Loss: 0.4827235 Vali Loss: 1.2925950 Test Loss: 0.4747638
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.832395076751709
Epoch: 10, Steps: 64 | Train Loss: 0.4822820 Vali Loss: 1.2871205 Test Loss: 0.4749711
Validation loss decreased (1.287858 --> 1.287120).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.9618701934814453
Epoch: 11, Steps: 64 | Train Loss: 0.4823545 Vali Loss: 1.2904152 Test Loss: 0.4753210
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.9599344730377197
Epoch: 12, Steps: 64 | Train Loss: 0.4818178 Vali Loss: 1.2849084 Test Loss: 0.4756395
Validation loss decreased (1.287120 --> 1.284908).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.2252964973449707
Epoch: 13, Steps: 64 | Train Loss: 0.4814847 Vali Loss: 1.2868214 Test Loss: 0.4759211
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.538003921508789
Epoch: 14, Steps: 64 | Train Loss: 0.4816710 Vali Loss: 1.2879720 Test Loss: 0.4761450
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.5925452709198
Epoch: 15, Steps: 64 | Train Loss: 0.4817908 Vali Loss: 1.2873904 Test Loss: 0.4763398
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.4844202995300293
Epoch: 16, Steps: 64 | Train Loss: 0.4817047 Vali Loss: 1.2856045 Test Loss: 0.4765953
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.569850444793701
Epoch: 17, Steps: 64 | Train Loss: 0.4813679 Vali Loss: 1.2889071 Test Loss: 0.4766825
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_96_336_FITS_ETTh1_ftM_sl96_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4751099646091461, mae:0.4431818127632141, rse:0.656219482421875, corr:[0.25470066 0.25294268 0.25474674 0.25349718 0.25190717 0.24944614
 0.24688761 0.2474802  0.24654405 0.2469834  0.2470883  0.2463751
 0.24666776 0.2457087  0.24541332 0.2452897  0.24470595 0.2450027
 0.24485227 0.24460323 0.24457872 0.24471971 0.24521364 0.24607205
 0.24559458 0.24515115 0.24480455 0.24416688 0.24360107 0.24265613
 0.24218577 0.24176621 0.24130605 0.24122813 0.2407664  0.24110767
 0.24111466 0.24054974 0.2410893  0.24103984 0.2412021  0.24186178
 0.24197705 0.2420295  0.24209118 0.24249461 0.24326292 0.24402118
 0.24413235 0.24347752 0.24219564 0.24082506 0.2397427  0.2380742
 0.2367349  0.23612072 0.23554593 0.23561035 0.23522769 0.2350381
 0.23537527 0.23548341 0.23554935 0.23526636 0.23491322 0.23537365
 0.2359407  0.2361799  0.23626165 0.23631665 0.23652506 0.23658185
 0.23550123 0.23440515 0.23332132 0.2326236  0.2323612  0.23173372
 0.23132503 0.23082666 0.23054132 0.23031019 0.2296523  0.22938903
 0.22938177 0.22923361 0.22958592 0.22979644 0.2296108  0.2297992
 0.22996143 0.22999191 0.23004887 0.23012304 0.23029484 0.23109883
 0.23125333 0.23055969 0.23028067 0.22974335 0.22901177 0.2285508
 0.22812505 0.22754224 0.22760136 0.22738057 0.22683635 0.22663188
 0.22644833 0.22624172 0.22663356 0.22664875 0.2270219  0.22766325
 0.22773738 0.22809221 0.22833651 0.22827163 0.22857976 0.22851197
 0.22750168 0.22664134 0.22586022 0.22445108 0.22358897 0.22287673
 0.22198276 0.22181989 0.22185689 0.22157444 0.22107577 0.2207778
 0.2206994  0.2206097  0.22094579 0.22085002 0.22069682 0.22119488
 0.22165571 0.22181377 0.22192106 0.22225796 0.22270837 0.2225914
 0.22182631 0.22119436 0.22046581 0.21911396 0.21812366 0.21729207
 0.21674177 0.21635903 0.21654285 0.21667768 0.21653661 0.2166199
 0.21625763 0.21613573 0.21628264 0.2158229  0.21580704 0.2161978
 0.2164139  0.21683022 0.21701333 0.21707122 0.21751311 0.21737589
 0.21644191 0.21596697 0.21546221 0.2143461  0.2136412  0.21296662
 0.21201104 0.21151838 0.2116995  0.21173719 0.21177062 0.21222289
 0.21212533 0.21160223 0.21180807 0.2116134  0.21159713 0.21231607
 0.21277507 0.21342504 0.21410836 0.2144667  0.21495274 0.21520203
 0.21420705 0.2135022  0.21272486 0.21146877 0.21075702 0.2102457
 0.20955653 0.20915972 0.20928097 0.20925683 0.20886368 0.20882589
 0.20854326 0.20879741 0.20890471 0.20832944 0.20855907 0.20907642
 0.2090713  0.20953448 0.21006425 0.21071477 0.21135484 0.21160755
 0.21223561 0.2124975  0.21166666 0.21098159 0.21085712 0.21049173
 0.21015406 0.20994249 0.210035   0.20986354 0.20911545 0.20910653
 0.20914762 0.208995   0.20953205 0.20936993 0.20934698 0.20986205
 0.20992184 0.21026668 0.21092997 0.21109746 0.21128452 0.21148556
 0.21095306 0.21049625 0.20974612 0.2090568  0.2088608  0.20839843
 0.20799793 0.20778468 0.20776637 0.20795797 0.20739685 0.20687024
 0.20687287 0.20670597 0.2069511  0.20678353 0.20681643 0.20740812
 0.20785333 0.20823772 0.2084921  0.20904331 0.20979275 0.20981915
 0.2097734  0.20986207 0.20932491 0.20862888 0.20821023 0.20780163
 0.20737703 0.20723885 0.20760407 0.20767793 0.20727886 0.20744106
 0.20727786 0.20739803 0.20768417 0.207121   0.20699802 0.2069953
 0.20699467 0.20743734 0.20765077 0.20824933 0.2087741  0.20882474
 0.20900464 0.20899902 0.20882656 0.20847133 0.20807332 0.20828508
 0.20827398 0.20802458 0.20845667 0.20855421 0.208368   0.20863515
 0.20853628 0.20816793 0.20842275 0.20792979 0.20810771 0.208663
 0.20878269 0.20891722 0.20904754 0.20902665 0.20936026 0.20882936
 0.20849305 0.20832057 0.20748964 0.20662504 0.20603576 0.20558155
 0.20502397 0.20468153 0.20554018 0.20535892 0.20464694 0.2051849
 0.20505269 0.20562209 0.20583524 0.20503202 0.20549376 0.20545894
 0.20576248 0.20606582 0.20415115 0.20621301 0.20345762 0.20967664]
