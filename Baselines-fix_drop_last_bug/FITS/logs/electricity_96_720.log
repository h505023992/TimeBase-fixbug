Args in experiment:
Namespace(is_training=1, model_id='electricity_96_720', model='FITS', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=321, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=34, base_T=36, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : electricity_96_720_FITS_custom_ftM_sl96_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17597
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=34, out_features=289, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  403730688.0
params:  10115.0
Trainable parameters:  10115
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.7853941
	speed: 0.1946s/iter; left time: 780.5637s
Epoch: 1 cost time: 25.968250036239624
Epoch: 1, Steps: 137 | Train Loss: 2.2291294 Vali Loss: 1.3235079 Test Loss: 1.5107579
Validation loss decreased (inf --> 1.323508).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 1.0820807
	speed: 0.4597s/iter; left time: 1780.8914s
Epoch: 2 cost time: 27.572449922561646
Epoch: 2, Steps: 137 | Train Loss: 1.2327574 Vali Loss: 0.8684233 Test Loss: 1.0147775
Validation loss decreased (1.323508 --> 0.868423).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.8653005
	speed: 0.4814s/iter; left time: 1798.8920s
Epoch: 3 cost time: 28.600831508636475
Epoch: 3, Steps: 137 | Train Loss: 0.9098601 Vali Loss: 0.7105731 Test Loss: 0.8406120
Validation loss decreased (0.868423 --> 0.710573).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.7746526
	speed: 0.4755s/iter; left time: 1711.9273s
Epoch: 4 cost time: 27.452362298965454
Epoch: 4, Steps: 137 | Train Loss: 0.7832231 Vali Loss: 0.6332449 Test Loss: 0.7529552
Validation loss decreased (0.710573 --> 0.633245).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6906084
	speed: 0.4679s/iter; left time: 1620.2578s
Epoch: 5 cost time: 26.747761487960815
Epoch: 5, Steps: 137 | Train Loss: 0.7096090 Vali Loss: 0.5806728 Test Loss: 0.6915452
Validation loss decreased (0.633245 --> 0.580673).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6392675
	speed: 0.4519s/iter; left time: 1502.9178s
Epoch: 6 cost time: 25.977936267852783
Epoch: 6, Steps: 137 | Train Loss: 0.6531955 Vali Loss: 0.5373531 Test Loss: 0.6409593
Validation loss decreased (0.580673 --> 0.537353).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5928721
	speed: 0.4679s/iter; left time: 1492.1723s
Epoch: 7 cost time: 27.912429809570312
Epoch: 7, Steps: 137 | Train Loss: 0.6054661 Vali Loss: 0.5010332 Test Loss: 0.5981427
Validation loss decreased (0.537353 --> 0.501033).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5660478
	speed: 0.4462s/iter; left time: 1361.6668s
Epoch: 8 cost time: 25.964122772216797
Epoch: 8, Steps: 137 | Train Loss: 0.5643114 Vali Loss: 0.4687069 Test Loss: 0.5603999
Validation loss decreased (0.501033 --> 0.468707).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5245092
	speed: 0.4634s/iter; left time: 1350.7958s
Epoch: 9 cost time: 27.63054084777832
Epoch: 9, Steps: 137 | Train Loss: 0.5283100 Vali Loss: 0.4411084 Test Loss: 0.5275152
Validation loss decreased (0.468707 --> 0.441108).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4834067
	speed: 0.4820s/iter; left time: 1339.0280s
Epoch: 10 cost time: 28.319796085357666
Epoch: 10, Steps: 137 | Train Loss: 0.4967894 Vali Loss: 0.4162935 Test Loss: 0.4984029
Validation loss decreased (0.441108 --> 0.416293).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4561625
	speed: 0.4697s/iter; left time: 1240.4888s
Epoch: 11 cost time: 26.471009731292725
Epoch: 11, Steps: 137 | Train Loss: 0.4691511 Vali Loss: 0.3951601 Test Loss: 0.4728970
Validation loss decreased (0.416293 --> 0.395160).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4288170
	speed: 0.4603s/iter; left time: 1152.5622s
Epoch: 12 cost time: 26.800440311431885
Epoch: 12, Steps: 137 | Train Loss: 0.4448822 Vali Loss: 0.3763830 Test Loss: 0.4510240
Validation loss decreased (0.395160 --> 0.376383).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4179866
	speed: 0.4744s/iter; left time: 1122.8588s
Epoch: 13 cost time: 27.801267623901367
Epoch: 13, Steps: 137 | Train Loss: 0.4235614 Vali Loss: 0.3599212 Test Loss: 0.4308858
Validation loss decreased (0.376383 --> 0.359921).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3951207
	speed: 0.4915s/iter; left time: 1096.0214s
Epoch: 14 cost time: 28.630953311920166
Epoch: 14, Steps: 137 | Train Loss: 0.4047017 Vali Loss: 0.3458789 Test Loss: 0.4133935
Validation loss decreased (0.359921 --> 0.345879).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3814504
	speed: 0.4861s/iter; left time: 1017.4428s
Epoch: 15 cost time: 27.6164288520813
Epoch: 15, Steps: 137 | Train Loss: 0.3879313 Vali Loss: 0.3331395 Test Loss: 0.3979645
Validation loss decreased (0.345879 --> 0.333139).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3607152
	speed: 0.4844s/iter; left time: 947.5325s
Epoch: 16 cost time: 29.206167697906494
Epoch: 16, Steps: 137 | Train Loss: 0.3730912 Vali Loss: 0.3221522 Test Loss: 0.3843554
Validation loss decreased (0.333139 --> 0.322152).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3701464
	speed: 0.5039s/iter; left time: 916.6220s
Epoch: 17 cost time: 29.739166259765625
Epoch: 17, Steps: 137 | Train Loss: 0.3600219 Vali Loss: 0.3113470 Test Loss: 0.3721546
Validation loss decreased (0.322152 --> 0.311347).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3259555
	speed: 0.4979s/iter; left time: 837.4839s
Epoch: 18 cost time: 27.989750862121582
Epoch: 18, Steps: 137 | Train Loss: 0.3482856 Vali Loss: 0.3027603 Test Loss: 0.3610625
Validation loss decreased (0.311347 --> 0.302760).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3270423
	speed: 0.4932s/iter; left time: 761.9505s
Epoch: 19 cost time: 30.155580520629883
Epoch: 19, Steps: 137 | Train Loss: 0.3378179 Vali Loss: 0.2954179 Test Loss: 0.3515798
Validation loss decreased (0.302760 --> 0.295418).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3274791
	speed: 0.5508s/iter; left time: 775.5451s
Epoch: 20 cost time: 29.658205032348633
Epoch: 20, Steps: 137 | Train Loss: 0.3285273 Vali Loss: 0.2882075 Test Loss: 0.3426867
Validation loss decreased (0.295418 --> 0.288208).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3098636
	speed: 0.4833s/iter; left time: 614.2206s
Epoch: 21 cost time: 26.44931435585022
Epoch: 21, Steps: 137 | Train Loss: 0.3201112 Vali Loss: 0.2820733 Test Loss: 0.3348520
Validation loss decreased (0.288208 --> 0.282073).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3114211
	speed: 0.4665s/iter; left time: 529.0592s
Epoch: 22 cost time: 27.373790979385376
Epoch: 22, Steps: 137 | Train Loss: 0.3127177 Vali Loss: 0.2764413 Test Loss: 0.3277157
Validation loss decreased (0.282073 --> 0.276441).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3088152
	speed: 0.4665s/iter; left time: 465.0945s
Epoch: 23 cost time: 27.970654726028442
Epoch: 23, Steps: 137 | Train Loss: 0.3060231 Vali Loss: 0.2712444 Test Loss: 0.3216100
Validation loss decreased (0.276441 --> 0.271244).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3159961
	speed: 0.4844s/iter; left time: 416.5804s
Epoch: 24 cost time: 28.798552751541138
Epoch: 24, Steps: 137 | Train Loss: 0.3000337 Vali Loss: 0.2670013 Test Loss: 0.3157610
Validation loss decreased (0.271244 --> 0.267001).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2819215
	speed: 0.4744s/iter; left time: 343.0141s
Epoch: 25 cost time: 26.221211910247803
Epoch: 25, Steps: 137 | Train Loss: 0.2946767 Vali Loss: 0.2631538 Test Loss: 0.3107687
Validation loss decreased (0.267001 --> 0.263154).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2845114
	speed: 0.4544s/iter; left time: 266.2761s
Epoch: 26 cost time: 26.215656042099
Epoch: 26, Steps: 137 | Train Loss: 0.2898229 Vali Loss: 0.2590990 Test Loss: 0.3061774
Validation loss decreased (0.263154 --> 0.259099).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2899824
	speed: 0.4592s/iter; left time: 206.1928s
Epoch: 27 cost time: 27.346616983413696
Epoch: 27, Steps: 137 | Train Loss: 0.2854473 Vali Loss: 0.2565449 Test Loss: 0.3020140
Validation loss decreased (0.259099 --> 0.256545).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2793440
	speed: 0.4600s/iter; left time: 143.5165s
Epoch: 28 cost time: 26.742003440856934
Epoch: 28, Steps: 137 | Train Loss: 0.2815572 Vali Loss: 0.2535749 Test Loss: 0.2983216
Validation loss decreased (0.256545 --> 0.253575).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2837471
	speed: 0.4477s/iter; left time: 78.3456s
Epoch: 29 cost time: 25.760765552520752
Epoch: 29, Steps: 137 | Train Loss: 0.2780001 Vali Loss: 0.2509094 Test Loss: 0.2948917
Validation loss decreased (0.253575 --> 0.250909).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2672361
	speed: 0.4464s/iter; left time: 16.9636s
Epoch: 30 cost time: 26.82670307159424
Epoch: 30, Steps: 137 | Train Loss: 0.2748267 Vali Loss: 0.2486168 Test Loss: 0.2918040
Validation loss decreased (0.250909 --> 0.248617).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 17597
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=34, out_features=289, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  403730688.0
params:  10115.0
Trainable parameters:  10115
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2984741
	speed: 0.2082s/iter; left time: 835.1611s
Epoch: 1 cost time: 27.830337524414062
Epoch: 1, Steps: 137 | Train Loss: 0.2864583 Vali Loss: 0.2286944 Test Loss: 0.2633469
Validation loss decreased (inf --> 0.228694).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2647174
	speed: 0.4653s/iter; left time: 1802.4402s
Epoch: 2 cost time: 26.35527729988098
Epoch: 2, Steps: 137 | Train Loss: 0.2733897 Vali Loss: 0.2257714 Test Loss: 0.2583707
Validation loss decreased (0.228694 --> 0.225771).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2815861
	speed: 0.4474s/iter; left time: 1671.9050s
Epoch: 3 cost time: 25.967583656311035
Epoch: 3, Steps: 137 | Train Loss: 0.2715882 Vali Loss: 0.2256478 Test Loss: 0.2577417
Validation loss decreased (0.225771 --> 0.225648).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2796651
	speed: 0.4463s/iter; left time: 1606.8576s
Epoch: 4 cost time: 25.889817476272583
Epoch: 4, Steps: 137 | Train Loss: 0.2713501 Vali Loss: 0.2263596 Test Loss: 0.2576712
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2634175
	speed: 0.4497s/iter; left time: 1557.2689s
Epoch: 5 cost time: 26.156298637390137
Epoch: 5, Steps: 137 | Train Loss: 0.2713625 Vali Loss: 0.2257656 Test Loss: 0.2576703
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2661512
	speed: 0.4494s/iter; left time: 1494.7592s
Epoch: 6 cost time: 25.9507098197937
Epoch: 6, Steps: 137 | Train Loss: 0.2713253 Vali Loss: 0.2263731 Test Loss: 0.2576731
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2770542
	speed: 0.4447s/iter; left time: 1418.1617s
Epoch: 7 cost time: 25.98844313621521
Epoch: 7, Steps: 137 | Train Loss: 0.2712478 Vali Loss: 0.2260656 Test Loss: 0.2576608
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2750213
	speed: 0.4728s/iter; left time: 1443.0484s
Epoch: 8 cost time: 27.47702932357788
Epoch: 8, Steps: 137 | Train Loss: 0.2713284 Vali Loss: 0.2253692 Test Loss: 0.2576503
Validation loss decreased (0.225648 --> 0.225369).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2743834
	speed: 0.4760s/iter; left time: 1387.5997s
Epoch: 9 cost time: 26.703203916549683
Epoch: 9, Steps: 137 | Train Loss: 0.2712908 Vali Loss: 0.2252687 Test Loss: 0.2576412
Validation loss decreased (0.225369 --> 0.225269).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2715663
	speed: 0.4455s/iter; left time: 1237.5347s
Epoch: 10 cost time: 25.78714632987976
Epoch: 10, Steps: 137 | Train Loss: 0.2712901 Vali Loss: 0.2258835 Test Loss: 0.2576392
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2654196
	speed: 0.4522s/iter; left time: 1194.3381s
Epoch: 11 cost time: 26.222017288208008
Epoch: 11, Steps: 137 | Train Loss: 0.2712872 Vali Loss: 0.2257688 Test Loss: 0.2576365
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2627323
	speed: 0.4489s/iter; left time: 1123.9236s
Epoch: 12 cost time: 25.938244819641113
Epoch: 12, Steps: 137 | Train Loss: 0.2712798 Vali Loss: 0.2261179 Test Loss: 0.2576534
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2734049
	speed: 0.4515s/iter; left time: 1068.7802s
Epoch: 13 cost time: 25.91262412071228
Epoch: 13, Steps: 137 | Train Loss: 0.2713007 Vali Loss: 0.2260726 Test Loss: 0.2576755
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2573252
	speed: 0.4452s/iter; left time: 992.8304s
Epoch: 14 cost time: 25.714771270751953
Epoch: 14, Steps: 137 | Train Loss: 0.2712851 Vali Loss: 0.2257501 Test Loss: 0.2576848
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : electricity_96_720_FITS_custom_ftM_sl96_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2562873363494873, mae:0.328567773103714, rse:0.5049995183944702, corr:[0.4431027  0.44220966 0.44156063 0.44004557 0.4390248  0.43837324
 0.43735763 0.43659893 0.43574336 0.43500292 0.43459353 0.4341806
 0.43374527 0.43347663 0.43307197 0.4326204  0.43258673 0.43247652
 0.4322507  0.4321709  0.43201542 0.43193412 0.43191895 0.4320475
 0.43157673 0.43062735 0.43008876 0.42897937 0.4278527  0.42736986
 0.4268547  0.4262813  0.42582145 0.4252671  0.4250016  0.42486206
 0.42474452 0.42472157 0.42474067 0.42476776 0.42463902 0.4249455
 0.4251776  0.42533213 0.42593497 0.4266063  0.4272626  0.42873648
 0.42990604 0.43004778 0.43002626 0.4299998  0.42980942 0.4296341
 0.4295964  0.429438   0.4292396  0.42930785 0.42941606 0.4295021
 0.42971414 0.42983893 0.43016067 0.43073955 0.4311978  0.43177375
 0.43272495 0.43347645 0.4344758  0.43593353 0.43732443 0.4397453
 0.44197318 0.44277373 0.4431247  0.4432732  0.44337806 0.44338363
 0.4434215  0.44349837 0.4434254  0.4433007  0.44328573 0.4432717
 0.44319242 0.4431832  0.4432407  0.44321668 0.44318858 0.44324374
 0.44328913 0.4432078  0.44315243 0.44318464 0.44323847 0.4434244
 0.44348416 0.44337073 0.4433617  0.4432969  0.44314757 0.44303775
 0.44300792 0.4429628  0.44282848 0.4426829  0.4426229  0.44257054
 0.44244602 0.4424098  0.44253743 0.4426099  0.44255874 0.44260135
 0.4426984  0.4426395  0.44261184 0.44266817 0.44266427 0.44274125
 0.4427857  0.44272414 0.44269252 0.442722   0.44267455 0.44254124
 0.44246212 0.44239965 0.44228557 0.44218418 0.44216868 0.4421511
 0.44206014 0.44203207 0.4421195  0.44213924 0.44210494 0.44215932
 0.44224808 0.4422842  0.44236186 0.44242245 0.4424037  0.4425273
 0.4425914  0.44256645 0.4426053  0.442606   0.44253957 0.44247526
 0.4424353  0.4423767  0.44228935 0.44223848 0.442232   0.44217673
 0.4420978  0.44208485 0.44207275 0.44197512 0.44193545 0.44193766
 0.44184288 0.44164637 0.4416017  0.4415842  0.44104692 0.44000244
 0.43804604 0.4361283  0.43480426 0.43334156 0.4321961  0.4312742
 0.43029404 0.42948154 0.4287281  0.42807162 0.42771897 0.4274224
 0.4270535  0.42695498 0.42681304 0.4264607  0.42644823 0.42658868
 0.4264427  0.42612574 0.4260554  0.42618865 0.42618954 0.42601347
 0.4249474  0.423723   0.42313612 0.42218035 0.4210812  0.42055282
 0.42021593 0.41974902 0.4193426  0.418858   0.4186296  0.41856757
 0.41855815 0.41852146 0.41852218 0.41866717 0.41872486 0.41904995
 0.41933477 0.41943    0.41990313 0.42057773 0.42125633 0.4224917
 0.4235495  0.4240261  0.4242113  0.42407885 0.42381224 0.42368606
 0.42367405 0.4235842  0.42349073 0.42364314 0.42382872 0.42388535
 0.42395636 0.4241271  0.42452693 0.42492872 0.42536727 0.4261151
 0.4270986  0.42780167 0.42871633 0.43003398 0.43137854 0.43380535
 0.4363589  0.43735072 0.43770728 0.43790892 0.43808714 0.43815833
 0.43816927 0.43814775 0.4381023  0.43801016 0.43790168 0.43787548
 0.4378609  0.43777424 0.43775317 0.43780014 0.43780598 0.43784407
 0.43793297 0.43785265 0.43776104 0.43780953 0.43788666 0.4380391
 0.438069   0.437978   0.43794346 0.4378804  0.43780082 0.4377149
 0.4376709  0.43765196 0.4375372  0.43737075 0.43732104 0.43733042
 0.43724906 0.43714353 0.43716106 0.43723586 0.43733373 0.43740648
 0.43733972 0.43719158 0.4372003  0.43724495 0.43729198 0.43744496
 0.43745342 0.43735823 0.43737942 0.4373879  0.43730682 0.4372594
 0.43725246 0.43714905 0.43702248 0.43695238 0.43692616 0.43697467
 0.43703055 0.43701422 0.43702215 0.43708494 0.43712592 0.43713003
 0.43716675 0.4372154  0.43731502 0.43742302 0.43743637 0.437485
 0.43744162 0.4373705  0.43735456 0.4372568  0.4371601  0.43709308
 0.43705013 0.4370198  0.43698788 0.4369434  0.4369712  0.43701458
 0.4370259  0.43704364 0.43702018 0.4369712  0.43697783 0.43691435
 0.4368034  0.43668547 0.43652427 0.43632337 0.43585822 0.4346848
 0.43223423 0.43009448 0.42893726 0.4274187  0.4259246  0.42497355
 0.42412007 0.42327517 0.42250142 0.4218992  0.42152965 0.4212379
 0.42090416 0.42070708 0.42054865 0.420261   0.42023072 0.42035592
 0.42041457 0.42028728 0.4201376  0.4201805  0.4201551  0.41969627
 0.4182839  0.4170253  0.4163533  0.415282   0.41422668 0.4137462
 0.41344273 0.41305548 0.4126382  0.41216373 0.41199812 0.41188228
 0.41178244 0.41177416 0.41180214 0.41199103 0.41203636 0.41233024
 0.41260028 0.41283765 0.4134338  0.41419944 0.41497242 0.41620666
 0.4172325  0.41769618 0.417935   0.41792032 0.41775194 0.4176745
 0.417685   0.4175607  0.41742262 0.4175851  0.4177576  0.41784573
 0.41804275 0.41822234 0.41860542 0.41914415 0.41956073 0.42036948
 0.42156234 0.42227972 0.42320633 0.42488468 0.42642692 0.4288093
 0.43152475 0.43279022 0.43323073 0.43346447 0.4336406  0.43366656
 0.4337338  0.43378183 0.43361053 0.43344387 0.43345332 0.4334056
 0.43322203 0.4331851  0.43333778 0.43335804 0.4333073  0.43341857
 0.43352535 0.4334261  0.43338606 0.43342674 0.43347663 0.43370914
 0.43377924 0.4336793  0.43371555 0.43371117 0.43361804 0.43356094
 0.43351263 0.43339366 0.4333084  0.43326652 0.43315405 0.4330082
 0.43291315 0.43289232 0.43296027 0.4330609  0.43311638 0.43317562
 0.43322435 0.43311372 0.43301645 0.43306863 0.4331569  0.43320882
 0.43321288 0.43323624 0.43324247 0.43322515 0.4332496  0.43326336
 0.43320915 0.43315902 0.43313685 0.43306395 0.4329897  0.43300033
 0.43300006 0.43298194 0.43307486 0.4331194  0.4330731  0.4331277
 0.4332653  0.4333035  0.4333754  0.43348137 0.4334933  0.43355688
 0.43352044 0.43346015 0.43350005 0.4335109  0.43343443 0.4333755
 0.4333623  0.43334064 0.433327   0.43336332 0.43339488 0.4333473
 0.43332    0.43337047 0.43333715 0.4332074  0.43318862 0.43325362
 0.43325606 0.43309695 0.43289074 0.43267998 0.4321926  0.4310332
 0.42861497 0.42648622 0.4252075  0.42366084 0.42239985 0.4215863
 0.42072418 0.41988498 0.4190978  0.41849825 0.4182017  0.41788304
 0.41751704 0.41743866 0.41737434 0.4171008  0.41708553 0.41717368
 0.41706842 0.41685912 0.41676143 0.41673505 0.41660434 0.41614062
 0.41465977 0.41328585 0.41266966 0.41160142 0.41048014 0.41008198
 0.40978813 0.4092228  0.40874675 0.4083161  0.4081015  0.40794897
 0.40798473 0.4081121  0.4082224  0.40845272 0.40851203 0.40880013
 0.40910256 0.40931347 0.40989506 0.41062376 0.4112694  0.41233695
 0.41322318 0.41352886 0.4135803  0.4134637  0.41338882 0.41332725
 0.41320994 0.41306204 0.4129969  0.41313392 0.41329932 0.41340438
 0.4135178  0.41356462 0.4139338  0.41451603 0.41502354 0.4156663
 0.41657436 0.41736746 0.41836563 0.41972274 0.42106196 0.42346862
 0.42609942 0.42724037 0.42767063 0.42786944 0.4279397  0.42793655
 0.42800537 0.42803028 0.42794514 0.42788252 0.4278375  0.42776328
 0.42769235 0.42768398 0.42774475 0.42774847 0.42772198 0.42781103
 0.42793977 0.42787296 0.42784712 0.4279844  0.4280769  0.4281532
 0.4281301  0.42809996 0.42813048 0.42803562 0.42787787 0.42773387
 0.4276381  0.42757714 0.42746326 0.42523003 0.4272765  0.4271889
 0.4269914  0.42273098 0.4270159  0.42701358 0.4270256  0.42720154
 0.42513472 0.42289633 0.42297694 0.42732763 0.42316428 0.4232358
 0.4232528  0.4232058  0.42327112 0.42323366 0.4230628  0.4230384
 0.42306826 0.4229113  0.42272064 0.4226265  0.42255202 0.42251295
 0.42248827 0.42244262 0.4225098  0.42265555 0.4226997  0.42266476
 0.42277384 0.4229539  0.423075   0.42315537 0.42314687 0.42321026
 0.42325297 0.42319357 0.42308104 0.42304638 0.42313352 0.4230549
 0.4229073  0.42290267 0.42284155 0.42272943 0.42276943 0.42273843
 0.4225471  0.42254815 0.42267036 0.42260578 0.42253897 0.42257488
 0.42251357 0.42231655 0.42225    0.42214155 0.42151633 0.42023802
 0.4178122  0.41548818 0.41414535 0.4127018  0.4114078  0.41058204
 0.40966788 0.40874302 0.40806633 0.40745825 0.40702683 0.40679035
 0.40645796 0.4062367  0.40618107 0.40601662 0.4059827  0.4060683
 0.4060295  0.40575662 0.4056329  0.40584052 0.40570238 0.40497947
 0.4036235  0.40246922 0.40146655 0.40039974 0.3996363  0.39889112
 0.39842924 0.39815187 0.3976484  0.39725935 0.39726746 0.39707062
 0.39704978 0.39724475 0.39713714 0.39725268 0.39747795 0.39741415
 0.39773777 0.3982882  0.39847863 0.39961874 0.40015948 0.40298805]
