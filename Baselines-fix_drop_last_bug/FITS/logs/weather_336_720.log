Args in experiment:
Namespace(is_training=1, model_id='weather_336_720', model='FITS', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=3, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=46, base_T=144, H_order=12)
Use GPU: cuda:0
>>>>>>>start training : weather_336_720_FITS_custom_ftM_sl336_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35832
val 4551
test 9820
Model(
  (freq_upsampler): Linear(in_features=46, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17805312.0
params:  6768.0
Trainable parameters:  6768
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8255793
	speed: 0.0981s/iter; left time: 811.0237s
	iters: 200, epoch: 1 | loss: 0.7248565
	speed: 0.0811s/iter; left time: 663.0120s
Epoch: 1 cost time: 24.639293432235718
Epoch: 1, Steps: 279 | Train Loss: 0.7354047 Vali Loss: 0.7520230 Test Loss: 0.3586251
Validation loss decreased (inf --> 0.752023).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4592824
	speed: 0.3716s/iter; left time: 2969.7525s
	iters: 200, epoch: 2 | loss: 0.4862116
	speed: 0.1036s/iter; left time: 817.9346s
Epoch: 2 cost time: 30.283622980117798
Epoch: 2, Steps: 279 | Train Loss: 0.5133991 Vali Loss: 0.7049611 Test Loss: 0.3439049
Validation loss decreased (0.752023 --> 0.704961).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4348705
	speed: 0.3956s/iter; left time: 3051.5912s
	iters: 200, epoch: 3 | loss: 0.5161009
	speed: 0.0999s/iter; left time: 760.7612s
Epoch: 3 cost time: 29.871017694473267
Epoch: 3, Steps: 279 | Train Loss: 0.4796539 Vali Loss: 0.6957075 Test Loss: 0.3403210
Validation loss decreased (0.704961 --> 0.695707).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5322214
	speed: 0.3830s/iter; left time: 2847.3013s
	iters: 200, epoch: 4 | loss: 0.4511254
	speed: 0.0930s/iter; left time: 682.3211s
Epoch: 4 cost time: 27.763184070587158
Epoch: 4, Steps: 279 | Train Loss: 0.4689599 Vali Loss: 0.6910909 Test Loss: 0.3383780
Validation loss decreased (0.695707 --> 0.691091).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4629035
	speed: 0.3601s/iter; left time: 2576.7959s
	iters: 200, epoch: 5 | loss: 0.4509512
	speed: 0.0893s/iter; left time: 630.2671s
Epoch: 5 cost time: 26.359834909439087
Epoch: 5, Steps: 279 | Train Loss: 0.4646190 Vali Loss: 0.6885714 Test Loss: 0.3372060
Validation loss decreased (0.691091 --> 0.688571).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4551255
	speed: 0.3642s/iter; left time: 2504.2199s
	iters: 200, epoch: 6 | loss: 0.5033116
	speed: 0.0947s/iter; left time: 641.5632s
Epoch: 6 cost time: 27.99385643005371
Epoch: 6, Steps: 279 | Train Loss: 0.4626311 Vali Loss: 0.6875347 Test Loss: 0.3366714
Validation loss decreased (0.688571 --> 0.687535).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5694453
	speed: 0.3657s/iter; left time: 2412.2418s
	iters: 200, epoch: 7 | loss: 0.3813359
	speed: 0.1090s/iter; left time: 708.1433s
Epoch: 7 cost time: 29.789111137390137
Epoch: 7, Steps: 279 | Train Loss: 0.4613298 Vali Loss: 0.6883960 Test Loss: 0.3363756
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4647856
	speed: 0.3960s/iter; left time: 2501.6950s
	iters: 200, epoch: 8 | loss: 0.3968625
	speed: 0.1078s/iter; left time: 670.3213s
Epoch: 8 cost time: 30.57190227508545
Epoch: 8, Steps: 279 | Train Loss: 0.4610299 Vali Loss: 0.6882876 Test Loss: 0.3361973
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5026607
	speed: 0.4056s/iter; left time: 2449.3131s
	iters: 200, epoch: 9 | loss: 0.5071324
	speed: 0.1047s/iter; left time: 621.7362s
Epoch: 9 cost time: 30.92406463623047
Epoch: 9, Steps: 279 | Train Loss: 0.4609668 Vali Loss: 0.6884543 Test Loss: 0.3360355
EarlyStopping counter: 3 out of 3
Early stopping
train 35832
val 4551
test 9820
Model(
  (freq_upsampler): Linear(in_features=46, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17805312.0
params:  6768.0
Trainable parameters:  6768
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6462607
	speed: 0.1205s/iter; left time: 996.3696s
	iters: 200, epoch: 1 | loss: 0.6179925
	speed: 0.1033s/iter; left time: 843.8483s
Epoch: 1 cost time: 31.147223711013794
Epoch: 1, Steps: 279 | Train Loss: 0.6329519 Vali Loss: 0.6855956 Test Loss: 0.3358695
Validation loss decreased (inf --> 0.685596).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6147404
	speed: 0.4010s/iter; left time: 3204.7992s
	iters: 200, epoch: 2 | loss: 0.6730363
	speed: 0.1059s/iter; left time: 835.4272s
Epoch: 2 cost time: 30.28912353515625
Epoch: 2, Steps: 279 | Train Loss: 0.6321108 Vali Loss: 0.6854455 Test Loss: 0.3352878
Validation loss decreased (0.685596 --> 0.685445).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6205794
	speed: 0.4039s/iter; left time: 3115.1677s
	iters: 200, epoch: 3 | loss: 0.6282610
	speed: 0.1024s/iter; left time: 779.6423s
Epoch: 3 cost time: 28.868694305419922
Epoch: 3, Steps: 279 | Train Loss: 0.6318387 Vali Loss: 0.6842377 Test Loss: 0.3352281
Validation loss decreased (0.685445 --> 0.684238).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6382632
	speed: 0.3499s/iter; left time: 2601.4717s
	iters: 200, epoch: 4 | loss: 0.7049702
	speed: 0.0886s/iter; left time: 649.6683s
Epoch: 4 cost time: 26.50141191482544
Epoch: 4, Steps: 279 | Train Loss: 0.6317966 Vali Loss: 0.6846399 Test Loss: 0.3350011
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6853964
	speed: 0.3756s/iter; left time: 2687.6025s
	iters: 200, epoch: 5 | loss: 0.5519758
	speed: 0.0985s/iter; left time: 694.9303s
Epoch: 5 cost time: 28.22156572341919
Epoch: 5, Steps: 279 | Train Loss: 0.6318586 Vali Loss: 0.6834579 Test Loss: 0.3348898
Validation loss decreased (0.684238 --> 0.683458).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6828787
	speed: 0.3644s/iter; left time: 2505.4960s
	iters: 200, epoch: 6 | loss: 0.7137102
	speed: 0.0935s/iter; left time: 633.7351s
Epoch: 6 cost time: 28.600152015686035
Epoch: 6, Steps: 279 | Train Loss: 0.6316150 Vali Loss: 0.6851549 Test Loss: 0.3350589
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6585079
	speed: 0.3798s/iter; left time: 2505.3636s
	iters: 200, epoch: 7 | loss: 0.5350736
	speed: 0.0952s/iter; left time: 618.6651s
Epoch: 7 cost time: 28.50454306602478
Epoch: 7, Steps: 279 | Train Loss: 0.6314130 Vali Loss: 0.6842731 Test Loss: 0.3350212
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6064099
	speed: 0.3787s/iter; left time: 2392.5047s
	iters: 200, epoch: 8 | loss: 0.6775395
	speed: 0.0990s/iter; left time: 615.2925s
Epoch: 8 cost time: 29.062363386154175
Epoch: 8, Steps: 279 | Train Loss: 0.6315678 Vali Loss: 0.6844236 Test Loss: 0.3347497
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : weather_336_720_FITS_custom_ftM_sl336_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.33409982919692993, mae:0.3432622253894806, rse:0.7606242895126343, corr:[0.4793015  0.48099616 0.48021358 0.4789268  0.47808048 0.47767437
 0.47725505 0.47653323 0.47548723 0.4743801  0.47358915 0.47326264
 0.47304046 0.4727015  0.47199637 0.4711407  0.47016814 0.4691479
 0.46839702 0.46779764 0.46720508 0.46653396 0.46581358 0.46497393
 0.4641767  0.46328884 0.46229622 0.46105766 0.4598269  0.45864302
 0.45769313 0.4568687  0.45610586 0.45522445 0.45431578 0.45331618
 0.4524008  0.45156786 0.45084968 0.45005023 0.44915816 0.44836402
 0.44755268 0.44675303 0.445973   0.44521487 0.44448858 0.44385985
 0.44309607 0.44231182 0.44159552 0.44098884 0.44059733 0.4402796
 0.43997648 0.43947378 0.43890953 0.43834296 0.4378801  0.4375569
 0.4373936  0.4372416  0.43698037 0.43664312 0.4362518  0.43595833
 0.43554607 0.4352465  0.43494156 0.43463808 0.43436065 0.43412745
 0.4338549  0.43356323 0.43322244 0.43289936 0.43248343 0.43210334
 0.43179104 0.43155402 0.43148947 0.4315244  0.43157485 0.4315473
 0.43152007 0.43135875 0.43131718 0.4312166  0.4311751  0.43115896
 0.43115172 0.43102348 0.43084288 0.43070754 0.43060446 0.4304588
 0.4302935  0.43013164 0.42995733 0.42975348 0.4295603  0.42939615
 0.42926943 0.42915648 0.42899188 0.42875051 0.42842504 0.428091
 0.42772055 0.42738903 0.4270644  0.42677397 0.4264668  0.42617738
 0.42589858 0.42562464 0.42535195 0.42507464 0.42474452 0.4243831
 0.42400312 0.42366737 0.4233416  0.4230253  0.42274737 0.42245674
 0.42217705 0.42184046 0.42145187 0.42101672 0.4205565  0.4200832
 0.41964576 0.41924247 0.41890538 0.41859165 0.41826656 0.41794112
 0.41759318 0.41723454 0.41688323 0.41649586 0.4161365  0.41579315
 0.41544166 0.41510338 0.41476452 0.41428655 0.41364592 0.4128117
 0.41197735 0.41125312 0.41074336 0.41042206 0.4100368  0.40949553
 0.40888944 0.40818354 0.40742007 0.40657154 0.40596166 0.40548223
 0.40508524 0.40469378 0.40421063 0.40370587 0.403106   0.4024351
 0.40181234 0.40113682 0.400436   0.3997441  0.3989223  0.3982525
 0.39749008 0.3968136  0.39612117 0.39557514 0.39513835 0.39466876
 0.39416265 0.39359576 0.39299187 0.3923351  0.39164177 0.39106536
 0.3906276  0.39019373 0.38986865 0.38963786 0.3894494  0.3892258
 0.38906226 0.38885123 0.38858452 0.38835213 0.3880889  0.38793856
 0.38783526 0.38771793 0.38750964 0.38731438 0.38704863 0.3867443
 0.38656116 0.38643122 0.38637996 0.38624102 0.38603413 0.38575682
 0.38548502 0.38520515 0.38496295 0.38474616 0.38455778 0.3843575
 0.3841696  0.3839922  0.38373148 0.38346782 0.38320082 0.38307297
 0.3830263  0.3830593  0.38306525 0.38290453 0.38283035 0.38278887
 0.38273627 0.38265327 0.38250983 0.38234964 0.3821467  0.3819266
 0.38167822 0.3813684  0.38099983 0.38077658 0.38051718 0.3802851
 0.38008842 0.37994513 0.37980643 0.37966597 0.37955508 0.37939888
 0.37923393 0.3790323  0.3787713  0.37849957 0.3782127  0.3778878
 0.3775702  0.37723967 0.376966   0.3767225  0.3764621  0.37613985
 0.3757718  0.37541607 0.37505874 0.3747416  0.37451443 0.37431163
 0.3741013  0.37383902 0.3735096  0.37313882 0.3727428  0.37232828
 0.37193197 0.37153232 0.37117565 0.37078723 0.3703875  0.3700175
 0.3696818  0.3693172  0.36894217 0.36854336 0.36805084 0.367501
 0.36690235 0.3662764  0.36569557 0.36519024 0.3646731  0.36408293
 0.3634467  0.36281836 0.3621656  0.36153236 0.36090177 0.36039153
 0.35982317 0.3592182  0.35854778 0.3579015  0.35724476 0.35665125
 0.3559048  0.3550452  0.354008   0.3529606  0.35200286 0.35117102
 0.35054854 0.35000113 0.3495913  0.34917176 0.34862357 0.34789446
 0.34709448 0.3464022  0.3457949  0.34533623 0.34489453 0.34453392
 0.34417456 0.34380257 0.34329075 0.3427078  0.34208414 0.34132537
 0.34043366 0.33953872 0.3387873  0.33828998 0.33791274 0.3375597
 0.3372292  0.33683887 0.33648115 0.3363062  0.33616066 0.33619705
 0.33611542 0.3359002  0.33573845 0.33544505 0.33520222 0.33500835
 0.33495373 0.3350224  0.33515963 0.33529025 0.3352675  0.33512175
 0.3348714  0.33450317 0.33409175 0.33353376 0.333086   0.33266577
 0.33231747 0.33215496 0.33211768 0.33216545 0.33222523 0.3323675
 0.33232048 0.3322317  0.33216715 0.33208698 0.33199444 0.33199275
 0.3319844  0.3319299  0.3319123  0.3318003  0.3317104  0.33170322
 0.33179304 0.33191448 0.3319805  0.3319344  0.33196625 0.3319335
 0.33195147 0.33197024 0.33202448 0.33206695 0.3321236  0.33201715
 0.33184022 0.33165857 0.33151639 0.3314445  0.33137432 0.33124125
 0.331017   0.33072847 0.33047566 0.33027813 0.33014795 0.33006
 0.3299034  0.3296634  0.3294465  0.3292113  0.32910648 0.32907993
 0.32911983 0.32906348 0.32890385 0.32862356 0.32829407 0.32797003
 0.32773212 0.32756567 0.32739586 0.32711956 0.3267748  0.32637444
 0.3260102  0.32580853 0.325802   0.32591859 0.32597575 0.32587117
 0.3255586  0.32512736 0.32460743 0.32419893 0.32389167 0.3236374
 0.32332918 0.3228988  0.32225567 0.32138413 0.32047626 0.31978777
 0.31931517 0.31902775 0.31887817 0.31868723 0.3183183  0.3177336
 0.31700605 0.31620562 0.3154199  0.3147007  0.31397653 0.31318563
 0.3123498  0.3115167  0.31062153 0.30982882 0.30903897 0.30839977
 0.30781394 0.3072403  0.30663812 0.30602622 0.30544823 0.30478442
 0.3042338  0.30373192 0.30318266 0.30277622 0.30229434 0.30192417
 0.30148324 0.30104113 0.3007004  0.30031925 0.29993862 0.29952595
 0.2991132  0.2986666  0.29809552 0.29746976 0.297069   0.29680026
 0.29679042 0.29699707 0.2972285  0.2974452  0.29757974 0.2976306
 0.29764637 0.2976967  0.29755154 0.29734126 0.29702443 0.29678994
 0.29653537 0.29630688 0.2962004  0.29622817 0.2962522  0.2960891
 0.29577664 0.29537585 0.29495683 0.29469854 0.29451188 0.2944508
 0.29450506 0.29447737 0.29435158 0.2942283  0.29416022 0.29411593
 0.29424435 0.29427546 0.29422262 0.294001   0.29376024 0.29344442
 0.29319632 0.29311192 0.29325745 0.29338396 0.29336864 0.29326412
 0.29301095 0.29272467 0.29243317 0.2922076  0.29198208 0.29168183
 0.29130653 0.29094148 0.29055825 0.2903367  0.29019544 0.29014677
 0.2901652  0.29020625 0.29005137 0.28975642 0.28930202 0.28875962
 0.28819758 0.2876584  0.28717116 0.2867615  0.2864195  0.28612068
 0.28585744 0.28561956 0.28538817 0.2851518  0.2849076  0.28466988
 0.2844642  0.28431574 0.28422353 0.28414395 0.2840694  0.28399396
 0.28391495 0.28381026 0.2836737  0.28348398 0.28326973 0.28299746
 0.28266168 0.28233182 0.2820408  0.28182635 0.28155378 0.28121033
 0.28076956 0.28020018 0.2795972  0.27897844 0.27848965 0.27816856
 0.27796578 0.2777764  0.27752432 0.2770895  0.27661306 0.27609107
 0.2755738  0.27504355 0.27448756 0.27385843 0.27314726 0.2724094
 0.27172494 0.27120355 0.27076587 0.27029017 0.26965755 0.2689054
 0.2680684  0.2673547  0.26682827 0.26661012 0.26648015 0.26641485
 0.26615113 0.26561317 0.26494938 0.2644577  0.26415005 0.26398468
 0.2638958  0.26372018 0.2632971  0.2624918  0.26144022 0.26047269
 0.25960088 0.25893557 0.2585125  0.25809947 0.25762278 0.2570607
 0.25651956 0.2560379  0.2556876  0.25510025 0.25458065 0.25396115
 0.25337502 0.25310242 0.2529799  0.2530329  0.25317782 0.25338492
 0.25344232 0.25323603 0.2529414  0.25250414 0.25232303 0.25239474
 0.25252736 0.25288922 0.25314003 0.25325674 0.25329915 0.25326276
 0.25316593 0.2529611  0.25288412 0.2526531  0.2526392  0.25244913
 0.25242007 0.25246096 0.25221482 0.25204346 0.2519491  0.25177667
 0.25181442 0.25176853 0.2517702  0.25178936 0.25179186 0.2517461
 0.25171146 0.25166115 0.2517099  0.2517307  0.25173363 0.25176552
 0.25173885 0.25175953 0.2518323  0.25191122 0.25198996 0.25196937
 0.25194326 0.25186133 0.2519094  0.25205746 0.25240162 0.2527662
 0.25310236 0.25329855 0.25331298 0.253199   0.2530684  0.25300762
 0.25303784 0.25315034 0.25323462 0.25322765 0.25311297 0.25296277
 0.25279042 0.2526638  0.2525978  0.25250024 0.2523251  0.25208622
 0.25181863 0.25162745 0.25158334 0.25163823 0.25170937 0.25167564
 0.2515423  0.2513326  0.25115392 0.2510588  0.25107515 0.25110996
 0.25110394 0.25101328 0.2508962  0.2509179  0.25108165 0.2512869
 0.2514031  0.25131541 0.25106686 0.2509403  0.2510292  0.2513371
 0.2515359  0.25135657 0.250526   0.24950661 0.24907482 0.25048473]
