Args in experiment:
Namespace(is_training=1, model_id='ECL_96_336', model='iTransformer', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=336, enc_in=321, dec_in=321, c_out=321, d_model=512, n_heads=8, e_layers=3, d_layers=1, d_ff=512, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=64, patience=3, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=2, use_multi_gpu=False, devices='0,1,2,3', exp_name='MTSF', channel_independence=False, inverse=False, class_strategy='projection', target_root_path='./data/electricity/', target_data_path='electricity.csv', efficient_training=False, use_norm=True, partial_start_index=0)
Use GPU: cuda:2
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.
MACs: 1619994624.0
Params: 4957008.0
1.62G MACs
>>>>>>>start training : ECL_96_336_iTransformer_custom_M_ft96_sl48_ll336_pl512_dm8_nh3_el1_dl512_df1_fctimeF_ebTrue_dtExp_projection_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17981
val 2297
test 4925
	iters: 100, epoch: 1 | loss: 0.2762837
	speed: 0.1403s/iter; left time: 378.9640s
	iters: 200, epoch: 1 | loss: 0.2704843
	speed: 0.1226s/iter; left time: 318.8566s
Max Memory (MB): 3122.0458984375
Epoch: 1 cost time: 36.19741415977478
Epoch: 1, Steps: 280 | Train Loss: 0.2990978 Vali Loss: 0.2018997 Test Loss: 0.2244308
Validation loss decreased (inf --> 0.201900).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2342868
	speed: 1.1794s/iter; left time: 2855.3589s
	iters: 200, epoch: 2 | loss: 0.2335142
	speed: 0.1193s/iter; left time: 276.8249s
Max Memory (MB): 3122.0458984375
Epoch: 2 cost time: 35.73983836174011
Epoch: 2, Steps: 280 | Train Loss: 0.2339638 Vali Loss: 0.1867776 Test Loss: 0.2099369
Validation loss decreased (0.201900 --> 0.186778).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2615952
	speed: 1.1246s/iter; left time: 2407.7093s
	iters: 200, epoch: 3 | loss: 0.2293941
	speed: 0.1205s/iter; left time: 245.8804s
Max Memory (MB): 3122.0458984375
Epoch: 3 cost time: 35.74161243438721
Epoch: 3, Steps: 280 | Train Loss: 0.2234455 Vali Loss: 0.1830758 Test Loss: 0.2055673
Validation loss decreased (0.186778 --> 0.183076).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.2233562
	speed: 1.3597s/iter; left time: 2530.3239s
	iters: 200, epoch: 4 | loss: 0.2342124
	speed: 0.1202s/iter; left time: 211.6182s
Max Memory (MB): 3122.0458984375
Epoch: 4 cost time: 35.65561890602112
Epoch: 4, Steps: 280 | Train Loss: 0.2194107 Vali Loss: 0.1808471 Test Loss: 0.2026148
Validation loss decreased (0.183076 --> 0.180847).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.2166728
	speed: 1.1784s/iter; left time: 1863.0069s
	iters: 200, epoch: 5 | loss: 0.2141393
	speed: 0.1182s/iter; left time: 174.9945s
Max Memory (MB): 3122.0458984375
Epoch: 5 cost time: 35.09023451805115
Epoch: 5, Steps: 280 | Train Loss: 0.2174370 Vali Loss: 0.1799872 Test Loss: 0.2016225
Validation loss decreased (0.180847 --> 0.179987).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.2161704
	speed: 1.1236s/iter; left time: 1461.8654s
	iters: 200, epoch: 6 | loss: 0.1977472
	speed: 0.1171s/iter; left time: 140.5780s
Max Memory (MB): 3122.0458984375
Epoch: 6 cost time: 34.63860893249512
Epoch: 6, Steps: 280 | Train Loss: 0.2165096 Vali Loss: 0.1788128 Test Loss: 0.2010913
Validation loss decreased (0.179987 --> 0.178813).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.2176130
	speed: 1.1730s/iter; left time: 1197.6417s
	iters: 200, epoch: 7 | loss: 0.2099095
	speed: 0.1232s/iter; left time: 113.4777s
Max Memory (MB): 3122.0458984375
Epoch: 7 cost time: 36.21301507949829
Epoch: 7, Steps: 280 | Train Loss: 0.2159512 Vali Loss: 0.1788222 Test Loss: 0.2006267
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.2001355
	speed: 1.1614s/iter; left time: 860.6325s
	iters: 200, epoch: 8 | loss: 0.2255822
	speed: 0.1203s/iter; left time: 77.1410s
Max Memory (MB): 3122.0458984375
Epoch: 8 cost time: 35.58632946014404
Epoch: 8, Steps: 280 | Train Loss: 0.2157858 Vali Loss: 0.1788194 Test Loss: 0.2004275
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.2193080
	speed: 1.3119s/iter; left time: 604.7684s
	iters: 200, epoch: 9 | loss: 0.2256904
	speed: 0.1195s/iter; left time: 43.1387s
Max Memory (MB): 3122.0458984375
Epoch: 9 cost time: 35.40836048126221
Epoch: 9, Steps: 280 | Train Loss: 0.2155796 Vali Loss: 0.1784207 Test Loss: 0.2003744
Validation loss decreased (0.178813 --> 0.178421).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.1987666
	speed: 1.2723s/iter; left time: 230.2808s
	iters: 200, epoch: 10 | loss: 0.2216566
	speed: 0.1203s/iter; left time: 9.7479s
Max Memory (MB): 3122.0458984375
Epoch: 10 cost time: 36.22699522972107
Epoch: 10, Steps: 280 | Train Loss: 0.2154916 Vali Loss: 0.1785523 Test Loss: 0.2003135
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : ECL_96_336_iTransformer_custom_M_ft96_sl48_ll336_pl512_dm8_nh3_el1_dl512_df1_fctimeF_ebTrue_dtExp_projection_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
test shape: (4925, 1, 336, 321) (4925, 1, 336, 321)
test shape: (4925, 336, 321) (4925, 336, 321)
mse:0.20037512481212616, mae:0.289277046918869
