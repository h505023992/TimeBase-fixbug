Args in experiment:
Namespace(is_training=1, model_id='ETTh2_192_720', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=64, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_192_720_FITS_ETTh2_ftM_sl192_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=64, out_features=304, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17432576.0
params:  19760.0
Trainable parameters:  19760
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.4766454696655273
Epoch: 1, Steps: 60 | Train Loss: 1.0614444 Vali Loss: 0.8069612 Test Loss: 0.5877673
Validation loss decreased (inf --> 0.806961).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.9873390197753906
Epoch: 2, Steps: 60 | Train Loss: 0.8714638 Vali Loss: 0.7376466 Test Loss: 0.5138021
Validation loss decreased (0.806961 --> 0.737647).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.916504144668579
Epoch: 3, Steps: 60 | Train Loss: 0.7767788 Vali Loss: 0.7035310 Test Loss: 0.4743663
Validation loss decreased (0.737647 --> 0.703531).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.947141408920288
Epoch: 4, Steps: 60 | Train Loss: 0.7288628 Vali Loss: 0.6809646 Test Loss: 0.4524867
Validation loss decreased (0.703531 --> 0.680965).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.9640603065490723
Epoch: 5, Steps: 60 | Train Loss: 0.7012398 Vali Loss: 0.6686852 Test Loss: 0.4403938
Validation loss decreased (0.680965 --> 0.668685).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.3484416007995605
Epoch: 6, Steps: 60 | Train Loss: 0.6861987 Vali Loss: 0.6639075 Test Loss: 0.4333129
Validation loss decreased (0.668685 --> 0.663908).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.2750980854034424
Epoch: 7, Steps: 60 | Train Loss: 0.6788941 Vali Loss: 0.6545110 Test Loss: 0.4288639
Validation loss decreased (0.663908 --> 0.654511).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.5762135982513428
Epoch: 8, Steps: 60 | Train Loss: 0.6725212 Vali Loss: 0.6567228 Test Loss: 0.4258188
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.278569459915161
Epoch: 9, Steps: 60 | Train Loss: 0.6684730 Vali Loss: 0.6525997 Test Loss: 0.4236667
Validation loss decreased (0.654511 --> 0.652600).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.161280870437622
Epoch: 10, Steps: 60 | Train Loss: 0.6671529 Vali Loss: 0.6477952 Test Loss: 0.4219697
Validation loss decreased (0.652600 --> 0.647795).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.1102426052093506
Epoch: 11, Steps: 60 | Train Loss: 0.6637574 Vali Loss: 0.6465714 Test Loss: 0.4205873
Validation loss decreased (0.647795 --> 0.646571).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.802385091781616
Epoch: 12, Steps: 60 | Train Loss: 0.6621012 Vali Loss: 0.6480676 Test Loss: 0.4194455
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.382315158843994
Epoch: 13, Steps: 60 | Train Loss: 0.6613057 Vali Loss: 0.6477245 Test Loss: 0.4184654
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.466078281402588
Epoch: 14, Steps: 60 | Train Loss: 0.6577978 Vali Loss: 0.6432405 Test Loss: 0.4176261
Validation loss decreased (0.646571 --> 0.643240).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.415695905685425
Epoch: 15, Steps: 60 | Train Loss: 0.6573237 Vali Loss: 0.6465863 Test Loss: 0.4168243
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.4056944847106934
Epoch: 16, Steps: 60 | Train Loss: 0.6575313 Vali Loss: 0.6401606 Test Loss: 0.4161344
Validation loss decreased (0.643240 --> 0.640161).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.3919081687927246
Epoch: 17, Steps: 60 | Train Loss: 0.6567884 Vali Loss: 0.6410450 Test Loss: 0.4154909
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.084449052810669
Epoch: 18, Steps: 60 | Train Loss: 0.6559317 Vali Loss: 0.6408643 Test Loss: 0.4149415
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.3183469772338867
Epoch: 19, Steps: 60 | Train Loss: 0.6552311 Vali Loss: 0.6403370 Test Loss: 0.4144369
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.2003414630889893
Epoch: 20, Steps: 60 | Train Loss: 0.6546754 Vali Loss: 0.6412594 Test Loss: 0.4139702
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.440084934234619
Epoch: 21, Steps: 60 | Train Loss: 0.6532756 Vali Loss: 0.6367993 Test Loss: 0.4135200
Validation loss decreased (0.640161 --> 0.636799).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.2064571380615234
Epoch: 22, Steps: 60 | Train Loss: 0.6540518 Vali Loss: 0.6405454 Test Loss: 0.4131272
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.2908012866973877
Epoch: 23, Steps: 60 | Train Loss: 0.6533791 Vali Loss: 0.6372614 Test Loss: 0.4127710
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.9772918224334717
Epoch: 24, Steps: 60 | Train Loss: 0.6513103 Vali Loss: 0.6358947 Test Loss: 0.4124230
Validation loss decreased (0.636799 --> 0.635895).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.5709736347198486
Epoch: 25, Steps: 60 | Train Loss: 0.6512327 Vali Loss: 0.6339960 Test Loss: 0.4121096
Validation loss decreased (0.635895 --> 0.633996).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.367265224456787
Epoch: 26, Steps: 60 | Train Loss: 0.6513742 Vali Loss: 0.6347817 Test Loss: 0.4118164
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.4755284786224365
Epoch: 27, Steps: 60 | Train Loss: 0.6526293 Vali Loss: 0.6353215 Test Loss: 0.4115365
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.4779555797576904
Epoch: 28, Steps: 60 | Train Loss: 0.6509079 Vali Loss: 0.6357800 Test Loss: 0.4112914
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.4412317276000977
Epoch: 29, Steps: 60 | Train Loss: 0.6503588 Vali Loss: 0.6329418 Test Loss: 0.4110450
Validation loss decreased (0.633996 --> 0.632942).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.0193252563476562
Epoch: 30, Steps: 60 | Train Loss: 0.6511497 Vali Loss: 0.6343298 Test Loss: 0.4108223
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011296777049628277
train 7729
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=64, out_features=304, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17432576.0
params:  19760.0
Trainable parameters:  19760
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.6351568698883057
Epoch: 1, Steps: 60 | Train Loss: 0.8177775 Vali Loss: 0.6339887 Test Loss: 0.4082170
Validation loss decreased (inf --> 0.633989).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.2203516960144043
Epoch: 2, Steps: 60 | Train Loss: 0.8143519 Vali Loss: 0.6305225 Test Loss: 0.4064643
Validation loss decreased (0.633989 --> 0.630522).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.320744276046753
Epoch: 3, Steps: 60 | Train Loss: 0.8100516 Vali Loss: 0.6301188 Test Loss: 0.4052982
Validation loss decreased (0.630522 --> 0.630119).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.21420955657959
Epoch: 4, Steps: 60 | Train Loss: 0.8102453 Vali Loss: 0.6317418 Test Loss: 0.4046466
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.24733567237854
Epoch: 5, Steps: 60 | Train Loss: 0.8098847 Vali Loss: 0.6290419 Test Loss: 0.4040879
Validation loss decreased (0.630119 --> 0.629042).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.043431758880615
Epoch: 6, Steps: 60 | Train Loss: 0.8092916 Vali Loss: 0.6290683 Test Loss: 0.4037754
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.4326939582824707
Epoch: 7, Steps: 60 | Train Loss: 0.8096271 Vali Loss: 0.6255708 Test Loss: 0.4035660
Validation loss decreased (0.629042 --> 0.625571).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.5778229236602783
Epoch: 8, Steps: 60 | Train Loss: 0.8075498 Vali Loss: 0.6261345 Test Loss: 0.4034685
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.591956377029419
Epoch: 9, Steps: 60 | Train Loss: 0.8074905 Vali Loss: 0.6270794 Test Loss: 0.4033227
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.6907434463500977
Epoch: 10, Steps: 60 | Train Loss: 0.8065153 Vali Loss: 0.6265274 Test Loss: 0.4032620
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.093613862991333
Epoch: 11, Steps: 60 | Train Loss: 0.8065943 Vali Loss: 0.6264127 Test Loss: 0.4032095
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.0825603008270264
Epoch: 12, Steps: 60 | Train Loss: 0.8079091 Vali Loss: 0.6229854 Test Loss: 0.4031017
Validation loss decreased (0.625571 --> 0.622985).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.3232944011688232
Epoch: 13, Steps: 60 | Train Loss: 0.8072563 Vali Loss: 0.6250204 Test Loss: 0.4031043
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.423006534576416
Epoch: 14, Steps: 60 | Train Loss: 0.8069934 Vali Loss: 0.6228700 Test Loss: 0.4030654
Validation loss decreased (0.622985 --> 0.622870).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.3480780124664307
Epoch: 15, Steps: 60 | Train Loss: 0.8076317 Vali Loss: 0.6257446 Test Loss: 0.4030297
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.4191994667053223
Epoch: 16, Steps: 60 | Train Loss: 0.8074417 Vali Loss: 0.6281722 Test Loss: 0.4029815
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.788327693939209
Epoch: 17, Steps: 60 | Train Loss: 0.8074575 Vali Loss: 0.6273383 Test Loss: 0.4029724
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.556941509246826
Epoch: 18, Steps: 60 | Train Loss: 0.8053108 Vali Loss: 0.6263846 Test Loss: 0.4029716
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.4660863876342773
Epoch: 19, Steps: 60 | Train Loss: 0.8074776 Vali Loss: 0.6247994 Test Loss: 0.4029368
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh2_192_720_FITS_ETTh2_ftM_sl192_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4012971520423889, mae:0.42899832129478455, rse:0.506335973739624, corr:[ 2.21839711e-01  2.21788689e-01  2.20560238e-01  2.20837519e-01
  2.18643457e-01  2.17906177e-01  2.17490494e-01  2.16213435e-01
  2.15724543e-01  2.14536414e-01  2.13112324e-01  2.12217703e-01
  2.10943162e-01  2.09670559e-01  2.08981827e-01  2.08531201e-01
  2.07966566e-01  2.07312062e-01  2.06352815e-01  2.05492079e-01
  2.04608664e-01  2.03887880e-01  2.03168631e-01  2.01800495e-01
  1.99731588e-01  1.98184207e-01  1.97088107e-01  1.95885420e-01
  1.95020631e-01  1.94301873e-01  1.93496048e-01  1.92325398e-01
  1.90847337e-01  1.89632222e-01  1.88731283e-01  1.87579364e-01
  1.86247215e-01  1.85469568e-01  1.84792623e-01  1.83805808e-01
  1.82909757e-01  1.82264566e-01  1.81737646e-01  1.80957451e-01
  1.79972097e-01  1.79121494e-01  1.78228647e-01  1.76266328e-01
  1.73863158e-01  1.72233462e-01  1.70749903e-01  1.69284269e-01
  1.68605819e-01  1.67984620e-01  1.66633070e-01  1.65774435e-01
  1.65578127e-01  1.64758131e-01  1.63893864e-01  1.63986415e-01
  1.63804069e-01  1.63042426e-01  1.63071871e-01  1.63199440e-01
  1.63137376e-01  1.62938803e-01  1.62595332e-01  1.62426099e-01
  1.62881956e-01  1.62911311e-01  1.62628129e-01  1.62314042e-01
  1.61455438e-01  1.60855442e-01  1.61099404e-01  1.60746574e-01
  1.59950659e-01  1.59877017e-01  1.59811944e-01  1.59152463e-01
  1.59023687e-01  1.59168303e-01  1.58995152e-01  1.59050658e-01
  1.58983335e-01  1.58781901e-01  1.59090593e-01  1.59262165e-01
  1.58931509e-01  1.58859879e-01  1.58978820e-01  1.58834651e-01
  1.59153804e-01  1.59597501e-01  1.59388199e-01  1.58806011e-01
  1.58567041e-01  1.58397153e-01  1.58019885e-01  1.57873169e-01
  1.57903880e-01  1.57531992e-01  1.57199249e-01  1.56937078e-01
  1.57035381e-01  1.57183513e-01  1.57341376e-01  1.57466456e-01
  1.57093480e-01  1.56517044e-01  1.56302318e-01  1.56394795e-01
  1.55901864e-01  1.55415803e-01  1.55602664e-01  1.55360207e-01
  1.54888213e-01  1.54611304e-01  1.54014647e-01  1.52911633e-01
  1.51568577e-01  1.50456935e-01  1.49944171e-01  1.49964124e-01
  1.49269879e-01  1.48065016e-01  1.47538587e-01  1.47321463e-01
  1.46863312e-01  1.46186411e-01  1.45748675e-01  1.45275846e-01
  1.44577831e-01  1.43809304e-01  1.43347383e-01  1.43315077e-01
  1.42738968e-01  1.41894117e-01  1.41393423e-01  1.40590042e-01
  1.40103802e-01  1.40498027e-01  1.40527204e-01  1.39474779e-01
  1.37714133e-01  1.36381730e-01  1.35501608e-01  1.35185063e-01
  1.34730265e-01  1.34117261e-01  1.33853123e-01  1.33345321e-01
  1.32695541e-01  1.32587641e-01  1.32526383e-01  1.31917179e-01
  1.31267503e-01  1.31074622e-01  1.30801573e-01  1.30664572e-01
  1.30298913e-01  1.29450008e-01  1.29279524e-01  1.29702598e-01
  1.29945472e-01  1.29772902e-01  1.29656181e-01  1.28987685e-01
  1.27888665e-01  1.27204508e-01  1.27004340e-01  1.27078906e-01
  1.26515955e-01  1.25481501e-01  1.25276014e-01  1.25009164e-01
  1.24073789e-01  1.23316139e-01  1.23488463e-01  1.23257071e-01
  1.22652337e-01  1.22501157e-01  1.22730151e-01  1.22757368e-01
  1.22434661e-01  1.22162275e-01  1.22388773e-01  1.22602597e-01
  1.22725748e-01  1.22949377e-01  1.23578519e-01  1.23868778e-01
  1.23746887e-01  1.23819865e-01  1.24174975e-01  1.24268338e-01
  1.24044865e-01  1.23999491e-01  1.24166474e-01  1.24169633e-01
  1.23916626e-01  1.23802990e-01  1.23809822e-01  1.23385742e-01
  1.23049460e-01  1.23354539e-01  1.24094896e-01  1.24410257e-01
  1.24386221e-01  1.24446303e-01  1.24680348e-01  1.25016525e-01
  1.25101596e-01  1.24863409e-01  1.25082761e-01  1.25242054e-01
  1.24738358e-01  1.24254234e-01  1.24163590e-01  1.24181658e-01
  1.23587199e-01  1.23001009e-01  1.23004280e-01  1.23583727e-01
  1.24001719e-01  1.23897217e-01  1.23947218e-01  1.24268971e-01
  1.24468438e-01  1.24437727e-01  1.24716938e-01  1.25562623e-01
  1.26273319e-01  1.26428127e-01  1.26495123e-01  1.26995668e-01
  1.27524555e-01  1.27980217e-01  1.28443331e-01  1.28805816e-01
  1.29124194e-01  1.29147992e-01  1.28828406e-01  1.28888085e-01
  1.29215494e-01  1.28979057e-01  1.28672093e-01  1.29055917e-01
  1.29797325e-01  1.30169317e-01  1.30793557e-01  1.31249025e-01
  1.31507531e-01  1.32084206e-01  1.32818267e-01  1.33715793e-01
  1.34179831e-01  1.34449720e-01  1.34932846e-01  1.35827288e-01
  1.36703387e-01  1.37339100e-01  1.38025746e-01  1.38546839e-01
  1.38880685e-01  1.39239043e-01  1.39368787e-01  1.39516398e-01
  1.40109330e-01  1.41017228e-01  1.41176626e-01  1.41357020e-01
  1.42090201e-01  1.42686114e-01  1.43140927e-01  1.43823683e-01
  1.44173056e-01  1.44347966e-01  1.44873962e-01  1.45459399e-01
  1.45742595e-01  1.46337435e-01  1.47118121e-01  1.47267193e-01
  1.48034543e-01  1.49574131e-01  1.49943471e-01  1.49316028e-01
  1.50281966e-01  1.51632100e-01  1.51526496e-01  1.51149035e-01
  1.51411861e-01  1.51087552e-01  1.50639325e-01  1.51312351e-01
  1.51888162e-01  1.52176648e-01  1.53206795e-01  1.53745353e-01
  1.53223261e-01  1.53371066e-01  1.54612631e-01  1.54939279e-01
  1.54219836e-01  1.54439390e-01  1.55436710e-01  1.56259701e-01
  1.56247333e-01  1.56379968e-01  1.57120898e-01  1.57543421e-01
  1.57899916e-01  1.58639565e-01  1.58954576e-01  1.58389360e-01
  1.57893598e-01  1.57902271e-01  1.58075914e-01  1.58063039e-01
  1.57927901e-01  1.58146665e-01  1.58680215e-01  1.58631727e-01
  1.58181742e-01  1.58617809e-01  1.59664154e-01  1.60206273e-01
  1.60211802e-01  1.60351604e-01  1.60930783e-01  1.61666676e-01
  1.62277937e-01  1.63008660e-01  1.63676739e-01  1.63535893e-01
  1.63281769e-01  1.63972110e-01  1.65045336e-01  1.65302470e-01
  1.64855078e-01  1.64715558e-01  1.65461764e-01  1.65735275e-01
  1.64788544e-01  1.64335042e-01  1.65029407e-01  1.65501386e-01
  1.65700257e-01  1.66075334e-01  1.65984586e-01  1.65540189e-01
  1.65752947e-01  1.66116953e-01  1.66695759e-01  1.67887300e-01
  1.68620318e-01  1.69001400e-01  1.69870332e-01  1.70546398e-01
  1.70466170e-01  1.70786068e-01  1.71823904e-01  1.71959251e-01
  1.71416491e-01  1.71291724e-01  1.71943903e-01  1.72985554e-01
  1.73425123e-01  1.73612624e-01  1.73865557e-01  1.74404994e-01
  1.75157785e-01  1.75806865e-01  1.75954238e-01  1.76069975e-01
  1.76439479e-01  1.76516011e-01  1.76501274e-01  1.77144274e-01
  1.77542791e-01  1.77432254e-01  1.77901402e-01  1.78739354e-01
  1.78988546e-01  1.78955689e-01  1.79132864e-01  1.79040089e-01
  1.79074362e-01  1.79519013e-01  1.79948390e-01  1.80440217e-01
  1.80857256e-01  1.80882245e-01  1.81016028e-01  1.81241348e-01
  1.81264505e-01  1.81097746e-01  1.81047752e-01  1.81094065e-01
  1.81079224e-01  1.80869341e-01  1.80286855e-01  1.79989710e-01
  1.80222392e-01  1.80343762e-01  1.80237994e-01  1.80115029e-01
  1.79869041e-01  1.79835215e-01  1.79893762e-01  1.79609805e-01
  1.79277584e-01  1.79158956e-01  1.79016203e-01  1.78930476e-01
  1.79022327e-01  1.78774461e-01  1.78295419e-01  1.78011760e-01
  1.77671701e-01  1.76969245e-01  1.76387817e-01  1.76140025e-01
  1.75731793e-01  1.75085261e-01  1.74435571e-01  1.73718005e-01
  1.73334002e-01  1.73266128e-01  1.73091948e-01  1.72530234e-01
  1.71607390e-01  1.70917332e-01  1.70800164e-01  1.70564622e-01
  1.69961378e-01  1.69492751e-01  1.69229388e-01  1.68702677e-01
  1.67954445e-01  1.67620882e-01  1.67351052e-01  1.66832075e-01
  1.66307420e-01  1.65646449e-01  1.65103570e-01  1.65215179e-01
  1.65283889e-01  1.64856434e-01  1.64401025e-01  1.63949907e-01
  1.63251758e-01  1.63141415e-01  1.63515985e-01  1.63439125e-01
  1.62779331e-01  1.61883280e-01  1.61095247e-01  1.60485163e-01
  1.59742415e-01  1.59095272e-01  1.59009710e-01  1.59105659e-01
  1.58609793e-01  1.58173800e-01  1.58195779e-01  1.57641530e-01
  1.56544343e-01  1.56004161e-01  1.56105116e-01  1.55455351e-01
  1.54363394e-01  1.54075861e-01  1.54136032e-01  1.53507680e-01
  1.52832538e-01  1.52817085e-01  1.52595013e-01  1.51769578e-01
  1.50661185e-01  1.49389327e-01  1.48365468e-01  1.47747308e-01
  1.46947071e-01  1.46120667e-01  1.45840913e-01  1.45319670e-01
  1.44032314e-01  1.42751992e-01  1.42104879e-01  1.41611844e-01
  1.40793651e-01  1.39869273e-01  1.39391616e-01  1.39103234e-01
  1.38492256e-01  1.38021782e-01  1.37778923e-01  1.36804864e-01
  1.35844365e-01  1.35940477e-01  1.35629728e-01  1.34102404e-01
  1.32635623e-01  1.31795138e-01  1.30937427e-01  1.30273834e-01
  1.29912764e-01  1.29540205e-01  1.29246444e-01  1.28858507e-01
  1.27933934e-01  1.27187654e-01  1.26746163e-01  1.25991002e-01
  1.25324488e-01  1.24501929e-01  1.23279028e-01  1.22316070e-01
  1.21676311e-01  1.20467424e-01  1.18889168e-01  1.18253335e-01
  1.18274279e-01  1.17806301e-01  1.16232090e-01  1.14269465e-01
  1.12370387e-01  1.10436998e-01  1.08799204e-01  1.07912421e-01
  1.07116818e-01  1.05555780e-01  1.04030751e-01  1.03193946e-01
  1.01988494e-01  1.00268953e-01  9.95491594e-02  9.89568233e-02
  9.70101878e-02  9.51662138e-02  9.45027620e-02  9.34198052e-02
  9.13850367e-02  8.99397507e-02  8.92541930e-02  8.87808278e-02
  8.83968845e-02  8.82293507e-02  8.75848085e-02  8.56130794e-02
  8.27144980e-02  8.01919252e-02  7.89430588e-02  7.82566518e-02
  7.71892890e-02  7.64082223e-02  7.61595741e-02  7.51033872e-02
  7.36959577e-02  7.32996464e-02  7.37671629e-02  7.32114911e-02
  7.15915114e-02  7.07307383e-02  7.07892999e-02  6.99313655e-02
  6.85991570e-02  6.82909116e-02  6.87624663e-02  6.85720891e-02
  6.75180703e-02  6.61616847e-02  6.44502565e-02  6.24271147e-02
  6.04835153e-02  5.87535091e-02  5.71987554e-02  5.60042076e-02
  5.50767928e-02  5.42812571e-02  5.34637868e-02  5.24120182e-02
  5.11141159e-02  4.97709177e-02  4.90498394e-02  4.92990538e-02
  4.88904975e-02  4.72021364e-02  4.58561406e-02  4.53621037e-02
  4.46008705e-02  4.37830873e-02  4.35960703e-02  4.37667035e-02
  4.35945392e-02  4.29222845e-02  4.15987447e-02  4.00834121e-02
  3.85631174e-02  3.66316810e-02  3.47514898e-02  3.39340158e-02
  3.37975100e-02  3.29569653e-02  3.07955164e-02  2.87240203e-02
  2.80001871e-02  2.75698844e-02  2.69562267e-02  2.67944690e-02
  2.65967008e-02  2.57719606e-02  2.51081083e-02  2.46733725e-02
  2.43903510e-02  2.49179993e-02  2.55789682e-02  2.50472687e-02
  2.42051836e-02  2.40558498e-02  2.33587269e-02  2.15963535e-02
  1.95400734e-02  1.85058378e-02  1.82627998e-02  1.74426381e-02
  1.59597881e-02  1.53766191e-02  1.52840270e-02  1.43080577e-02
  1.36109609e-02  1.38266869e-02  1.39251361e-02  1.34426188e-02
  1.27457594e-02  1.22130690e-02  1.17090391e-02  1.15126418e-02
  1.11879027e-02  1.06935650e-02  9.74805187e-03  8.96419771e-03
  9.28981137e-03  9.92611330e-03  9.73976962e-03  8.30599107e-03
  6.50554663e-03  4.95570851e-03  3.78282275e-03  2.73183291e-03
  1.52230659e-03  5.83910267e-04  1.45972110e-04 -3.08712537e-04
 -7.95500644e-04 -9.54521704e-04 -7.12874287e-04 -8.65372247e-04
 -1.26457482e-03 -1.89564016e-03 -2.59142183e-03 -3.16651189e-03
 -3.31219821e-03 -3.08106258e-03 -3.05783120e-03 -3.64985596e-03
 -3.88245168e-03 -3.23953619e-03 -3.28465598e-03 -4.71523544e-03
 -6.62796292e-03 -7.70049216e-03 -7.87529536e-03 -8.30820575e-03
 -9.44722071e-03 -1.06096948e-02 -1.13097103e-02 -1.11212134e-02
 -1.04914960e-02 -1.03317201e-02 -1.05983140e-02 -1.05463304e-02
 -1.06373103e-02 -1.12458859e-02 -1.09697022e-02 -1.12113105e-02
 -1.17537687e-02 -1.08218063e-02 -1.02802906e-02 -1.14894267e-02
 -1.17668696e-02 -1.05840489e-02 -1.08364588e-02 -1.21382307e-02
 -1.31932152e-02 -1.43027930e-02 -1.56773869e-02 -1.67671498e-02
 -1.73629764e-02 -1.74424518e-02 -1.78399440e-02 -1.88665222e-02
 -1.96127184e-02 -1.97718889e-02 -2.01326367e-02 -2.02308130e-02
 -2.11822074e-02 -2.21026633e-02 -2.12347358e-02 -2.18129456e-02
 -2.35254522e-02 -2.27135625e-02 -2.26251259e-02 -2.44279541e-02
 -2.41624787e-02 -2.47874223e-02 -2.68387925e-02 -2.15655472e-02]
