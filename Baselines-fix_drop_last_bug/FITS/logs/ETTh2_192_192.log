Args in experiment:
Namespace(is_training=1, model_id='ETTh2_192_192', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=64, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_192_192_FITS_ETTh2_ftM_sl192_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8257
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=64, out_features=128, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7340032.0
params:  8320.0
Trainable parameters:  8320
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.8167543411254883
Epoch: 1, Steps: 64 | Train Loss: 0.5809127 Vali Loss: 0.3781488 Test Loss: 0.5079340
Validation loss decreased (inf --> 0.378149).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.9626104831695557
Epoch: 2, Steps: 64 | Train Loss: 0.4802982 Vali Loss: 0.3477396 Test Loss: 0.4681280
Validation loss decreased (0.378149 --> 0.347740).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.957746744155884
Epoch: 3, Steps: 64 | Train Loss: 0.4235383 Vali Loss: 0.3309820 Test Loss: 0.4463593
Validation loss decreased (0.347740 --> 0.330982).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.1091182231903076
Epoch: 4, Steps: 64 | Train Loss: 0.3910908 Vali Loss: 0.3210310 Test Loss: 0.4340005
Validation loss decreased (0.330982 --> 0.321031).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2279820442199707
Epoch: 5, Steps: 64 | Train Loss: 0.3696277 Vali Loss: 0.3149182 Test Loss: 0.4262311
Validation loss decreased (0.321031 --> 0.314918).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2470548152923584
Epoch: 6, Steps: 64 | Train Loss: 0.3559523 Vali Loss: 0.3107337 Test Loss: 0.4210396
Validation loss decreased (0.314918 --> 0.310734).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1971099376678467
Epoch: 7, Steps: 64 | Train Loss: 0.3450233 Vali Loss: 0.3076014 Test Loss: 0.4171384
Validation loss decreased (0.310734 --> 0.307601).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.1949422359466553
Epoch: 8, Steps: 64 | Train Loss: 0.3362823 Vali Loss: 0.3051371 Test Loss: 0.4139751
Validation loss decreased (0.307601 --> 0.305137).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.780278205871582
Epoch: 9, Steps: 64 | Train Loss: 0.3289748 Vali Loss: 0.3027087 Test Loss: 0.4114163
Validation loss decreased (0.305137 --> 0.302709).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.052659749984741
Epoch: 10, Steps: 64 | Train Loss: 0.3223477 Vali Loss: 0.3013890 Test Loss: 0.4090621
Validation loss decreased (0.302709 --> 0.301389).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.9038827419281006
Epoch: 11, Steps: 64 | Train Loss: 0.3174854 Vali Loss: 0.2993766 Test Loss: 0.4070988
Validation loss decreased (0.301389 --> 0.299377).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.602379083633423
Epoch: 12, Steps: 64 | Train Loss: 0.3123077 Vali Loss: 0.2980623 Test Loss: 0.4052707
Validation loss decreased (0.299377 --> 0.298062).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.9438085556030273
Epoch: 13, Steps: 64 | Train Loss: 0.3101785 Vali Loss: 0.2972154 Test Loss: 0.4036091
Validation loss decreased (0.298062 --> 0.297215).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.877429962158203
Epoch: 14, Steps: 64 | Train Loss: 0.3071098 Vali Loss: 0.2959909 Test Loss: 0.4021556
Validation loss decreased (0.297215 --> 0.295991).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.0984416007995605
Epoch: 15, Steps: 64 | Train Loss: 0.3032787 Vali Loss: 0.2950074 Test Loss: 0.4007017
Validation loss decreased (0.295991 --> 0.295007).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.197469472885132
Epoch: 16, Steps: 64 | Train Loss: 0.3002140 Vali Loss: 0.2939827 Test Loss: 0.3994669
Validation loss decreased (0.295007 --> 0.293983).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.22428560256958
Epoch: 17, Steps: 64 | Train Loss: 0.2982475 Vali Loss: 0.2931304 Test Loss: 0.3982650
Validation loss decreased (0.293983 --> 0.293130).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.25003719329834
Epoch: 18, Steps: 64 | Train Loss: 0.2964319 Vali Loss: 0.2922014 Test Loss: 0.3971789
Validation loss decreased (0.293130 --> 0.292201).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.147925615310669
Epoch: 19, Steps: 64 | Train Loss: 0.2946290 Vali Loss: 0.2915488 Test Loss: 0.3962156
Validation loss decreased (0.292201 --> 0.291549).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.2747979164123535
Epoch: 20, Steps: 64 | Train Loss: 0.2929603 Vali Loss: 0.2906978 Test Loss: 0.3952709
Validation loss decreased (0.291549 --> 0.290698).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.7158679962158203
Epoch: 21, Steps: 64 | Train Loss: 0.2909604 Vali Loss: 0.2900662 Test Loss: 0.3944422
Validation loss decreased (0.290698 --> 0.290066).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.7862489223480225
Epoch: 22, Steps: 64 | Train Loss: 0.2896825 Vali Loss: 0.2895173 Test Loss: 0.3936450
Validation loss decreased (0.290066 --> 0.289517).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.9500961303710938
Epoch: 23, Steps: 64 | Train Loss: 0.2881507 Vali Loss: 0.2889217 Test Loss: 0.3929118
Validation loss decreased (0.289517 --> 0.288922).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.75527286529541
Epoch: 24, Steps: 64 | Train Loss: 0.2874839 Vali Loss: 0.2884420 Test Loss: 0.3922343
Validation loss decreased (0.288922 --> 0.288442).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.9517650604248047
Epoch: 25, Steps: 64 | Train Loss: 0.2858528 Vali Loss: 0.2879144 Test Loss: 0.3916267
Validation loss decreased (0.288442 --> 0.287914).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.886073350906372
Epoch: 26, Steps: 64 | Train Loss: 0.2847627 Vali Loss: 0.2874820 Test Loss: 0.3910138
Validation loss decreased (0.287914 --> 0.287482).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.832667827606201
Epoch: 27, Steps: 64 | Train Loss: 0.2838702 Vali Loss: 0.2869789 Test Loss: 0.3904677
Validation loss decreased (0.287482 --> 0.286979).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.650035381317139
Epoch: 28, Steps: 64 | Train Loss: 0.2833114 Vali Loss: 0.2866293 Test Loss: 0.3899834
Validation loss decreased (0.286979 --> 0.286629).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.1313698291778564
Epoch: 29, Steps: 64 | Train Loss: 0.2830178 Vali Loss: 0.2862518 Test Loss: 0.3894862
Validation loss decreased (0.286629 --> 0.286252).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.1998980045318604
Epoch: 30, Steps: 64 | Train Loss: 0.2821862 Vali Loss: 0.2859048 Test Loss: 0.3890510
Validation loss decreased (0.286252 --> 0.285905).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 8257
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=64, out_features=128, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7340032.0
params:  8320.0
Trainable parameters:  8320
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.2950189113616943
Epoch: 1, Steps: 64 | Train Loss: 0.5243561 Vali Loss: 0.2803402 Test Loss: 0.3839002
Validation loss decreased (inf --> 0.280340).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.2608232498168945
Epoch: 2, Steps: 64 | Train Loss: 0.5190718 Vali Loss: 0.2770463 Test Loss: 0.3807778
Validation loss decreased (0.280340 --> 0.277046).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.183333396911621
Epoch: 3, Steps: 64 | Train Loss: 0.5177960 Vali Loss: 0.2751015 Test Loss: 0.3789985
Validation loss decreased (0.277046 --> 0.275102).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.843568801879883
Epoch: 4, Steps: 64 | Train Loss: 0.5157217 Vali Loss: 0.2740900 Test Loss: 0.3778187
Validation loss decreased (0.275102 --> 0.274090).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.4180617332458496
Epoch: 5, Steps: 64 | Train Loss: 0.5142217 Vali Loss: 0.2732085 Test Loss: 0.3770799
Validation loss decreased (0.274090 --> 0.273209).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.908135175704956
Epoch: 6, Steps: 64 | Train Loss: 0.5137185 Vali Loss: 0.2728862 Test Loss: 0.3766043
Validation loss decreased (0.273209 --> 0.272886).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.9868857860565186
Epoch: 7, Steps: 64 | Train Loss: 0.5125747 Vali Loss: 0.2724769 Test Loss: 0.3761684
Validation loss decreased (0.272886 --> 0.272477).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.889267921447754
Epoch: 8, Steps: 64 | Train Loss: 0.5131873 Vali Loss: 0.2721846 Test Loss: 0.3760370
Validation loss decreased (0.272477 --> 0.272185).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.951404333114624
Epoch: 9, Steps: 64 | Train Loss: 0.5118892 Vali Loss: 0.2717842 Test Loss: 0.3758196
Validation loss decreased (0.272185 --> 0.271784).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.954129695892334
Epoch: 10, Steps: 64 | Train Loss: 0.5123530 Vali Loss: 0.2718157 Test Loss: 0.3756175
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.328681230545044
Epoch: 11, Steps: 64 | Train Loss: 0.5120134 Vali Loss: 0.2717125 Test Loss: 0.3754579
Validation loss decreased (0.271784 --> 0.271713).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.2735443115234375
Epoch: 12, Steps: 64 | Train Loss: 0.5091820 Vali Loss: 0.2717259 Test Loss: 0.3751868
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.2524571418762207
Epoch: 13, Steps: 64 | Train Loss: 0.5111259 Vali Loss: 0.2716462 Test Loss: 0.3751042
Validation loss decreased (0.271713 --> 0.271646).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.262258291244507
Epoch: 14, Steps: 64 | Train Loss: 0.5110732 Vali Loss: 0.2715774 Test Loss: 0.3750345
Validation loss decreased (0.271646 --> 0.271577).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.197453022003174
Epoch: 15, Steps: 64 | Train Loss: 0.5096510 Vali Loss: 0.2714099 Test Loss: 0.3749280
Validation loss decreased (0.271577 --> 0.271410).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.6735899448394775
Epoch: 16, Steps: 64 | Train Loss: 0.5092550 Vali Loss: 0.2713700 Test Loss: 0.3749294
Validation loss decreased (0.271410 --> 0.271370).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.7698023319244385
Epoch: 17, Steps: 64 | Train Loss: 0.5110360 Vali Loss: 0.2713586 Test Loss: 0.3748065
Validation loss decreased (0.271370 --> 0.271359).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.8779852390289307
Epoch: 18, Steps: 64 | Train Loss: 0.5087164 Vali Loss: 0.2711336 Test Loss: 0.3748263
Validation loss decreased (0.271359 --> 0.271134).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.85105037689209
Epoch: 19, Steps: 64 | Train Loss: 0.5110198 Vali Loss: 0.2712797 Test Loss: 0.3747170
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.045902729034424
Epoch: 20, Steps: 64 | Train Loss: 0.5113420 Vali Loss: 0.2712177 Test Loss: 0.3746709
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.973172187805176
Epoch: 21, Steps: 64 | Train Loss: 0.5104477 Vali Loss: 0.2711945 Test Loss: 0.3745208
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.9985766410827637
Epoch: 22, Steps: 64 | Train Loss: 0.5091862 Vali Loss: 0.2711530 Test Loss: 0.3744968
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.640570640563965
Epoch: 23, Steps: 64 | Train Loss: 0.5098709 Vali Loss: 0.2710948 Test Loss: 0.3744365
Validation loss decreased (0.271134 --> 0.271095).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.181622266769409
Epoch: 24, Steps: 64 | Train Loss: 0.5081122 Vali Loss: 0.2710605 Test Loss: 0.3743981
Validation loss decreased (0.271095 --> 0.271060).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.169198751449585
Epoch: 25, Steps: 64 | Train Loss: 0.5094083 Vali Loss: 0.2710247 Test Loss: 0.3743446
Validation loss decreased (0.271060 --> 0.271025).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.1703507900238037
Epoch: 26, Steps: 64 | Train Loss: 0.5102955 Vali Loss: 0.2709476 Test Loss: 0.3743837
Validation loss decreased (0.271025 --> 0.270948).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.356675148010254
Epoch: 27, Steps: 64 | Train Loss: 0.5091169 Vali Loss: 0.2709855 Test Loss: 0.3743449
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.8472447395324707
Epoch: 28, Steps: 64 | Train Loss: 0.5098073 Vali Loss: 0.2710060 Test Loss: 0.3742884
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.862248659133911
Epoch: 29, Steps: 64 | Train Loss: 0.5095597 Vali Loss: 0.2709102 Test Loss: 0.3742863
Validation loss decreased (0.270948 --> 0.270910).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.305548906326294
Epoch: 30, Steps: 64 | Train Loss: 0.5096334 Vali Loss: 0.2704812 Test Loss: 0.3742639
Validation loss decreased (0.270910 --> 0.270481).  Saving model ...
Updating learning rate to 0.00011296777049628277
>>>>>>>testing : ETTh2_192_192_FITS_ETTh2_ftM_sl192_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.35565119981765747, mae:0.38512593507766724, rse:0.4782477915287018, corr:[0.2679204  0.2699069  0.26849934 0.26906785 0.26722962 0.26624113
 0.2658063  0.26443413 0.26351678 0.26251638 0.26118767 0.2595889
 0.25800553 0.2567864  0.2560088  0.25567743 0.25511906 0.25434226
 0.2534299  0.25222477 0.2506549  0.24947493 0.24820907 0.2460521
 0.2433591  0.24087188 0.23895785 0.23733516 0.23552667 0.23382536
 0.23264217 0.23088184 0.22887918 0.22730474 0.22588444 0.22443421
 0.22287229 0.22152303 0.22063367 0.219599   0.21845077 0.21791972
 0.2176155  0.2161986  0.21440166 0.21316202 0.21195933 0.20979352
 0.20682378 0.20460525 0.20319046 0.20126037 0.19938995 0.19792949
 0.19546244 0.19285767 0.19168161 0.19024947 0.18857718 0.18764663
 0.18711859 0.1860911  0.18630245 0.1861653  0.18515942 0.18469146
 0.18420362 0.18278074 0.18197724 0.18167803 0.18079866 0.17978474
 0.1781117  0.17689325 0.17641208 0.17509218 0.17345887 0.1731374
 0.17287527 0.17153488 0.1714041  0.17138965 0.17077586 0.17085862
 0.17071466 0.16989449 0.17009328 0.17035453 0.16932149 0.16892998
 0.16903767 0.16816065 0.16778545 0.16813403 0.16784526 0.1667371
 0.1656551  0.1646515  0.16376512 0.16292015 0.16206083 0.16147627
 0.16117513 0.16029939 0.15999137 0.16011389 0.15989369 0.16007276
 0.15995431 0.15898858 0.1581613  0.158135   0.15748337 0.1567565
 0.1566989  0.15605024 0.15520684 0.15458716 0.15324299 0.15140872
 0.14958896 0.14770891 0.14651342 0.14577885 0.14448915 0.14318383
 0.14252158 0.14179549 0.1410951  0.14032796 0.13968638 0.13918583
 0.13847196 0.13774642 0.13746224 0.13719253 0.13631555 0.13577932
 0.13559079 0.13477322 0.13415986 0.13432954 0.13357614 0.13190569
 0.13003464 0.12856296 0.12737831 0.12634046 0.1252783  0.12440681
 0.12411692 0.12352578 0.12294874 0.12254501 0.12266523 0.1226975
 0.1221885  0.12191401 0.12206025 0.12272597 0.12233526 0.12182458
 0.12205649 0.1218718  0.12177405 0.12204399 0.12189168 0.12074585
 0.11936108 0.11812907 0.11717764 0.11668786 0.11584859 0.11425682
 0.11350586 0.11249415 0.11153584 0.11077704 0.11054365 0.11025736
 0.10958491 0.10944752 0.10984775 0.11013925 0.10955667 0.1095756
 0.10872424 0.10730375 0.10837962 0.10761242 0.10746174 0.10966085]
