Args in experiment:
Namespace(is_training=1, model_id='ETTm2_336_720', model='FITS', data='ETTm2', root_path='./dataset/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=100, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_336_720_FITS_ETTm2_ftM_sl336_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=100, out_features=314, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  28134400.0
params:  31714.0
Trainable parameters:  31714
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5227402
	speed: 0.0525s/iter; left time: 406.2536s
	iters: 200, epoch: 1 | loss: 0.5239496
	speed: 0.0443s/iter; left time: 337.8755s
Epoch: 1 cost time: 12.483924865722656
Epoch: 1, Steps: 261 | Train Loss: 0.4885752 Vali Loss: 0.2969452 Test Loss: 0.4049855
Validation loss decreased (inf --> 0.296945).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3286653
	speed: 0.1740s/iter; left time: 1299.4938s
	iters: 200, epoch: 2 | loss: 0.3375565
	speed: 0.0391s/iter; left time: 287.9771s
Epoch: 2 cost time: 11.181870222091675
Epoch: 2, Steps: 261 | Train Loss: 0.3940513 Vali Loss: 0.2826162 Test Loss: 0.3882529
Validation loss decreased (0.296945 --> 0.282616).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4389753
	speed: 0.1783s/iter; left time: 1285.6729s
	iters: 200, epoch: 3 | loss: 0.3191984
	speed: 0.0385s/iter; left time: 273.9790s
Epoch: 3 cost time: 10.601420402526855
Epoch: 3, Steps: 261 | Train Loss: 0.3768976 Vali Loss: 0.2763971 Test Loss: 0.3814849
Validation loss decreased (0.282616 --> 0.276397).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4434097
	speed: 0.1752s/iter; left time: 1217.0173s
	iters: 200, epoch: 4 | loss: 0.3159077
	speed: 0.0376s/iter; left time: 257.5623s
Epoch: 4 cost time: 10.616211652755737
Epoch: 4, Steps: 261 | Train Loss: 0.3696722 Vali Loss: 0.2727502 Test Loss: 0.3774737
Validation loss decreased (0.276397 --> 0.272750).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3029395
	speed: 0.1983s/iter; left time: 1326.2439s
	iters: 200, epoch: 5 | loss: 0.3319710
	speed: 0.0509s/iter; left time: 335.5025s
Epoch: 5 cost time: 13.812031745910645
Epoch: 5, Steps: 261 | Train Loss: 0.3656536 Vali Loss: 0.2708952 Test Loss: 0.3750319
Validation loss decreased (0.272750 --> 0.270895).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2838207
	speed: 0.2108s/iter; left time: 1354.5684s
	iters: 200, epoch: 6 | loss: 0.3589431
	speed: 0.0456s/iter; left time: 288.6801s
Epoch: 6 cost time: 12.432533025741577
Epoch: 6, Steps: 261 | Train Loss: 0.3629625 Vali Loss: 0.2694840 Test Loss: 0.3737283
Validation loss decreased (0.270895 --> 0.269484).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3759584
	speed: 0.2003s/iter; left time: 1234.7557s
	iters: 200, epoch: 7 | loss: 0.3168063
	speed: 0.0430s/iter; left time: 260.5176s
Epoch: 7 cost time: 12.944522857666016
Epoch: 7, Steps: 261 | Train Loss: 0.3616838 Vali Loss: 0.2687163 Test Loss: 0.3727579
Validation loss decreased (0.269484 --> 0.268716).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4011613
	speed: 0.1930s/iter; left time: 1139.2386s
	iters: 200, epoch: 8 | loss: 0.3767509
	speed: 0.0427s/iter; left time: 247.9115s
Epoch: 8 cost time: 11.292628765106201
Epoch: 8, Steps: 261 | Train Loss: 0.3606168 Vali Loss: 0.2675291 Test Loss: 0.3720847
Validation loss decreased (0.268716 --> 0.267529).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2981880
	speed: 0.1953s/iter; left time: 1102.1041s
	iters: 200, epoch: 9 | loss: 0.4288902
	speed: 0.0496s/iter; left time: 274.8317s
Epoch: 9 cost time: 12.694387435913086
Epoch: 9, Steps: 261 | Train Loss: 0.3599656 Vali Loss: 0.2673385 Test Loss: 0.3717719
Validation loss decreased (0.267529 --> 0.267339).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3491823
	speed: 0.2037s/iter; left time: 1096.0928s
	iters: 200, epoch: 10 | loss: 0.3235064
	speed: 0.0373s/iter; left time: 196.8824s
Epoch: 10 cost time: 11.60872483253479
Epoch: 10, Steps: 261 | Train Loss: 0.3590850 Vali Loss: 0.2672671 Test Loss: 0.3714630
Validation loss decreased (0.267339 --> 0.267267).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2910442
	speed: 0.1976s/iter; left time: 1011.9420s
	iters: 200, epoch: 11 | loss: 0.3611038
	speed: 0.0399s/iter; left time: 200.1663s
Epoch: 11 cost time: 12.11592960357666
Epoch: 11, Steps: 261 | Train Loss: 0.3592316 Vali Loss: 0.2667347 Test Loss: 0.3714133
Validation loss decreased (0.267267 --> 0.266735).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2703378
	speed: 0.2236s/iter; left time: 1086.8395s
	iters: 200, epoch: 12 | loss: 0.4563725
	speed: 0.0448s/iter; left time: 213.3989s
Epoch: 12 cost time: 12.502041101455688
Epoch: 12, Steps: 261 | Train Loss: 0.3582932 Vali Loss: 0.2664536 Test Loss: 0.3712313
Validation loss decreased (0.266735 --> 0.266454).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3861898
	speed: 0.2010s/iter; left time: 924.4123s
	iters: 200, epoch: 13 | loss: 0.3186812
	speed: 0.0389s/iter; left time: 175.0268s
Epoch: 13 cost time: 11.088501453399658
Epoch: 13, Steps: 261 | Train Loss: 0.3582678 Vali Loss: 0.2663346 Test Loss: 0.3711344
Validation loss decreased (0.266454 --> 0.266335).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3970929
	speed: 0.2044s/iter; left time: 886.8588s
	iters: 200, epoch: 14 | loss: 0.3706043
	speed: 0.0395s/iter; left time: 167.5308s
Epoch: 14 cost time: 11.495398044586182
Epoch: 14, Steps: 261 | Train Loss: 0.3582117 Vali Loss: 0.2661585 Test Loss: 0.3709439
Validation loss decreased (0.266335 --> 0.266159).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4254076
	speed: 0.1708s/iter; left time: 696.4475s
	iters: 200, epoch: 15 | loss: 0.2992573
	speed: 0.0396s/iter; left time: 157.4022s
Epoch: 15 cost time: 11.145927667617798
Epoch: 15, Steps: 261 | Train Loss: 0.3579436 Vali Loss: 0.2658336 Test Loss: 0.3709393
Validation loss decreased (0.266159 --> 0.265834).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3378602
	speed: 0.1759s/iter; left time: 671.2203s
	iters: 200, epoch: 16 | loss: 0.2919160
	speed: 0.0402s/iter; left time: 149.2237s
Epoch: 16 cost time: 11.27654218673706
Epoch: 16, Steps: 261 | Train Loss: 0.3576116 Vali Loss: 0.2658806 Test Loss: 0.3708727
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3795037
	speed: 0.1811s/iter; left time: 643.8206s
	iters: 200, epoch: 17 | loss: 0.3273009
	speed: 0.0379s/iter; left time: 130.8995s
Epoch: 17 cost time: 9.815504789352417
Epoch: 17, Steps: 261 | Train Loss: 0.3576482 Vali Loss: 0.2660583 Test Loss: 0.3708147
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6128066
	speed: 0.1735s/iter; left time: 571.6459s
	iters: 200, epoch: 18 | loss: 0.2980194
	speed: 0.0395s/iter; left time: 126.0249s
Epoch: 18 cost time: 10.774207353591919
Epoch: 18, Steps: 261 | Train Loss: 0.3580399 Vali Loss: 0.2660038 Test Loss: 0.3708677
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3770585
	speed: 0.1953s/iter; left time: 592.4964s
	iters: 200, epoch: 19 | loss: 0.4760616
	speed: 0.0412s/iter; left time: 120.8080s
Epoch: 19 cost time: 11.480607986450195
Epoch: 19, Steps: 261 | Train Loss: 0.3579551 Vali Loss: 0.2657472 Test Loss: 0.3708041
Validation loss decreased (0.265834 --> 0.265747).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4018308
	speed: 0.1827s/iter; left time: 506.4035s
	iters: 200, epoch: 20 | loss: 0.3423017
	speed: 0.0396s/iter; left time: 105.8713s
Epoch: 20 cost time: 11.32086968421936
Epoch: 20, Steps: 261 | Train Loss: 0.3576591 Vali Loss: 0.2658864 Test Loss: 0.3707806
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2950862
	speed: 0.2018s/iter; left time: 506.7617s
	iters: 200, epoch: 21 | loss: 0.2084722
	speed: 0.0425s/iter; left time: 102.5509s
Epoch: 21 cost time: 11.879096269607544
Epoch: 21, Steps: 261 | Train Loss: 0.3579003 Vali Loss: 0.2656147 Test Loss: 0.3707435
Validation loss decreased (0.265747 --> 0.265615).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3009075
	speed: 0.1870s/iter; left time: 420.7118s
	iters: 200, epoch: 22 | loss: 0.3142911
	speed: 0.0463s/iter; left time: 99.5150s
Epoch: 22 cost time: 12.061258554458618
Epoch: 22, Steps: 261 | Train Loss: 0.3574406 Vali Loss: 0.2655823 Test Loss: 0.3707491
Validation loss decreased (0.265615 --> 0.265582).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3612444
	speed: 0.1959s/iter; left time: 389.6185s
	iters: 200, epoch: 23 | loss: 0.3269738
	speed: 0.0428s/iter; left time: 80.8043s
Epoch: 23 cost time: 11.85393238067627
Epoch: 23, Steps: 261 | Train Loss: 0.3572110 Vali Loss: 0.2657047 Test Loss: 0.3707125
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5343440
	speed: 0.1806s/iter; left time: 312.0293s
	iters: 200, epoch: 24 | loss: 0.3461674
	speed: 0.0370s/iter; left time: 60.2432s
Epoch: 24 cost time: 9.816427946090698
Epoch: 24, Steps: 261 | Train Loss: 0.3572404 Vali Loss: 0.2657079 Test Loss: 0.3706864
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4375653
	speed: 0.1699s/iter; left time: 249.2584s
	iters: 200, epoch: 25 | loss: 0.3872195
	speed: 0.0410s/iter; left time: 56.0981s
Epoch: 25 cost time: 10.679457902908325
Epoch: 25, Steps: 261 | Train Loss: 0.3569571 Vali Loss: 0.2656175 Test Loss: 0.3706808
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2524284
	speed: 0.1849s/iter; left time: 223.0001s
	iters: 200, epoch: 26 | loss: 0.4341088
	speed: 0.0421s/iter; left time: 46.5529s
Epoch: 26 cost time: 11.220752477645874
Epoch: 26, Steps: 261 | Train Loss: 0.3574891 Vali Loss: 0.2653427 Test Loss: 0.3706421
Validation loss decreased (0.265582 --> 0.265343).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3339509
	speed: 0.1773s/iter; left time: 167.5489s
	iters: 200, epoch: 27 | loss: 0.3379613
	speed: 0.0391s/iter; left time: 33.0679s
Epoch: 27 cost time: 11.391396284103394
Epoch: 27, Steps: 261 | Train Loss: 0.3574780 Vali Loss: 0.2653992 Test Loss: 0.3706510
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4462171
	speed: 0.1979s/iter; left time: 135.3951s
	iters: 200, epoch: 28 | loss: 0.3141249
	speed: 0.0422s/iter; left time: 24.6337s
Epoch: 28 cost time: 12.03596305847168
Epoch: 28, Steps: 261 | Train Loss: 0.3572549 Vali Loss: 0.2656967 Test Loss: 0.3706618
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3116536
	speed: 0.1911s/iter; left time: 80.8528s
	iters: 200, epoch: 29 | loss: 0.3653187
	speed: 0.0459s/iter; left time: 14.8148s
Epoch: 29 cost time: 11.893089532852173
Epoch: 29, Steps: 261 | Train Loss: 0.3571861 Vali Loss: 0.2656042 Test Loss: 0.3706575
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3689147
	speed: 0.1943s/iter; left time: 31.4731s
	iters: 200, epoch: 30 | loss: 0.3141094
	speed: 0.0456s/iter; left time: 2.8288s
Epoch: 30 cost time: 12.557498455047607
Epoch: 30, Steps: 261 | Train Loss: 0.3573795 Vali Loss: 0.2652821 Test Loss: 0.3706337
Validation loss decreased (0.265343 --> 0.265282).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 33505
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=100, out_features=314, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  28134400.0
params:  31714.0
Trainable parameters:  31714
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4703986
	speed: 0.0475s/iter; left time: 367.0730s
	iters: 200, epoch: 1 | loss: 0.4508458
	speed: 0.0457s/iter; left time: 348.5439s
Epoch: 1 cost time: 11.933388471603394
Epoch: 1, Steps: 261 | Train Loss: 0.5204202 Vali Loss: 0.2648782 Test Loss: 0.3701824
Validation loss decreased (inf --> 0.264878).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4992498
	speed: 0.2086s/iter; left time: 1557.8708s
	iters: 200, epoch: 2 | loss: 0.6533108
	speed: 0.0497s/iter; left time: 366.3224s
Epoch: 2 cost time: 12.920410871505737
Epoch: 2, Steps: 261 | Train Loss: 0.5195120 Vali Loss: 0.2651716 Test Loss: 0.3700996
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6026464
	speed: 0.2014s/iter; left time: 1451.8108s
	iters: 200, epoch: 3 | loss: 0.5910168
	speed: 0.0408s/iter; left time: 289.9818s
Epoch: 3 cost time: 11.351109027862549
Epoch: 3, Steps: 261 | Train Loss: 0.5196929 Vali Loss: 0.2648868 Test Loss: 0.3700799
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6627899
	speed: 0.1912s/iter; left time: 1328.3083s
	iters: 200, epoch: 4 | loss: 0.4894367
	speed: 0.0396s/iter; left time: 271.2011s
Epoch: 4 cost time: 11.312763929367065
Epoch: 4, Steps: 261 | Train Loss: 0.5194433 Vali Loss: 0.2649806 Test Loss: 0.3699467
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5415943
	speed: 0.2065s/iter; left time: 1380.6359s
	iters: 200, epoch: 5 | loss: 0.5912830
	speed: 0.0424s/iter; left time: 278.9955s
Epoch: 5 cost time: 12.11858320236206
Epoch: 5, Steps: 261 | Train Loss: 0.5190433 Vali Loss: 0.2649694 Test Loss: 0.3700736
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5108249
	speed: 0.1955s/iter; left time: 1256.0426s
	iters: 200, epoch: 6 | loss: 0.4692162
	speed: 0.0447s/iter; left time: 282.6970s
Epoch: 6 cost time: 12.577452659606934
Epoch: 6, Steps: 261 | Train Loss: 0.5189485 Vali Loss: 0.2647920 Test Loss: 0.3699031
Validation loss decreased (0.264878 --> 0.264792).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4225022
	speed: 0.2044s/iter; left time: 1259.9779s
	iters: 200, epoch: 7 | loss: 0.5254362
	speed: 0.0447s/iter; left time: 270.8978s
Epoch: 7 cost time: 12.51586365699768
Epoch: 7, Steps: 261 | Train Loss: 0.5189559 Vali Loss: 0.2645125 Test Loss: 0.3699825
Validation loss decreased (0.264792 --> 0.264512).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5577335
	speed: 0.2052s/iter; left time: 1211.2961s
	iters: 200, epoch: 8 | loss: 0.4410338
	speed: 0.0379s/iter; left time: 219.8404s
Epoch: 8 cost time: 12.258078575134277
Epoch: 8, Steps: 261 | Train Loss: 0.5185487 Vali Loss: 0.2650008 Test Loss: 0.3698407
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4832021
	speed: 0.2021s/iter; left time: 1140.5050s
	iters: 200, epoch: 9 | loss: 0.3411665
	speed: 0.0484s/iter; left time: 268.4435s
Epoch: 9 cost time: 13.999883890151978
Epoch: 9, Steps: 261 | Train Loss: 0.5191350 Vali Loss: 0.2644091 Test Loss: 0.3698817
Validation loss decreased (0.264512 --> 0.264409).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4895836
	speed: 0.2146s/iter; left time: 1155.2415s
	iters: 200, epoch: 10 | loss: 0.6131575
	speed: 0.0469s/iter; left time: 247.8376s
Epoch: 10 cost time: 12.579399824142456
Epoch: 10, Steps: 261 | Train Loss: 0.5185371 Vali Loss: 0.2646005 Test Loss: 0.3699115
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4028265
	speed: 0.2121s/iter; left time: 1085.9529s
	iters: 200, epoch: 11 | loss: 0.6057451
	speed: 0.0528s/iter; left time: 265.2900s
Epoch: 11 cost time: 13.517558813095093
Epoch: 11, Steps: 261 | Train Loss: 0.5190081 Vali Loss: 0.2645691 Test Loss: 0.3698626
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4935463
	speed: 0.2172s/iter; left time: 1055.4659s
	iters: 200, epoch: 12 | loss: 0.4819508
	speed: 0.0438s/iter; left time: 208.3454s
Epoch: 12 cost time: 13.119070291519165
Epoch: 12, Steps: 261 | Train Loss: 0.5186000 Vali Loss: 0.2645760 Test Loss: 0.3698753
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3709251
	speed: 0.2161s/iter; left time: 993.9027s
	iters: 200, epoch: 13 | loss: 0.5186232
	speed: 0.0456s/iter; left time: 205.1022s
Epoch: 13 cost time: 13.572422504425049
Epoch: 13, Steps: 261 | Train Loss: 0.5176936 Vali Loss: 0.2643514 Test Loss: 0.3698879
Validation loss decreased (0.264409 --> 0.264351).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3149913
	speed: 0.2361s/iter; left time: 1024.0924s
	iters: 200, epoch: 14 | loss: 0.6250467
	speed: 0.0535s/iter; left time: 226.8768s
Epoch: 14 cost time: 14.115234136581421
Epoch: 14, Steps: 261 | Train Loss: 0.5182829 Vali Loss: 0.2642748 Test Loss: 0.3697978
Validation loss decreased (0.264351 --> 0.264275).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.7097241
	speed: 0.2247s/iter; left time: 915.9921s
	iters: 200, epoch: 15 | loss: 0.4997091
	speed: 0.0467s/iter; left time: 185.7324s
Epoch: 15 cost time: 13.31049656867981
Epoch: 15, Steps: 261 | Train Loss: 0.5185595 Vali Loss: 0.2644210 Test Loss: 0.3698255
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5518727
	speed: 0.2054s/iter; left time: 783.9666s
	iters: 200, epoch: 16 | loss: 0.5037208
	speed: 0.0429s/iter; left time: 159.3324s
Epoch: 16 cost time: 11.82950210571289
Epoch: 16, Steps: 261 | Train Loss: 0.5184803 Vali Loss: 0.2645718 Test Loss: 0.3697765
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4198308
	speed: 0.2015s/iter; left time: 716.4655s
	iters: 200, epoch: 17 | loss: 0.6459146
	speed: 0.0462s/iter; left time: 159.6224s
Epoch: 17 cost time: 12.73158049583435
Epoch: 17, Steps: 261 | Train Loss: 0.5179999 Vali Loss: 0.2647025 Test Loss: 0.3697874
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5888491
	speed: 0.2102s/iter; left time: 692.3660s
	iters: 200, epoch: 18 | loss: 0.6010311
	speed: 0.0448s/iter; left time: 143.1736s
Epoch: 18 cost time: 12.815965414047241
Epoch: 18, Steps: 261 | Train Loss: 0.5182748 Vali Loss: 0.2645424 Test Loss: 0.3697950
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4516121
	speed: 0.2079s/iter; left time: 630.5893s
	iters: 200, epoch: 19 | loss: 0.5707688
	speed: 0.0448s/iter; left time: 131.5018s
Epoch: 19 cost time: 12.615192890167236
Epoch: 19, Steps: 261 | Train Loss: 0.5184766 Vali Loss: 0.2644947 Test Loss: 0.3697585
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTm2_336_720_FITS_ETTm2_ftM_sl336_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3673650026321411, mae:0.38263002038002014, rse:0.4871843159198761, corr:[0.5523841  0.5446587  0.54323107 0.5433318  0.54104185 0.5409979
 0.5409168  0.5393946  0.5389984  0.53853047 0.5373156  0.53722596
 0.53746605 0.5366129  0.5360372  0.53595275 0.5352069  0.53451407
 0.53447473 0.53384125 0.53268796 0.5322283  0.53211397 0.531625
 0.53110385 0.53061837 0.5301075  0.53002214 0.5300486  0.5293435
 0.5284376  0.52803737 0.5275189  0.52669454 0.5262514  0.5260218
 0.5253459  0.5247808  0.5246502  0.5242316  0.5234169  0.5229363
 0.5229029  0.52269304 0.52218205 0.5215006  0.52069485 0.51993567
 0.5192779  0.5185706  0.517811   0.5171454  0.5163189  0.51521176
 0.51432323 0.5139047  0.51359165 0.5130392  0.51241374 0.5118193
 0.51142144 0.5112076  0.51111436 0.5108976  0.5105857  0.51029956
 0.50997096 0.5097481  0.5095846  0.50937045 0.5091798  0.5091045
 0.50892353 0.5085543  0.5081803  0.5078968  0.50751096 0.50701654
 0.5065696  0.5061097  0.50558597 0.5050817  0.50465006 0.504114
 0.5034133  0.502728   0.5023498  0.5022082  0.50182486 0.50115114
 0.5006053  0.50028664 0.49978983 0.49897918 0.49791083 0.49647734
 0.49475184 0.4931663  0.4917503  0.4903111  0.48903814 0.48796275
 0.48676276 0.48535648 0.4840944  0.48288426 0.48144397 0.48008627
 0.47921535 0.4783169  0.47714335 0.47616374 0.47549564 0.47454497
 0.47336844 0.47256082 0.4719137  0.47095972 0.46998248 0.46917224
 0.46844944 0.46770945 0.46694702 0.46608898 0.46523893 0.4645187
 0.46395943 0.46322408 0.4623695  0.46146807 0.46061674 0.46001348
 0.4596848  0.45916626 0.45843118 0.45803255 0.45785135 0.45741382
 0.4568616  0.45646903 0.45600164 0.45535487 0.4548421  0.45431414
 0.4535915  0.45285133 0.45221835 0.45148626 0.4508301  0.4503234
 0.44969442 0.44883734 0.44806364 0.44740131 0.44651923 0.44569957
 0.44525635 0.44478792 0.44388697 0.4433282  0.44343388 0.44344047
 0.44284278 0.44207975 0.44177955 0.44162166 0.44119465 0.44074824
 0.44055542 0.44029576 0.4397691  0.4395362  0.43965748 0.4393658
 0.43849224 0.4379844  0.4380587  0.4376194  0.4363569  0.43545687
 0.43540272 0.43525127 0.43487942 0.43487945 0.43491817 0.4343475
 0.43346655 0.4331021  0.43317875 0.4328675  0.43194214 0.4305826
 0.4288715  0.4271198  0.42554918 0.4240045  0.42204753 0.41990793
 0.41850287 0.41775513 0.41665494 0.4151212  0.41349018 0.4116079
 0.40975577 0.40872514 0.40819156 0.4069733  0.40510288 0.4038336
 0.4033462  0.40303463 0.40253973 0.40154743 0.40005496 0.39848876
 0.39753595 0.3968621  0.3960337  0.39506784 0.3941706  0.39329562
 0.3924189  0.39125904 0.38991034 0.3885456  0.38767368 0.38701022
 0.38627937 0.38578516 0.38498813 0.38333198 0.3818512  0.3816849
 0.38161686 0.38073534 0.38018054 0.38045245 0.38038486 0.37972346
 0.37933803 0.37912762 0.37859458 0.377867   0.37718302 0.37656775
 0.37628394 0.37639225 0.3764478  0.3760919  0.375929   0.37600455
 0.37575153 0.37524474 0.3751837  0.37577486 0.37594858 0.3752209
 0.37430164 0.37389025 0.3737411  0.37361702 0.3736431  0.37365276
 0.37308827 0.3724953  0.3725306  0.37219492 0.37082088 0.37013462
 0.37084022 0.37093106 0.3697238  0.36904517 0.36895916 0.36842856
 0.368031   0.3684746  0.368418   0.3677483  0.36792323 0.36844328
 0.36777422 0.36699542 0.36779377 0.36848232 0.36754856 0.3662202
 0.3652161  0.36364707 0.36166453 0.36074218 0.3603299  0.35913402
 0.35759345 0.35633892 0.35554352 0.35529467 0.35476184 0.35334
 0.352242   0.35268927 0.3532075  0.35233867 0.35123625 0.3505883
 0.34924823 0.34789622 0.3480837  0.34875217 0.34808472 0.34712544
 0.3470078  0.34720004 0.3470423  0.34667674 0.3460501  0.34553656
 0.34555015 0.34526184 0.34390184 0.34285218 0.34354448 0.3442384
 0.34360102 0.34335822 0.34418324 0.34412867 0.34305698 0.34308612
 0.3442469  0.34450972 0.34408998 0.34427702 0.3446259  0.344325
 0.344021   0.3443341  0.34466434 0.34435272 0.34358191 0.342545
 0.34161234 0.3416399  0.34241077 0.3425564  0.34156084 0.34083492
 0.34137642 0.34228274 0.3424981  0.342351   0.34243324 0.34258437
 0.34217724 0.3414619  0.3410911  0.34120142 0.34120703 0.3411283
 0.3411886  0.34114647 0.3405392  0.34006923 0.3404285  0.34081975
 0.34013966 0.33938393 0.33955047 0.33997387 0.34015954 0.34014046
 0.3395194  0.33798748 0.3372566  0.33838874 0.33923817 0.33885628
 0.33931082 0.34066257 0.34047264 0.33928004 0.33960032 0.3399369
 0.33830205 0.3365186  0.3360157  0.33473366 0.3322313  0.3309603
 0.33049846 0.3291843  0.32827458 0.32830405 0.32710937 0.32485974
 0.3244256  0.325158   0.32415915 0.3222136  0.3215784  0.32124567
 0.31984878 0.31830662 0.3176232  0.317326   0.3171162  0.3167542
 0.31571925 0.31464925 0.31442732 0.31450176 0.31395856 0.31314883
 0.3128256  0.31291723 0.3131769  0.31330535 0.31304926 0.31273913
 0.313002   0.3134648  0.31338623 0.31312165 0.31319788 0.31327608
 0.31335434 0.3140343  0.31496128 0.3149944  0.3146628  0.31521055
 0.3158513  0.31528613 0.31444684 0.31463474 0.31480545 0.31399706
 0.31318003 0.3132542  0.3132747  0.31276876 0.31243014 0.31215572
 0.31142566 0.31108037 0.31182212 0.3124776  0.31200483 0.31127962
 0.31135872 0.3115742  0.310907   0.309691   0.30903217 0.309248
 0.30964917 0.30939284 0.30834964 0.3076066  0.30770946 0.30780458
 0.3072556  0.30660385 0.30617863 0.3059968  0.3058861  0.30566508
 0.30523482 0.30525276 0.3060584  0.30616242 0.3049433  0.30402356
 0.30435696 0.30459338 0.30401915 0.30338544 0.3025753  0.30128413
 0.30000928 0.29843426 0.2960241  0.29443067 0.2943737  0.29365024
 0.2917131  0.2905899  0.2902218  0.2889428  0.28747576 0.28679457
 0.28553048 0.28386495 0.28370637 0.28406063 0.28275105 0.28073764
 0.2798831  0.2792502  0.27828032 0.2780176  0.27805558 0.27714953
 0.27598143 0.27555802 0.27541348 0.27506292 0.2748142  0.27444553
 0.27372414 0.27328968 0.27336186 0.273454   0.27331075 0.27309892
 0.272888   0.27271426 0.27260816 0.27220044 0.27173832 0.2719919
 0.2727249  0.27277562 0.27204335 0.27167454 0.27190372 0.27176028
 0.27120525 0.27117372 0.27170855 0.27174303 0.27105904 0.27051494
 0.27033433 0.26967695 0.26856935 0.26801163 0.2684678  0.26894206
 0.26853526 0.2677262  0.26753527 0.26820683 0.26873177 0.26843306
 0.2672142  0.2659575  0.2655454  0.265869   0.26601803 0.26592955
 0.26589167 0.26574686 0.26527265 0.2649321  0.26468915 0.26422754
 0.26408365 0.2647333  0.26501063 0.2643367  0.26398173 0.26473984
 0.26528072 0.2649431  0.26438564 0.26393005 0.26370212 0.26413584
 0.26480824 0.26458967 0.2636555  0.2629473  0.2621237  0.26037195
 0.25838345 0.2570311  0.25572327 0.25410813 0.2526851  0.25135154
 0.24982068 0.24856754 0.24771406 0.24649541 0.24475843 0.24325413
 0.2421657  0.24080688 0.23937547 0.2386223  0.23859209 0.23822448
 0.23688616 0.23541173 0.2346607  0.23425323 0.23346862 0.23234361
 0.23165655 0.23128076 0.23054515 0.22986303 0.23027967 0.23102845
 0.23067182 0.22955972 0.22903188 0.22911718 0.22894792 0.22850755
 0.22861354 0.22915915 0.22924489 0.22876436 0.22854379 0.22887027
 0.22906494 0.2288851  0.22875762 0.22910629 0.22959194 0.22967069
 0.22941358 0.22914155 0.22908887 0.2291728  0.2292247  0.2292357
 0.22894076 0.2284854  0.22824533 0.22802374 0.2274057  0.22679637
 0.2272181  0.22795774 0.22787921 0.22743356 0.22708282 0.2267236
 0.22650085 0.22684658 0.22703016 0.22635682 0.22566146 0.22542708
 0.22491361 0.2240155  0.2238598  0.22468399 0.22500242 0.22415441
 0.22345763 0.22361831 0.22407477 0.22461398 0.22512156 0.22512546
 0.22465856 0.22474445 0.22592485 0.22657132 0.22564006 0.2249793
 0.22592783 0.22690243 0.22639169 0.22552738 0.22515659 0.22404037
 0.2221698  0.22102548 0.22049066 0.21906179 0.21674642 0.21488415
 0.21396545 0.21328783 0.21254376 0.21178967 0.21076846 0.20937206
 0.20860721 0.20892599 0.20935337 0.20871547 0.20733687 0.20569257
 0.20470369 0.20448565 0.20395869 0.20271257 0.20241323 0.20309237
 0.20253876 0.2010747  0.20086074 0.20094982 0.2001086  0.19985045
 0.20024846 0.1996512  0.19924957 0.2002416  0.20012754 0.19892763
 0.199558   0.20070276 0.19989458 0.19953054 0.20053384 0.20004542
 0.20005533 0.2023044  0.20141706 0.1993982  0.20359921 0.19881646]
