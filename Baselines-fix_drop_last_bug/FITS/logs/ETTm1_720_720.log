Args in experiment:
Namespace(is_training=1, model_id='ETTm1_720_720', model='FITS', data='ETTm1', root_path='./dataset/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=196, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4435017
	speed: 0.0829s/iter; left time: 633.1650s
	iters: 200, epoch: 1 | loss: 0.3813865
	speed: 0.0725s/iter; left time: 546.8170s
Epoch: 1 cost time: 19.82778573036194
Epoch: 1, Steps: 258 | Train Loss: 0.4652649 Vali Loss: 1.1568681 Test Loss: 0.5606091
Validation loss decreased (inf --> 1.156868).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3021667
	speed: 0.3107s/iter; left time: 2293.6036s
	iters: 200, epoch: 2 | loss: 0.2924099
	speed: 0.0634s/iter; left time: 461.7019s
Epoch: 2 cost time: 17.770398378372192
Epoch: 2, Steps: 258 | Train Loss: 0.3079215 Vali Loss: 1.0500487 Test Loss: 0.4900487
Validation loss decreased (1.156868 --> 1.050049).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2617062
	speed: 0.3004s/iter; left time: 2140.3294s
	iters: 200, epoch: 3 | loss: 0.2531183
	speed: 0.0779s/iter; left time: 547.0485s
Epoch: 3 cost time: 20.336018800735474
Epoch: 3, Steps: 258 | Train Loss: 0.2610067 Vali Loss: 1.0055952 Test Loss: 0.4604694
Validation loss decreased (1.050049 --> 1.005595).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2581064
	speed: 0.3416s/iter; left time: 2346.0406s
	iters: 200, epoch: 4 | loss: 0.2472820
	speed: 0.0681s/iter; left time: 460.7159s
Epoch: 4 cost time: 19.02520513534546
Epoch: 4, Steps: 258 | Train Loss: 0.2385529 Vali Loss: 0.9783005 Test Loss: 0.4432352
Validation loss decreased (1.005595 --> 0.978300).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2243102
	speed: 0.2990s/iter; left time: 1975.7912s
	iters: 200, epoch: 5 | loss: 0.2386600
	speed: 0.0583s/iter; left time: 379.3005s
Epoch: 5 cost time: 17.50383424758911
Epoch: 5, Steps: 258 | Train Loss: 0.2260372 Vali Loss: 0.9651271 Test Loss: 0.4325456
Validation loss decreased (0.978300 --> 0.965127).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2132032
	speed: 0.3167s/iter; left time: 2011.2324s
	iters: 200, epoch: 6 | loss: 0.2139130
	speed: 0.0692s/iter; left time: 432.8681s
Epoch: 6 cost time: 19.012670755386353
Epoch: 6, Steps: 258 | Train Loss: 0.2185607 Vali Loss: 0.9536594 Test Loss: 0.4258859
Validation loss decreased (0.965127 --> 0.953659).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2222873
	speed: 0.3122s/iter; left time: 1902.4619s
	iters: 200, epoch: 7 | loss: 0.2030971
	speed: 0.0810s/iter; left time: 485.7060s
Epoch: 7 cost time: 20.43281364440918
Epoch: 7, Steps: 258 | Train Loss: 0.2139387 Vali Loss: 0.9478609 Test Loss: 0.4213288
Validation loss decreased (0.953659 --> 0.947861).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2035410
	speed: 0.3166s/iter; left time: 1847.5136s
	iters: 200, epoch: 8 | loss: 0.2189086
	speed: 0.0704s/iter; left time: 403.8188s
Epoch: 8 cost time: 19.49175000190735
Epoch: 8, Steps: 258 | Train Loss: 0.2110030 Vali Loss: 0.9445586 Test Loss: 0.4186252
Validation loss decreased (0.947861 --> 0.944559).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2179553
	speed: 0.2963s/iter; left time: 1652.6730s
	iters: 200, epoch: 9 | loss: 0.2109344
	speed: 0.0653s/iter; left time: 357.7266s
Epoch: 9 cost time: 19.483577251434326
Epoch: 9, Steps: 258 | Train Loss: 0.2090811 Vali Loss: 0.9419015 Test Loss: 0.4178440
Validation loss decreased (0.944559 --> 0.941902).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2199484
	speed: 0.3052s/iter; left time: 1623.4320s
	iters: 200, epoch: 10 | loss: 0.2202937
	speed: 0.0799s/iter; left time: 416.8352s
Epoch: 10 cost time: 20.885192394256592
Epoch: 10, Steps: 258 | Train Loss: 0.2077737 Vali Loss: 0.9410235 Test Loss: 0.4170197
Validation loss decreased (0.941902 --> 0.941024).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2266578
	speed: 0.3332s/iter; left time: 1686.3844s
	iters: 200, epoch: 11 | loss: 0.1916489
	speed: 0.0768s/iter; left time: 381.0599s
Epoch: 11 cost time: 19.55611538887024
Epoch: 11, Steps: 258 | Train Loss: 0.2068821 Vali Loss: 0.9399293 Test Loss: 0.4175324
Validation loss decreased (0.941024 --> 0.939929).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2036913
	speed: 0.2894s/iter; left time: 1389.7852s
	iters: 200, epoch: 12 | loss: 0.2109288
	speed: 0.0552s/iter; left time: 259.5960s
Epoch: 12 cost time: 15.982593059539795
Epoch: 12, Steps: 258 | Train Loss: 0.2061711 Vali Loss: 0.9387136 Test Loss: 0.4174718
Validation loss decreased (0.939929 --> 0.938714).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1986681
	speed: 0.3075s/iter; left time: 1397.6273s
	iters: 200, epoch: 13 | loss: 0.2182073
	speed: 0.0736s/iter; left time: 327.0975s
Epoch: 13 cost time: 19.5348060131073
Epoch: 13, Steps: 258 | Train Loss: 0.2057514 Vali Loss: 0.9384311 Test Loss: 0.4174722
Validation loss decreased (0.938714 --> 0.938431).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1888755
	speed: 0.3036s/iter; left time: 1301.6697s
	iters: 200, epoch: 14 | loss: 0.2081129
	speed: 0.0795s/iter; left time: 332.7190s
Epoch: 14 cost time: 20.33174228668213
Epoch: 14, Steps: 258 | Train Loss: 0.2054771 Vali Loss: 0.9382564 Test Loss: 0.4176803
Validation loss decreased (0.938431 --> 0.938256).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2025314
	speed: 0.3411s/iter; left time: 1374.2858s
	iters: 200, epoch: 15 | loss: 0.2063322
	speed: 0.0709s/iter; left time: 278.4255s
Epoch: 15 cost time: 19.89159846305847
Epoch: 15, Steps: 258 | Train Loss: 0.2052310 Vali Loss: 0.9388050 Test Loss: 0.4178857
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1916521
	speed: 0.2994s/iter; left time: 1128.9323s
	iters: 200, epoch: 16 | loss: 0.2125225
	speed: 0.0715s/iter; left time: 262.4758s
Epoch: 16 cost time: 19.308537244796753
Epoch: 16, Steps: 258 | Train Loss: 0.2051204 Vali Loss: 0.9377906 Test Loss: 0.4175156
Validation loss decreased (0.938256 --> 0.937791).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2015622
	speed: 0.3102s/iter; left time: 1089.6187s
	iters: 200, epoch: 17 | loss: 0.2014248
	speed: 0.0674s/iter; left time: 229.9012s
Epoch: 17 cost time: 19.051218509674072
Epoch: 17, Steps: 258 | Train Loss: 0.2050286 Vali Loss: 0.9387364 Test Loss: 0.4175154
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2069220
	speed: 0.3331s/iter; left time: 1084.0950s
	iters: 200, epoch: 18 | loss: 0.1897700
	speed: 0.0760s/iter; left time: 239.6776s
Epoch: 18 cost time: 20.357190132141113
Epoch: 18, Steps: 258 | Train Loss: 0.2050024 Vali Loss: 0.9377660 Test Loss: 0.4177488
Validation loss decreased (0.937791 --> 0.937766).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2245645
	speed: 0.3190s/iter; left time: 956.1235s
	iters: 200, epoch: 19 | loss: 0.2130179
	speed: 0.0741s/iter; left time: 214.8094s
Epoch: 19 cost time: 20.24539589881897
Epoch: 19, Steps: 258 | Train Loss: 0.2050062 Vali Loss: 0.9379460 Test Loss: 0.4170029
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2132687
	speed: 0.3064s/iter; left time: 839.1298s
	iters: 200, epoch: 20 | loss: 0.1967551
	speed: 0.0628s/iter; left time: 165.8379s
Epoch: 20 cost time: 18.290159940719604
Epoch: 20, Steps: 258 | Train Loss: 0.2048406 Vali Loss: 0.9383128 Test Loss: 0.4170742
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1845861
	speed: 0.2996s/iter; left time: 743.1945s
	iters: 200, epoch: 21 | loss: 0.2109101
	speed: 0.0877s/iter; left time: 208.8729s
Epoch: 21 cost time: 20.83878493309021
Epoch: 21, Steps: 258 | Train Loss: 0.2049068 Vali Loss: 0.9381383 Test Loss: 0.4173950
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1975981
	speed: 0.3414s/iter; left time: 758.9207s
	iters: 200, epoch: 22 | loss: 0.1824756
	speed: 0.0725s/iter; left time: 154.0182s
Epoch: 22 cost time: 19.379817962646484
Epoch: 22, Steps: 258 | Train Loss: 0.2048319 Vali Loss: 0.9380197 Test Loss: 0.4172731
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2135943
	speed: 0.2669s/iter; left time: 524.4362s
	iters: 200, epoch: 23 | loss: 0.2001219
	speed: 0.0531s/iter; left time: 99.0269s
Epoch: 23 cost time: 15.066529273986816
Epoch: 23, Steps: 258 | Train Loss: 0.2048405 Vali Loss: 0.9389436 Test Loss: 0.4175940
EarlyStopping counter: 5 out of 5
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3850961
	speed: 0.0756s/iter; left time: 577.9111s
	iters: 200, epoch: 1 | loss: 0.3600545
	speed: 0.0672s/iter; left time: 507.0646s
Epoch: 1 cost time: 18.259466409683228
Epoch: 1, Steps: 258 | Train Loss: 0.3978890 Vali Loss: 0.9336754 Test Loss: 0.4177180
Validation loss decreased (inf --> 0.933675).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4170939
	speed: 0.2777s/iter; left time: 2050.0132s
	iters: 200, epoch: 2 | loss: 0.4088680
	speed: 0.0811s/iter; left time: 590.6613s
Epoch: 2 cost time: 19.779333353042603
Epoch: 2, Steps: 258 | Train Loss: 0.3972981 Vali Loss: 0.9321769 Test Loss: 0.4173551
Validation loss decreased (0.933675 --> 0.932177).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3945174
	speed: 0.3303s/iter; left time: 2353.0567s
	iters: 200, epoch: 3 | loss: 0.4024485
	speed: 0.0739s/iter; left time: 519.2152s
Epoch: 3 cost time: 20.27083110809326
Epoch: 3, Steps: 258 | Train Loss: 0.3970067 Vali Loss: 0.9324372 Test Loss: 0.4175806
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3750723
	speed: 0.2869s/iter; left time: 1970.1158s
	iters: 200, epoch: 4 | loss: 0.4045449
	speed: 0.0640s/iter; left time: 432.9139s
Epoch: 4 cost time: 18.602195739746094
Epoch: 4, Steps: 258 | Train Loss: 0.3967424 Vali Loss: 0.9321395 Test Loss: 0.4173090
Validation loss decreased (0.932177 --> 0.932140).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3940644
	speed: 0.2945s/iter; left time: 1946.1318s
	iters: 200, epoch: 5 | loss: 0.4233254
	speed: 0.0651s/iter; left time: 424.0569s
Epoch: 5 cost time: 18.308101177215576
Epoch: 5, Steps: 258 | Train Loss: 0.3967096 Vali Loss: 0.9317837 Test Loss: 0.4173305
Validation loss decreased (0.932140 --> 0.931784).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4258513
	speed: 0.2897s/iter; left time: 1839.8043s
	iters: 200, epoch: 6 | loss: 0.3877950
	speed: 0.0757s/iter; left time: 473.2759s
Epoch: 6 cost time: 18.504017114639282
Epoch: 6, Steps: 258 | Train Loss: 0.3964917 Vali Loss: 0.9310357 Test Loss: 0.4174098
Validation loss decreased (0.931784 --> 0.931036).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3924652
	speed: 0.3153s/iter; left time: 1921.0453s
	iters: 200, epoch: 7 | loss: 0.4199634
	speed: 0.0682s/iter; left time: 408.5162s
Epoch: 7 cost time: 19.500937700271606
Epoch: 7, Steps: 258 | Train Loss: 0.3964571 Vali Loss: 0.9332598 Test Loss: 0.4175306
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3575273
	speed: 0.2920s/iter; left time: 1703.9225s
	iters: 200, epoch: 8 | loss: 0.4283313
	speed: 0.0625s/iter; left time: 358.2295s
Epoch: 8 cost time: 17.69587230682373
Epoch: 8, Steps: 258 | Train Loss: 0.3965557 Vali Loss: 0.9308864 Test Loss: 0.4175850
Validation loss decreased (0.931036 --> 0.930886).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4030633
	speed: 0.2848s/iter; left time: 1588.3589s
	iters: 200, epoch: 9 | loss: 0.3812349
	speed: 0.0715s/iter; left time: 391.7294s
Epoch: 9 cost time: 18.460641622543335
Epoch: 9, Steps: 258 | Train Loss: 0.3964163 Vali Loss: 0.9309486 Test Loss: 0.4172629
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3834797
	speed: 0.3144s/iter; left time: 1672.1923s
	iters: 200, epoch: 10 | loss: 0.3782939
	speed: 0.0728s/iter; left time: 380.1608s
Epoch: 10 cost time: 19.726698637008667
Epoch: 10, Steps: 258 | Train Loss: 0.3963359 Vali Loss: 0.9299027 Test Loss: 0.4176809
Validation loss decreased (0.930886 --> 0.929903).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4045131
	speed: 0.3119s/iter; left time: 1578.4095s
	iters: 200, epoch: 11 | loss: 0.3729294
	speed: 0.0580s/iter; left time: 287.9814s
Epoch: 11 cost time: 17.844598054885864
Epoch: 11, Steps: 258 | Train Loss: 0.3962136 Vali Loss: 0.9300829 Test Loss: 0.4173080
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3900982
	speed: 0.3003s/iter; left time: 1442.1687s
	iters: 200, epoch: 12 | loss: 0.3929603
	speed: 0.0680s/iter; left time: 319.9533s
Epoch: 12 cost time: 18.6121826171875
Epoch: 12, Steps: 258 | Train Loss: 0.3962009 Vali Loss: 0.9301177 Test Loss: 0.4173434
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4282775
	speed: 0.2830s/iter; left time: 1286.0166s
	iters: 200, epoch: 13 | loss: 0.3814299
	speed: 0.0722s/iter; left time: 320.9013s
Epoch: 13 cost time: 18.478787183761597
Epoch: 13, Steps: 258 | Train Loss: 0.3963239 Vali Loss: 0.9306474 Test Loss: 0.4173728
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4059142
	speed: 0.3155s/iter; left time: 1352.4891s
	iters: 200, epoch: 14 | loss: 0.3951320
	speed: 0.0744s/iter; left time: 311.7211s
Epoch: 14 cost time: 20.446391820907593
Epoch: 14, Steps: 258 | Train Loss: 0.3961949 Vali Loss: 0.9308639 Test Loss: 0.4170408
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4005812
	speed: 0.3190s/iter; left time: 1285.1460s
	iters: 200, epoch: 15 | loss: 0.4111903
	speed: 0.0616s/iter; left time: 241.9813s
Epoch: 15 cost time: 18.371042251586914
Epoch: 15, Steps: 258 | Train Loss: 0.3961062 Vali Loss: 0.9294574 Test Loss: 0.4177331
Validation loss decreased (0.929903 --> 0.929457).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3702201
	speed: 0.3031s/iter; left time: 1143.0467s
	iters: 200, epoch: 16 | loss: 0.4382003
	speed: 0.0668s/iter; left time: 245.0770s
Epoch: 16 cost time: 18.967216968536377
Epoch: 16, Steps: 258 | Train Loss: 0.3961046 Vali Loss: 0.9305446 Test Loss: 0.4178920
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3914359
	speed: 0.2935s/iter; left time: 1031.1677s
	iters: 200, epoch: 17 | loss: 0.4429415
	speed: 0.0646s/iter; left time: 220.6464s
Epoch: 17 cost time: 17.44429659843445
Epoch: 17, Steps: 258 | Train Loss: 0.3960339 Vali Loss: 0.9298335 Test Loss: 0.4170947
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3930593
	speed: 0.3146s/iter; left time: 1024.0493s
	iters: 200, epoch: 18 | loss: 0.3820679
	speed: 0.0782s/iter; left time: 246.7713s
Epoch: 18 cost time: 20.686387300491333
Epoch: 18, Steps: 258 | Train Loss: 0.3960523 Vali Loss: 0.9291248 Test Loss: 0.4174100
Validation loss decreased (0.929457 --> 0.929125).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3964878
	speed: 0.3208s/iter; left time: 961.3724s
	iters: 200, epoch: 19 | loss: 0.3891767
	speed: 0.0602s/iter; left time: 174.3246s
Epoch: 19 cost time: 17.290069818496704
Epoch: 19, Steps: 258 | Train Loss: 0.3959650 Vali Loss: 0.9301628 Test Loss: 0.4172609
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4333856
	speed: 0.2875s/iter; left time: 787.4912s
	iters: 200, epoch: 20 | loss: 0.3736552
	speed: 0.0695s/iter; left time: 183.5173s
Epoch: 20 cost time: 18.356975317001343
Epoch: 20, Steps: 258 | Train Loss: 0.3960959 Vali Loss: 0.9292641 Test Loss: 0.4172401
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4257195
	speed: 0.2989s/iter; left time: 741.6116s
	iters: 200, epoch: 21 | loss: 0.3853826
	speed: 0.0638s/iter; left time: 151.8689s
Epoch: 21 cost time: 18.257959365844727
Epoch: 21, Steps: 258 | Train Loss: 0.3959734 Vali Loss: 0.9309019 Test Loss: 0.4175722
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3926271
	speed: 0.3119s/iter; left time: 693.4637s
	iters: 200, epoch: 22 | loss: 0.3840304
	speed: 0.0702s/iter; left time: 149.0752s
Epoch: 22 cost time: 20.241304874420166
Epoch: 22, Steps: 258 | Train Loss: 0.3959271 Vali Loss: 0.9302939 Test Loss: 0.4171579
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4008646
	speed: 0.2979s/iter; left time: 585.4533s
	iters: 200, epoch: 23 | loss: 0.3977727
	speed: 0.0579s/iter; left time: 107.9448s
Epoch: 23 cost time: 16.529890298843384
Epoch: 23, Steps: 258 | Train Loss: 0.3959032 Vali Loss: 0.9298841 Test Loss: 0.4171195
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41740902853012085, mae:0.4174798903465271, rse:0.6132093071937561, corr:[0.52753174 0.5300033  0.5323302  0.53732085 0.53883034 0.53903663
 0.5406811  0.5414643  0.541099   0.541837   0.542937   0.54261035
 0.54228425 0.5429419  0.54291385 0.5415994  0.54018    0.5388172
 0.5372906  0.5362927  0.5356749  0.5342825  0.5321352  0.53043026
 0.5292359  0.52808106 0.5269508  0.5263452  0.52634674 0.52621686
 0.52601296 0.5265597  0.5276438  0.52864105 0.52893823 0.52926487
 0.529606   0.52959573 0.5288278  0.52773577 0.5274237  0.5274694
 0.5270849  0.5268426  0.5274681  0.52822024 0.52810276 0.52752477
 0.5274444  0.52762127 0.52734095 0.52671343 0.5264841  0.5264346
 0.5263711  0.52634126 0.52637845 0.5264518  0.5267464  0.5269034
 0.52683634 0.5268562  0.52708054 0.52688754 0.5263508  0.52624375
 0.52670497 0.5268873  0.5265058  0.52621865 0.52646905 0.5266303
 0.52643186 0.526308   0.5264411  0.52631825 0.5258849  0.5257562
 0.52600574 0.52600926 0.5254323  0.5248268  0.5247452  0.5247358
 0.52426374 0.523572   0.5236319  0.5244211  0.52494544 0.52481866
 0.5246605  0.52512026 0.5257763  0.5259736  0.5258777  0.52582926
 0.5256833  0.52524126 0.52477986 0.524824   0.5249223  0.5245655
 0.52406186 0.5237818  0.5236936  0.5235579  0.5233447  0.5233499
 0.5233093  0.5230804  0.52283883 0.52275056 0.5225929  0.5221094
 0.52146626 0.52118355 0.52128345 0.5211143  0.520381   0.51981544
 0.5201269  0.5207962  0.5207567  0.5201838  0.51979935 0.5196047
 0.5192461  0.51902646 0.5192286  0.51945007 0.51915455 0.5188745
 0.5191102  0.51912075 0.51836354 0.5178831  0.5184388  0.5189655
 0.5189046  0.5188485  0.5191514  0.5194096  0.5193766  0.5194285
 0.519654   0.5196554  0.5194611  0.5192737  0.51930684 0.51946735
 0.51953673 0.51919883 0.5186217  0.518569   0.5190694  0.519298
 0.5188728  0.51867247 0.51925904 0.51955354 0.5189831  0.51832277
 0.51856303 0.51923573 0.519475   0.51928246 0.5193325  0.5199161
 0.52055585 0.5205902  0.5202231  0.52017176 0.5204681  0.52049613
 0.51994354 0.51956964 0.5199976  0.52062565 0.52063495 0.52029806
 0.5202129  0.52037424 0.5204187  0.52051634 0.52078134 0.52094334
 0.5209138  0.5210682  0.521569   0.5220634  0.52221715 0.5220022
 0.5217385  0.5216655  0.5215504  0.5211492  0.5205542  0.51994896
 0.5192606  0.5183715  0.51733834 0.51642835 0.5156527  0.51490414
 0.5142038  0.5137003  0.513334   0.512845   0.51210195 0.51138604
 0.5108887  0.5105058  0.50994843 0.5092608  0.5086286  0.5082263
 0.5078033  0.5070907  0.50652415 0.50643104 0.50655735 0.50649846
 0.506431   0.50664264 0.5071374  0.5073636  0.50710416 0.50679886
 0.5067442  0.5068757  0.507155   0.5074221  0.5074468  0.5071186
 0.5068592  0.5071711  0.5075994  0.5077584  0.5075858  0.5077065
 0.50821656 0.50850594 0.50825864 0.50781864 0.5077758  0.50794035
 0.5078028  0.50743353 0.507403   0.50789195 0.5081264  0.5078789
 0.5074801  0.5074752  0.50753313 0.5074761  0.50756836 0.5077487
 0.50758874 0.5072066  0.5071746  0.5076336  0.5079543  0.5080237
 0.5081308  0.5083156  0.5081653  0.5077874  0.50781894 0.5083553
 0.50873554 0.5084867  0.5080772  0.5079288  0.5079137  0.5079464
 0.50802416 0.5080924  0.5079426  0.5079441  0.5082569  0.5085095
 0.50847536 0.5084232  0.50863516 0.5087554  0.50834465 0.5078002
 0.50765693 0.5075262  0.5067534  0.50565827 0.5051594  0.5051916
 0.5050363  0.5043615  0.5035528  0.5028356  0.5022888  0.50188684
 0.5014035  0.50082123 0.5004308  0.5003588  0.5002552  0.49977323
 0.4992887  0.4990935  0.49904513 0.49876064 0.49839708 0.49816316
 0.4981946  0.49838135 0.49846336 0.49845603 0.4983248  0.49818957
 0.49833953 0.49884373 0.4991378  0.49862236 0.4977599  0.49756914
 0.49800882 0.4980944  0.49750122 0.49717227 0.4974131  0.49746498
 0.49716753 0.49714816 0.49756172 0.4976003  0.4970037  0.49680227
 0.49721903 0.4972479  0.49675986 0.4965244  0.49690944 0.4971691
 0.49681857 0.4965229  0.49667454 0.4968969  0.49669093 0.49633795
 0.49634203 0.49666202 0.49691752 0.49691138 0.49682713 0.49666327
 0.4963471  0.49611503 0.49618334 0.49649554 0.49682823 0.49691227
 0.49674234 0.49659824 0.49651426 0.49641776 0.49641663 0.49656114
 0.49656895 0.49641785 0.49612632 0.49583045 0.49585605 0.4960927
 0.49610445 0.49576297 0.49559203 0.4961092  0.49688837 0.49720958
 0.49711734 0.49725452 0.49786383 0.49843502 0.49857107 0.49833453
 0.4980404  0.4977607  0.49740633 0.4970502  0.496825   0.4965644
 0.49596533 0.49529204 0.4948849  0.4946454  0.49416175 0.4936697
 0.49353233 0.4936204  0.4934431  0.4930739  0.4927634  0.49248186
 0.49191338 0.49131465 0.49084133 0.49053058 0.4904063  0.49058086
 0.49071956 0.49049166 0.49006876 0.49002683 0.49046525 0.49078056
 0.4909295  0.49099132 0.49125937 0.49135864 0.4912335  0.49112025
 0.49108115 0.49118143 0.49124685 0.49127233 0.49109554 0.49081293
 0.49059236 0.49065807 0.49078703 0.49091142 0.490993   0.49118862
 0.49114725 0.49080822 0.4905856  0.4905646  0.4903995  0.49007756
 0.48994717 0.4901077  0.49017772 0.49001464 0.48999125 0.49021518
 0.49030277 0.4901115  0.48989215 0.48987818 0.48993182 0.48974302
 0.48953834 0.4896628  0.49007228 0.49034476 0.4903378  0.49032313
 0.490377   0.49037635 0.49022162 0.4901028  0.48995453 0.48985755
 0.4898487  0.4899978  0.4899247  0.48966494 0.48960245 0.48986965
 0.49019173 0.49023765 0.4901892  0.4901385  0.49011907 0.49020085
 0.49038145 0.49051556 0.49048546 0.49049097 0.4906159  0.49043068
 0.48965785 0.48877588 0.488348   0.4882549  0.4880355  0.48751184
 0.4868088  0.48598194 0.48523852 0.4846264  0.4839928  0.48335752
 0.48281398 0.48232672 0.48174798 0.481091   0.4805545  0.48006234
 0.47942248 0.47889668 0.47862688 0.47839102 0.4782436  0.478024
 0.47789127 0.47800845 0.478232   0.47826374 0.47803798 0.47800758
 0.47844866 0.478861   0.4788869  0.478838   0.47890544 0.47916663
 0.47952464 0.47996393 0.48023596 0.48017943 0.4800474  0.48003224
 0.48016414 0.48044536 0.48067898 0.4805823  0.48044565 0.48077822
 0.48129368 0.48121214 0.48065555 0.48049754 0.48083776 0.48099154
 0.48082384 0.48078802 0.4809013  0.48067644 0.48023736 0.48033178
 0.4810019  0.48131704 0.48090288 0.48043403 0.48048887 0.4807737
 0.4807839  0.48074958 0.48091185 0.48103252 0.48090217 0.48081428
 0.48090228 0.48099124 0.48084262 0.48069713 0.48090696 0.48144358
 0.48177722 0.48174176 0.481546   0.48139367 0.48131296 0.481216
 0.48105773 0.48100775 0.48096445 0.48096153 0.4809245  0.48101538
 0.4813174  0.481536   0.48145008 0.4812703  0.4813394  0.48143148
 0.48104033 0.48011106 0.47906715 0.47836113 0.47798306 0.47776607
 0.47745112 0.47702417 0.47656238 0.47620693 0.47590277 0.4754198
 0.47478858 0.47411    0.47355244 0.47292888 0.47215185 0.47144896
 0.47109684 0.47090515 0.47049743 0.46988288 0.4695623  0.46967188
 0.46987888 0.4696239  0.46918944 0.4691381  0.4695177  0.46980783
 0.4698669  0.46983343 0.46994102 0.4701281  0.47028002 0.4705424
 0.47075245 0.4705897  0.4703628  0.47046947 0.47089136 0.47121432
 0.47141352 0.4716125  0.47157893 0.4713555  0.47145408 0.47194707
 0.4722756  0.47222894 0.471984   0.47174776 0.4716702  0.47189692
 0.47231314 0.4725534  0.47233704 0.4720864  0.4722956  0.4723339
 0.47174236 0.47121188 0.47141886 0.47169694 0.47135645 0.47059855
 0.47032645 0.4705988  0.47082466 0.4707221  0.47064686 0.47108054
 0.47166437 0.47166908 0.47118273 0.47084102 0.4708514  0.47087282
 0.4708984  0.47108695 0.4711805  0.47083604 0.47039294 0.47044617
 0.47077546 0.47083127 0.4706863  0.47082373 0.47125176 0.4714809
 0.4715354  0.4717034  0.47193843 0.47202688 0.47196683 0.47181886
 0.47145915 0.47088504 0.47034624 0.47006145 0.46982116 0.46932623
 0.4687234  0.46817294 0.46762088 0.46693614 0.46635196 0.4661845
 0.46591273 0.46542662 0.46499035 0.46484306 0.46458393 0.46407938
 0.46380058 0.4639103  0.46394584 0.46343723 0.46291545 0.46291345
 0.46322826 0.463602   0.4638653  0.4640274  0.46409884 0.4639351
 0.46386638 0.46406767 0.46464166 0.46547756 0.46594286 0.46601114
 0.4661573  0.46655726 0.46656543 0.4661303  0.46649432 0.46755305
 0.46804485 0.4681797  0.4685554  0.4681228  0.46748042 0.46665683]
