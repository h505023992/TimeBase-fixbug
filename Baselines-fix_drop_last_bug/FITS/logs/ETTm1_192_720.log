Args in experiment:
Namespace(is_training=1, model_id='ETTm1_192_720', model='FITS', data='ETTm1', root_path='./dataset/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=64, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_192_720_FITS_ETTm1_ftM_sl192_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=64, out_features=304, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17432576.0
params:  19760.0
Trainable parameters:  19760
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6310805
	speed: 0.0719s/iter; left time: 558.0391s
	iters: 200, epoch: 1 | loss: 0.4968068
	speed: 0.0571s/iter; left time: 437.2762s
Epoch: 1 cost time: 16.950401782989502
Epoch: 1, Steps: 262 | Train Loss: 0.6405474 Vali Loss: 1.2049896 Test Loss: 0.6323684
Validation loss decreased (inf --> 1.204990).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4010883
	speed: 0.2675s/iter; left time: 2005.6523s
	iters: 200, epoch: 2 | loss: 0.3906535
	speed: 0.0712s/iter; left time: 526.6556s
Epoch: 2 cost time: 17.425339221954346
Epoch: 2, Steps: 262 | Train Loss: 0.4163985 Vali Loss: 1.0945896 Test Loss: 0.5380023
Validation loss decreased (1.204990 --> 1.094590).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3872133
	speed: 0.2925s/iter; left time: 2116.6946s
	iters: 200, epoch: 3 | loss: 0.3748591
	speed: 0.0671s/iter; left time: 478.6426s
Epoch: 3 cost time: 18.737372875213623
Epoch: 3, Steps: 262 | Train Loss: 0.3834864 Vali Loss: 1.0427066 Test Loss: 0.4941370
Validation loss decreased (1.094590 --> 1.042707).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3853281
	speed: 0.3122s/iter; left time: 2177.9360s
	iters: 200, epoch: 4 | loss: 0.3566511
	speed: 0.0600s/iter; left time: 412.8383s
Epoch: 4 cost time: 17.454965353012085
Epoch: 4, Steps: 262 | Train Loss: 0.3674109 Vali Loss: 1.0126754 Test Loss: 0.4695677
Validation loss decreased (1.042707 --> 1.012675).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3136699
	speed: 0.2662s/iter; left time: 1786.9413s
	iters: 200, epoch: 5 | loss: 0.3393785
	speed: 0.0585s/iter; left time: 387.1319s
Epoch: 5 cost time: 16.226158618927002
Epoch: 5, Steps: 262 | Train Loss: 0.3588832 Vali Loss: 0.9963140 Test Loss: 0.4564481
Validation loss decreased (1.012675 --> 0.996314).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3741681
	speed: 0.2741s/iter; left time: 1768.3814s
	iters: 200, epoch: 6 | loss: 0.3593313
	speed: 0.0644s/iter; left time: 409.1014s
Epoch: 6 cost time: 18.489269495010376
Epoch: 6, Steps: 262 | Train Loss: 0.3543749 Vali Loss: 0.9884576 Test Loss: 0.4496455
Validation loss decreased (0.996314 --> 0.988458).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3598987
	speed: 0.3090s/iter; left time: 1912.5548s
	iters: 200, epoch: 7 | loss: 0.3204712
	speed: 0.0619s/iter; left time: 376.7661s
Epoch: 7 cost time: 17.25858998298645
Epoch: 7, Steps: 262 | Train Loss: 0.3520437 Vali Loss: 0.9850460 Test Loss: 0.4464859
Validation loss decreased (0.988458 --> 0.985046).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3702502
	speed: 0.2757s/iter; left time: 1633.8144s
	iters: 200, epoch: 8 | loss: 0.3433861
	speed: 0.0586s/iter; left time: 341.2022s
Epoch: 8 cost time: 17.323072910308838
Epoch: 8, Steps: 262 | Train Loss: 0.3510372 Vali Loss: 0.9833549 Test Loss: 0.4449925
Validation loss decreased (0.985046 --> 0.983355).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3330763
	speed: 0.2751s/iter; left time: 1558.3050s
	iters: 200, epoch: 9 | loss: 0.3586728
	speed: 0.0576s/iter; left time: 320.7893s
Epoch: 9 cost time: 15.891944885253906
Epoch: 9, Steps: 262 | Train Loss: 0.3505481 Vali Loss: 0.9815311 Test Loss: 0.4442876
Validation loss decreased (0.983355 --> 0.981531).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3362543
	speed: 0.3012s/iter; left time: 1627.2259s
	iters: 200, epoch: 10 | loss: 0.3568488
	speed: 0.0685s/iter; left time: 363.5048s
Epoch: 10 cost time: 20.05374526977539
Epoch: 10, Steps: 262 | Train Loss: 0.3501490 Vali Loss: 0.9795914 Test Loss: 0.4442910
Validation loss decreased (0.981531 --> 0.979591).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3147770
	speed: 0.2854s/iter; left time: 1467.4655s
	iters: 200, epoch: 11 | loss: 0.3529342
	speed: 0.0547s/iter; left time: 275.5353s
Epoch: 11 cost time: 16.64100933074951
Epoch: 11, Steps: 262 | Train Loss: 0.3500610 Vali Loss: 0.9802372 Test Loss: 0.4440226
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3383013
	speed: 0.2783s/iter; left time: 1358.0578s
	iters: 200, epoch: 12 | loss: 0.3570880
	speed: 0.0618s/iter; left time: 295.4813s
Epoch: 12 cost time: 17.258564472198486
Epoch: 12, Steps: 262 | Train Loss: 0.3499357 Vali Loss: 0.9805452 Test Loss: 0.4440338
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3674741
	speed: 0.2844s/iter; left time: 1312.9357s
	iters: 200, epoch: 13 | loss: 0.3245189
	speed: 0.0708s/iter; left time: 319.6507s
Epoch: 13 cost time: 19.174386501312256
Epoch: 13, Steps: 262 | Train Loss: 0.3499816 Vali Loss: 0.9802333 Test Loss: 0.4439530
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3598840
	speed: 0.3180s/iter; left time: 1384.9046s
	iters: 200, epoch: 14 | loss: 0.3801259
	speed: 0.0619s/iter; left time: 263.5487s
Epoch: 14 cost time: 17.625295162200928
Epoch: 14, Steps: 262 | Train Loss: 0.3499901 Vali Loss: 0.9802711 Test Loss: 0.4438699
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3526562
	speed: 0.2775s/iter; left time: 1135.7683s
	iters: 200, epoch: 15 | loss: 0.3617278
	speed: 0.0674s/iter; left time: 269.0481s
Epoch: 15 cost time: 17.31839108467102
Epoch: 15, Steps: 262 | Train Loss: 0.3500671 Vali Loss: 0.9803540 Test Loss: 0.4438257
EarlyStopping counter: 5 out of 5
Early stopping
train 33649
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=64, out_features=304, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17432576.0
params:  19760.0
Trainable parameters:  19760
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4663697
	speed: 0.0784s/iter; left time: 608.8275s
	iters: 200, epoch: 1 | loss: 0.4146935
	speed: 0.0692s/iter; left time: 530.2244s
Epoch: 1 cost time: 19.22147846221924
Epoch: 1, Steps: 262 | Train Loss: 0.4398611 Vali Loss: 0.9780833 Test Loss: 0.4424523
Validation loss decreased (inf --> 0.978083).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4302280
	speed: 0.3021s/iter; left time: 2265.1532s
	iters: 200, epoch: 2 | loss: 0.4371098
	speed: 0.0585s/iter; left time: 432.7076s
Epoch: 2 cost time: 16.822264909744263
Epoch: 2, Steps: 262 | Train Loss: 0.4395876 Vali Loss: 0.9770835 Test Loss: 0.4419578
Validation loss decreased (0.978083 --> 0.977084).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3922818
	speed: 0.2723s/iter; left time: 1970.6076s
	iters: 200, epoch: 3 | loss: 0.4050089
	speed: 0.0590s/iter; left time: 420.9541s
Epoch: 3 cost time: 17.131660223007202
Epoch: 3, Steps: 262 | Train Loss: 0.4394853 Vali Loss: 0.9780337 Test Loss: 0.4421693
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4254950
	speed: 0.2783s/iter; left time: 1941.0628s
	iters: 200, epoch: 4 | loss: 0.4293640
	speed: 0.0664s/iter; left time: 456.3725s
Epoch: 4 cost time: 18.164896965026855
Epoch: 4, Steps: 262 | Train Loss: 0.4395420 Vali Loss: 0.9776873 Test Loss: 0.4422082
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4466185
	speed: 0.3095s/iter; left time: 2077.7701s
	iters: 200, epoch: 5 | loss: 0.4182402
	speed: 0.0643s/iter; left time: 425.0021s
Epoch: 5 cost time: 18.43698740005493
Epoch: 5, Steps: 262 | Train Loss: 0.4393730 Vali Loss: 0.9782502 Test Loss: 0.4423930
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4338724
	speed: 0.2693s/iter; left time: 1737.1718s
	iters: 200, epoch: 6 | loss: 0.4425726
	speed: 0.0621s/iter; left time: 394.2454s
Epoch: 6 cost time: 16.272356033325195
Epoch: 6, Steps: 262 | Train Loss: 0.4394624 Vali Loss: 0.9776813 Test Loss: 0.4423341
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4439048
	speed: 0.2827s/iter; left time: 1749.7405s
	iters: 200, epoch: 7 | loss: 0.4324481
	speed: 0.0641s/iter; left time: 390.3104s
Epoch: 7 cost time: 17.543091773986816
Epoch: 7, Steps: 262 | Train Loss: 0.4394329 Vali Loss: 0.9766955 Test Loss: 0.4424675
Validation loss decreased (0.977084 --> 0.976696).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4336120
	speed: 0.3145s/iter; left time: 1863.9089s
	iters: 200, epoch: 8 | loss: 0.4461135
	speed: 0.0793s/iter; left time: 462.1787s
Epoch: 8 cost time: 20.87814974784851
Epoch: 8, Steps: 262 | Train Loss: 0.4394862 Vali Loss: 0.9770360 Test Loss: 0.4423167
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4741222
	speed: 0.2998s/iter; left time: 1698.6352s
	iters: 200, epoch: 9 | loss: 0.4358490
	speed: 0.0600s/iter; left time: 333.6681s
Epoch: 9 cost time: 16.122617721557617
Epoch: 9, Steps: 262 | Train Loss: 0.4394012 Vali Loss: 0.9769475 Test Loss: 0.4424761
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4237137
	speed: 0.2804s/iter; left time: 1514.9651s
	iters: 200, epoch: 10 | loss: 0.4624789
	speed: 0.0628s/iter; left time: 333.1458s
Epoch: 10 cost time: 17.975178956985474
Epoch: 10, Steps: 262 | Train Loss: 0.4393805 Vali Loss: 0.9772561 Test Loss: 0.4425469
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4410251
	speed: 0.3012s/iter; left time: 1548.7242s
	iters: 200, epoch: 11 | loss: 0.3948090
	speed: 0.0684s/iter; left time: 344.9946s
Epoch: 11 cost time: 18.995614290237427
Epoch: 11, Steps: 262 | Train Loss: 0.4392775 Vali Loss: 0.9769946 Test Loss: 0.4425273
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4690793
	speed: 0.2833s/iter; left time: 1382.1096s
	iters: 200, epoch: 12 | loss: 0.4263860
	speed: 0.0584s/iter; left time: 278.9220s
Epoch: 12 cost time: 16.826719760894775
Epoch: 12, Steps: 262 | Train Loss: 0.4394029 Vali Loss: 0.9770156 Test Loss: 0.4422705
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTm1_192_720_FITS_ETTm1_ftM_sl192_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4405673146247864, mae:0.42247211933135986, rse:0.6315051317214966, corr:[0.5315759  0.53240275 0.53055507 0.5326978  0.5294074  0.5299173
 0.5294955  0.52688515 0.5276052  0.5276603  0.5259383  0.5258771
 0.52566344 0.5245962  0.5236057  0.5216331  0.51913595 0.51749307
 0.5157343  0.51311076 0.5108021  0.5087559  0.50662637 0.50429684
 0.50187176 0.49974188 0.49812713 0.4966035  0.49588552 0.49559557
 0.49506095 0.49478972 0.4954227  0.49646464 0.49667966 0.49663574
 0.4962326  0.4960259  0.4962747  0.49599567 0.49545178 0.4955502
 0.49577507 0.4956718  0.4953845  0.49519268 0.49547404 0.495554
 0.49553248 0.49606818 0.4967498  0.49678954 0.49655944 0.49655032
 0.49666318 0.49657008 0.496666   0.496904   0.49693045 0.4965714
 0.49633232 0.49632144 0.49622712 0.4959887  0.4960905  0.4960309
 0.4955755  0.49551392 0.49622312 0.49679363 0.4970434  0.49733824
 0.49743682 0.4973374  0.49750525 0.49765033 0.497577   0.49780473
 0.49816954 0.4980442  0.4976665  0.4979175  0.49845004 0.4985372
 0.49886867 0.49924707 0.4991442  0.49921957 0.49939904 0.49908337
 0.49948764 0.5007459  0.50109416 0.5013604  0.502466   0.5025821
 0.5019625  0.50194496 0.50163585 0.5006119  0.50030625 0.5006677
 0.50042725 0.49969062 0.4992683  0.49921718 0.49927375 0.49962458
 0.49986598 0.49927253 0.49837732 0.49787018 0.49756956 0.4972605
 0.49700153 0.49659452 0.49586126 0.4951563  0.4943153  0.4934238
 0.49333462 0.49353322 0.4930401  0.4923413  0.49176723 0.4908894
 0.49025366 0.4900126  0.48956266 0.4894112  0.48974392 0.48971912
 0.48955798 0.489979   0.48987758 0.48922053 0.48927313 0.48960212
 0.48935112 0.48877433 0.488655   0.48887885 0.48879096 0.48870295
 0.48895472 0.4892959  0.4893381  0.4894497  0.48960757 0.4896363
 0.48973328 0.48957643 0.48911986 0.48957035 0.4899167  0.4893172
 0.48923847 0.4896873  0.48921084 0.4887299  0.4891445  0.48939422
 0.4894965  0.4896908  0.48950222 0.48953193 0.4902431  0.4906463
 0.4902655  0.49019447 0.49081272 0.4911777  0.4907809  0.49061805
 0.491397   0.49213183 0.49186426 0.49135765 0.49165067 0.49246958
 0.49293333 0.49280035 0.49292144 0.4938345  0.49431577 0.49399668
 0.49480093 0.49631655 0.4963747  0.49581817 0.49713737 0.49819022
 0.49802864 0.49801883 0.49800688 0.4974219  0.4965461  0.49580267
 0.4952116  0.4941428  0.4927586  0.49185985 0.49115634 0.48989385
 0.48857325 0.48782137 0.4870993  0.48582163 0.4847003  0.4842393
 0.4834046  0.48214874 0.4813206  0.48090214 0.48005286 0.47886726
 0.47777212 0.47684574 0.47574973 0.4750046  0.4746508  0.4743252
 0.47409874 0.47453508 0.47477168 0.47456083 0.47467655 0.4747809
 0.4743894  0.4739529  0.47411418 0.47423616 0.47401702 0.47367254
 0.47359157 0.47353876 0.47314703 0.47280315 0.47312278 0.47368917
 0.47399816 0.47414836 0.47448516 0.47472656 0.47462708 0.47452235
 0.47463953 0.47463834 0.47442988 0.47428533 0.47419864 0.474483
 0.47463226 0.47436404 0.47438422 0.47495624 0.4749015  0.47437933
 0.47442567 0.47479084 0.47495264 0.47524875 0.47604075 0.476842
 0.47713745 0.47714058 0.47761592 0.47820696 0.4784301  0.47852048
 0.47904754 0.4795336  0.48020977 0.4809872  0.48140466 0.4818707
 0.48268858 0.4831888  0.48324034 0.4838065  0.48447636 0.4845373
 0.48481295 0.4856641  0.48618892 0.48631662 0.48669055 0.48667598
 0.4857802  0.48528782 0.48514193 0.48440632 0.48307166 0.48217413
 0.48171473 0.480944   0.4798766  0.4791469  0.47886923 0.47853678
 0.47796404 0.4772104  0.476477   0.47579947 0.4754478  0.47536072
 0.4750463  0.47428417 0.47364825 0.47341308 0.47304168 0.47261557
 0.47226474 0.4720856  0.47170782 0.47109565 0.47030312 0.4698728
 0.46970737 0.4698063  0.46980122 0.4695924  0.46973243 0.47003424
 0.4696866  0.4691927  0.46921602 0.46956775 0.46941283 0.4691089
 0.4688251  0.46820727 0.46781012 0.46814877 0.46853384 0.46819973
 0.46771514 0.46784756 0.46839526 0.46861136 0.46881348 0.46909693
 0.4691059  0.46872434 0.4686465  0.4692808  0.4696506  0.46944055
 0.46907207 0.46889433 0.46870905 0.46859545 0.46878996 0.46900383
 0.46911675 0.46923393 0.46923533 0.46942392 0.47008124 0.47038025
 0.47015798 0.47030464 0.47078365 0.47085777 0.47080606 0.4712271
 0.47171253 0.47197032 0.47229093 0.4728591  0.4734378  0.4738289
 0.47446525 0.4753953  0.4758305  0.47552958 0.47548473 0.47629818
 0.47713688 0.47761503 0.47796762 0.47844562 0.47894076 0.47968718
 0.4799361  0.47966698 0.47971752 0.47938436 0.4787197  0.47855568
 0.47845936 0.47766352 0.47661018 0.4761078  0.4759612  0.4756164
 0.47499076 0.47435942 0.47393438 0.47371954 0.47343653 0.47294718
 0.47236973 0.47184452 0.47151393 0.47126654 0.47069618 0.46990976
 0.46930414 0.46914035 0.46905267 0.46879578 0.46841845 0.4682088
 0.46840197 0.4686307  0.46846598 0.46798238 0.4678882  0.46799704
 0.467557   0.4674634  0.46754465 0.46712267 0.46660152 0.4667847
 0.46660694 0.46622497 0.46625772 0.4664701  0.4662766  0.4663956
 0.46670505 0.46681696 0.46701697 0.46724877 0.46682024 0.4664566
 0.46701926 0.467681   0.46753597 0.4670541  0.46691364 0.46726105
 0.46728143 0.46695355 0.46698543 0.46712458 0.46687648 0.46682826
 0.4671746  0.46704182 0.46697947 0.46755493 0.46775386 0.4674549
 0.46776494 0.4684876  0.46855944 0.4685036  0.46881947 0.46891356
 0.46879312 0.46899217 0.46920097 0.46930054 0.4694985  0.46972346
 0.47004825 0.4703652  0.47051764 0.47081968 0.47110575 0.47085863
 0.47082308 0.47136968 0.47135183 0.47106925 0.4712725  0.47111052
 0.46963003 0.46894938 0.46940944 0.46853656 0.46683788 0.46634716
 0.4659028  0.4646902  0.46406242 0.46376902 0.46280983 0.461601
 0.46064562 0.45967528 0.4588851  0.45839423 0.45788696 0.45731804
 0.45652384 0.4555614  0.45494315 0.45482814 0.45448023 0.45371273
 0.45334482 0.45310798 0.45254523 0.45200595 0.45179135 0.45197684
 0.4522758  0.4523948  0.45240068 0.45279488 0.45311207 0.45270258
 0.45212695 0.4520316  0.4520468  0.4519833  0.45167622 0.45100328
 0.45082465 0.4512725  0.45119536 0.45087802 0.45109415 0.4510268
 0.45061174 0.45103168 0.45155677 0.45078707 0.45031115 0.45124072
 0.45185503 0.4513599  0.45127863 0.4515792  0.45150852 0.45120257
 0.45113972 0.45098335 0.45075136 0.45075038 0.45080572 0.45060655
 0.45031717 0.45064768 0.45135936 0.45183352 0.45192108 0.45235622
 0.45279053 0.452717   0.45258173 0.45276964 0.4527993  0.4527259
 0.45301792 0.45322022 0.45327917 0.45376098 0.45430186 0.45450538
 0.4547525  0.4550286  0.45500976 0.45514873 0.45545137 0.45539096
 0.45561725 0.4563824  0.45646998 0.45624536 0.45635763 0.4558325
 0.45486963 0.45436358 0.45374906 0.45290443 0.45250192 0.45243904
 0.45183092 0.45069113 0.45005134 0.450008   0.4495336  0.44873932
 0.44824606 0.44752616 0.4465741  0.44612774 0.4457195  0.44497073
 0.44440392 0.4440659  0.44345042 0.44283602 0.44250065 0.44180262
 0.4410215  0.44067222 0.4403751  0.4399461  0.43983135 0.43978602
 0.43966648 0.44000924 0.4403357  0.44030863 0.44041505 0.44081733
 0.44083583 0.44074312 0.44092402 0.44091907 0.44077125 0.44058585
 0.44052088 0.44046116 0.4402514  0.4400893  0.44027683 0.4402927
 0.4399288  0.43997008 0.4403662  0.440344   0.4402844  0.44036382
 0.43999475 0.4394184  0.43945384 0.43970302 0.44002378 0.44055638
 0.44067073 0.44012246 0.4397446  0.43983948 0.43994987 0.43984005
 0.43966088 0.4397417  0.4401593  0.44057167 0.44094244 0.4415697
 0.44186252 0.44172457 0.44221392 0.44292238 0.44299376 0.4431354
 0.44364706 0.4437974  0.44384226 0.44426754 0.4443952  0.44440135
 0.44480488 0.44490635 0.44498694 0.44580963 0.44623423 0.44612634
 0.4469346  0.44785726 0.4479858  0.44815665 0.44849458 0.44797477
 0.44785735 0.448335   0.44808856 0.4476245  0.44722098 0.44617197
 0.44536597 0.4452124  0.44435015 0.44309357 0.44267976 0.4421801
 0.44109514 0.44049212 0.44007412 0.43912172 0.4381449  0.43727985
 0.43676424 0.43681887 0.43637562 0.4352701  0.43460318 0.4339808
 0.4332522  0.4330866  0.43258563 0.4315772  0.43142366 0.431366
 0.43078864 0.43059212 0.43033016 0.42976502 0.42992985 0.43031958
 0.43000215 0.42987496 0.42967173 0.42978415 0.43047023 0.42955238
 0.42918372 0.43054757 0.43046045 0.43130004 0.43254083 0.432963  ]
