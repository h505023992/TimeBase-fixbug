Args in experiment:
Namespace(is_training=1, model_id='ETTh1_96_720', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=40, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_96_720_FITS_ETTh1_ftM_sl96_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=40, out_features=340, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12185600.0
params:  13940.0
Trainable parameters:  13940
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.866101026535034
Epoch: 1, Steps: 61 | Train Loss: 1.5485843 Vali Loss: 3.0593131 Test Loss: 1.6848084
Validation loss decreased (inf --> 3.059313).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.750258684158325
Epoch: 2, Steps: 61 | Train Loss: 1.1413357 Vali Loss: 2.4958222 Test Loss: 1.2139758
Validation loss decreased (3.059313 --> 2.495822).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.568117141723633
Epoch: 3, Steps: 61 | Train Loss: 0.9124227 Vali Loss: 2.1851320 Test Loss: 0.9508551
Validation loss decreased (2.495822 --> 2.185132).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.6247806549072266
Epoch: 4, Steps: 61 | Train Loss: 0.7850346 Vali Loss: 2.0045295 Test Loss: 0.8037471
Validation loss decreased (2.185132 --> 2.004529).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.534569263458252
Epoch: 5, Steps: 61 | Train Loss: 0.7132495 Vali Loss: 1.9001188 Test Loss: 0.7187803
Validation loss decreased (2.004529 --> 1.900119).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.6420226097106934
Epoch: 6, Steps: 61 | Train Loss: 0.6717031 Vali Loss: 1.8343489 Test Loss: 0.6691601
Validation loss decreased (1.900119 --> 1.834349).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.6085381507873535
Epoch: 7, Steps: 61 | Train Loss: 0.6471576 Vali Loss: 1.8057697 Test Loss: 0.6382095
Validation loss decreased (1.834349 --> 1.805770).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.151250123977661
Epoch: 8, Steps: 61 | Train Loss: 0.6312573 Vali Loss: 1.7814524 Test Loss: 0.6176764
Validation loss decreased (1.805770 --> 1.781452).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.411731481552124
Epoch: 9, Steps: 61 | Train Loss: 0.6206823 Vali Loss: 1.7549417 Test Loss: 0.6034049
Validation loss decreased (1.781452 --> 1.754942).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2381086349487305
Epoch: 10, Steps: 61 | Train Loss: 0.6130480 Vali Loss: 1.7442609 Test Loss: 0.5920905
Validation loss decreased (1.754942 --> 1.744261).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.121393918991089
Epoch: 11, Steps: 61 | Train Loss: 0.6068770 Vali Loss: 1.7363744 Test Loss: 0.5830708
Validation loss decreased (1.744261 --> 1.736374).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.1679341793060303
Epoch: 12, Steps: 61 | Train Loss: 0.6023716 Vali Loss: 1.7212071 Test Loss: 0.5752116
Validation loss decreased (1.736374 --> 1.721207).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.0162253379821777
Epoch: 13, Steps: 61 | Train Loss: 0.5983746 Vali Loss: 1.7115284 Test Loss: 0.5685447
Validation loss decreased (1.721207 --> 1.711528).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.116837739944458
Epoch: 14, Steps: 61 | Train Loss: 0.5947153 Vali Loss: 1.7075986 Test Loss: 0.5624049
Validation loss decreased (1.711528 --> 1.707599).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.1288044452667236
Epoch: 15, Steps: 61 | Train Loss: 0.5917164 Vali Loss: 1.6977007 Test Loss: 0.5569432
Validation loss decreased (1.707599 --> 1.697701).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.2030155658721924
Epoch: 16, Steps: 61 | Train Loss: 0.5886381 Vali Loss: 1.6918100 Test Loss: 0.5522248
Validation loss decreased (1.697701 --> 1.691810).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.3384249210357666
Epoch: 17, Steps: 61 | Train Loss: 0.5860887 Vali Loss: 1.6855175 Test Loss: 0.5476494
Validation loss decreased (1.691810 --> 1.685518).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.2727296352386475
Epoch: 18, Steps: 61 | Train Loss: 0.5838559 Vali Loss: 1.6766646 Test Loss: 0.5435476
Validation loss decreased (1.685518 --> 1.676665).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.295515537261963
Epoch: 19, Steps: 61 | Train Loss: 0.5815598 Vali Loss: 1.6732521 Test Loss: 0.5398583
Validation loss decreased (1.676665 --> 1.673252).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.8723392486572266
Epoch: 20, Steps: 61 | Train Loss: 0.5795923 Vali Loss: 1.6693776 Test Loss: 0.5363140
Validation loss decreased (1.673252 --> 1.669378).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.8079540729522705
Epoch: 21, Steps: 61 | Train Loss: 0.5777702 Vali Loss: 1.6655644 Test Loss: 0.5330817
Validation loss decreased (1.669378 --> 1.665564).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.86991286277771
Epoch: 22, Steps: 61 | Train Loss: 0.5759385 Vali Loss: 1.6594641 Test Loss: 0.5301504
Validation loss decreased (1.665564 --> 1.659464).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.844478130340576
Epoch: 23, Steps: 61 | Train Loss: 0.5747055 Vali Loss: 1.6642063 Test Loss: 0.5274108
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.773752450942993
Epoch: 24, Steps: 61 | Train Loss: 0.5730539 Vali Loss: 1.6512311 Test Loss: 0.5249245
Validation loss decreased (1.659464 --> 1.651231).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.8979976177215576
Epoch: 25, Steps: 61 | Train Loss: 0.5716346 Vali Loss: 1.6567371 Test Loss: 0.5225503
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.9187283515930176
Epoch: 26, Steps: 61 | Train Loss: 0.5705988 Vali Loss: 1.6501199 Test Loss: 0.5203841
Validation loss decreased (1.651231 --> 1.650120).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.959463119506836
Epoch: 27, Steps: 61 | Train Loss: 0.5692527 Vali Loss: 1.6418204 Test Loss: 0.5183530
Validation loss decreased (1.650120 --> 1.641820).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.0189499855041504
Epoch: 28, Steps: 61 | Train Loss: 0.5684411 Vali Loss: 1.6384640 Test Loss: 0.5165100
Validation loss decreased (1.641820 --> 1.638464).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.8472611904144287
Epoch: 29, Steps: 61 | Train Loss: 0.5670063 Vali Loss: 1.6413096 Test Loss: 0.5146986
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.7904438972473145
Epoch: 30, Steps: 61 | Train Loss: 0.5661232 Vali Loss: 1.6392906 Test Loss: 0.5131273
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00011296777049628277
train 7825
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=40, out_features=340, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12185600.0
params:  13940.0
Trainable parameters:  13940
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.6556544303894043
Epoch: 1, Steps: 61 | Train Loss: 0.6321764 Vali Loss: 1.6102133 Test Loss: 0.4949381
Validation loss decreased (inf --> 1.610213).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.6347312927246094
Epoch: 2, Steps: 61 | Train Loss: 0.6229824 Vali Loss: 1.6033418 Test Loss: 0.4820488
Validation loss decreased (1.610213 --> 1.603342).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.520214796066284
Epoch: 3, Steps: 61 | Train Loss: 0.6169255 Vali Loss: 1.5877720 Test Loss: 0.4743052
Validation loss decreased (1.603342 --> 1.587772).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.5384433269500732
Epoch: 4, Steps: 61 | Train Loss: 0.6128552 Vali Loss: 1.5744456 Test Loss: 0.4698010
Validation loss decreased (1.587772 --> 1.574446).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.469526767730713
Epoch: 5, Steps: 61 | Train Loss: 0.6102545 Vali Loss: 1.5750697 Test Loss: 0.4674180
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.577576160430908
Epoch: 6, Steps: 61 | Train Loss: 0.6088246 Vali Loss: 1.5688633 Test Loss: 0.4661372
Validation loss decreased (1.574446 --> 1.568863).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.4822840690612793
Epoch: 7, Steps: 61 | Train Loss: 0.6074064 Vali Loss: 1.5639164 Test Loss: 0.4655323
Validation loss decreased (1.568863 --> 1.563916).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.501746892929077
Epoch: 8, Steps: 61 | Train Loss: 0.6068100 Vali Loss: 1.5633249 Test Loss: 0.4653718
Validation loss decreased (1.563916 --> 1.563325).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.580244302749634
Epoch: 9, Steps: 61 | Train Loss: 0.6062039 Vali Loss: 1.5572500 Test Loss: 0.4653871
Validation loss decreased (1.563325 --> 1.557250).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.5392582416534424
Epoch: 10, Steps: 61 | Train Loss: 0.6057587 Vali Loss: 1.5691710 Test Loss: 0.4654593
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.673452138900757
Epoch: 11, Steps: 61 | Train Loss: 0.6058726 Vali Loss: 1.5604535 Test Loss: 0.4657832
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.6310017108917236
Epoch: 12, Steps: 61 | Train Loss: 0.6055142 Vali Loss: 1.5537832 Test Loss: 0.4659401
Validation loss decreased (1.557250 --> 1.553783).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.590989112854004
Epoch: 13, Steps: 61 | Train Loss: 0.6055128 Vali Loss: 1.5588932 Test Loss: 0.4661388
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.5019826889038086
Epoch: 14, Steps: 61 | Train Loss: 0.6051706 Vali Loss: 1.5634055 Test Loss: 0.4663012
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.6436398029327393
Epoch: 15, Steps: 61 | Train Loss: 0.6050343 Vali Loss: 1.5643234 Test Loss: 0.4664684
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.953211545944214
Epoch: 16, Steps: 61 | Train Loss: 0.6053772 Vali Loss: 1.5684024 Test Loss: 0.4666186
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.1008214950561523
Epoch: 17, Steps: 61 | Train Loss: 0.6050031 Vali Loss: 1.5531559 Test Loss: 0.4667564
Validation loss decreased (1.553783 --> 1.553156).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.744114398956299
Epoch: 18, Steps: 61 | Train Loss: 0.6051451 Vali Loss: 1.5610083 Test Loss: 0.4668365
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.74843692779541
Epoch: 19, Steps: 61 | Train Loss: 0.6049513 Vali Loss: 1.5643976 Test Loss: 0.4669199
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.3297863006591797
Epoch: 20, Steps: 61 | Train Loss: 0.6050357 Vali Loss: 1.5570741 Test Loss: 0.4670278
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.341317653656006
Epoch: 21, Steps: 61 | Train Loss: 0.6047665 Vali Loss: 1.5637300 Test Loss: 0.4670939
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.616973638534546
Epoch: 22, Steps: 61 | Train Loss: 0.6049087 Vali Loss: 1.5620463 Test Loss: 0.4671727
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_96_720_FITS_ETTh1_ftM_sl96_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.46552035212516785, mae:0.4591066539287567, rse:0.6531625986099243, corr:[0.22956695 0.22890483 0.23059036 0.22903402 0.22801618 0.2254516
 0.22367984 0.22535829 0.22458357 0.22537994 0.2252087  0.22456391
 0.22507916 0.22437231 0.22390945 0.22341084 0.22306718 0.22346012
 0.22327897 0.22324032 0.22340676 0.22354658 0.22381258 0.22544442
 0.22530252 0.22455135 0.22382978 0.22337681 0.2230693  0.22236103
 0.22172418 0.22094467 0.22060405 0.22040081 0.21978469 0.21995546
 0.21990651 0.21951964 0.21990865 0.21958548 0.21972303 0.22022592
 0.22037554 0.22062622 0.22105172 0.22140743 0.22166172 0.22278823
 0.22351551 0.22301619 0.22206576 0.22076541 0.21957609 0.2181557
 0.21683101 0.21592315 0.21538064 0.21529193 0.21487592 0.21460803
 0.21456358 0.21456654 0.21437912 0.21401341 0.21395358 0.21440338
 0.21482563 0.21530686 0.2156983  0.21594034 0.21598965 0.21607412
 0.21560575 0.21483341 0.2137515  0.21322638 0.21313079 0.21272744
 0.21249023 0.21196474 0.21150286 0.21116205 0.21067956 0.21075585
 0.21040529 0.21023539 0.21058995 0.21038942 0.21020521 0.21042378
 0.2103848  0.21029359 0.21026677 0.21041735 0.21110474 0.21240084
 0.21320146 0.21329178 0.21319501 0.21281001 0.21267939 0.21245462
 0.2117567  0.21114582 0.21112661 0.21050012 0.21009016 0.20976514
 0.20932062 0.2092814  0.20968837 0.20965454 0.21009721 0.21063888
 0.21084826 0.21116304 0.2113327  0.211307   0.21159475 0.21190463
 0.21177308 0.21134405 0.21061544 0.20943889 0.20866865 0.20830342
 0.20785032 0.20748183 0.20737813 0.20692836 0.20617019 0.20601301
 0.20575128 0.20540121 0.20574947 0.20573354 0.20571847 0.20603763
 0.20628063 0.20661187 0.20691396 0.20717157 0.2076148  0.20802683
 0.2078839  0.20752424 0.20690921 0.2055237  0.20451534 0.20358951
 0.20299408 0.20272642 0.2028407  0.20310156 0.20295943 0.20292465
 0.20274054 0.2024089  0.20235167 0.20215562 0.20229308 0.2025671
 0.2030048  0.20332016 0.2032956  0.20382077 0.2042932  0.20436624
 0.20439458 0.20405303 0.20354947 0.20307678 0.20255716 0.20174879
 0.20100173 0.20050003 0.20056415 0.20046428 0.20042467 0.20070644
 0.20058908 0.20022567 0.20044415 0.19991378 0.19972669 0.20051363
 0.20095904 0.20162462 0.20236094 0.20264173 0.20307387 0.20345956
 0.20247523 0.20173867 0.20109057 0.19969466 0.19891937 0.1984027
 0.19778337 0.1972593  0.19710143 0.19691211 0.19651107 0.19666941
 0.19635786 0.19629791 0.1964467  0.1962288  0.19619812 0.1967868
 0.19719493 0.1973527  0.197719   0.19854508 0.19925882 0.19981168
 0.20043285 0.20055234 0.20024677 0.199848   0.19947675 0.19895276
 0.19865921 0.19840485 0.19824894 0.19802795 0.19727965 0.19703954
 0.19711562 0.19680285 0.19692348 0.19675046 0.19675289 0.19717771
 0.19727054 0.19744089 0.19773272 0.1979005  0.19822864 0.1983677
 0.19848745 0.19850098 0.19795246 0.19736148 0.19707012 0.19669361
 0.19642328 0.19616501 0.1958959  0.1956131  0.19500808 0.19455232
 0.19463992 0.19445994 0.19444996 0.19447728 0.19451152 0.19475916
 0.19516142 0.1955726  0.19591415 0.19649823 0.19696157 0.19734676
 0.1977628  0.19793512 0.19786945 0.19749023 0.1970993  0.19658251
 0.19609426 0.19618824 0.1960316  0.19558279 0.19560331 0.19552861
 0.19509922 0.19545715 0.19537425 0.19487491 0.19491635 0.19489731
 0.19498281 0.19543186 0.1959054  0.19640811 0.19688389 0.19799377
 0.19862504 0.19852588 0.19865322 0.19875452 0.19861387 0.19860117
 0.1984341  0.19847669 0.19865093 0.19855145 0.19844055 0.19805731
 0.19784015 0.19784915 0.19790468 0.19768688 0.19777618 0.1980388
 0.19839263 0.19851595 0.19851802 0.19834512 0.19850734 0.19871256
 0.19863616 0.19875593 0.1987627  0.1980756  0.19760151 0.19701034
 0.19642238 0.19636735 0.19621216 0.19550979 0.19498722 0.19517832
 0.19503768 0.19503771 0.19504455 0.19499066 0.19512913 0.19553249
 0.19607836 0.19650896 0.19657022 0.19673507 0.19708078 0.19709447
 0.1971066  0.19746155 0.19708334 0.19631435 0.19598581 0.19538058
 0.1948414  0.19464798 0.19470999 0.19459642 0.19429019 0.19455346
 0.19468397 0.1944827  0.19458173 0.19460279 0.194556   0.19477804
 0.19523682 0.1956166  0.1955579  0.19545346 0.19554855 0.19530371
 0.19467893 0.19484758 0.19464186 0.19376606 0.19333486 0.19256534
 0.19192775 0.19163299 0.1912286  0.19086471 0.19069573 0.1904088
 0.19013458 0.19000775 0.18989211 0.18971272 0.18979831 0.19013835
 0.19060548 0.19119142 0.19121014 0.19138502 0.19215359 0.19317399
 0.19416314 0.19508454 0.19539492 0.19505143 0.19494747 0.19446735
 0.19356976 0.19295605 0.19275872 0.19230741 0.19167668 0.19166523
 0.19152452 0.1915603  0.19190057 0.19177908 0.19158979 0.1923087
 0.19307606 0.1934588  0.1940336  0.19439897 0.19469006 0.1956632
 0.19628268 0.19681095 0.19719097 0.19681287 0.1965288  0.19626257
 0.19600159 0.19589843 0.19593817 0.19583535 0.1954672  0.19536422
 0.1952031  0.19497697 0.1951752  0.1950769  0.19478056 0.19517383
 0.1956859  0.19588009 0.19594419 0.19669206 0.19714977 0.19778347
 0.19881937 0.1995511  0.19963476 0.19921584 0.19923238 0.19884235
 0.19836636 0.19870849 0.19886227 0.19856699 0.19825344 0.19829988
 0.19845659 0.19827883 0.19833077 0.19830596 0.19818118 0.19842894
 0.1987422  0.199117   0.19955745 0.19971329 0.19987726 0.20123366
 0.20253676 0.20328355 0.2033813  0.20270681 0.20225765 0.20173311
 0.20110744 0.20111388 0.20124735 0.2011894  0.20164534 0.2018636
 0.20153612 0.20162791 0.20198305 0.20203379 0.2020798  0.2021606
 0.2023225  0.20264064 0.20282558 0.20263197 0.20292394 0.20378587
 0.20405883 0.20418863 0.20367324 0.20274158 0.20250317 0.20205568
 0.20184675 0.20204881 0.20183893 0.20174807 0.20168106 0.20134062
 0.20125604 0.20124248 0.20088668 0.20058595 0.20078203 0.20054418
 0.20085602 0.20166904 0.20203552 0.20244445 0.20302215 0.20428437
 0.20543954 0.2062587  0.20645659 0.2057312  0.2054449  0.20522158
 0.20461506 0.2040717  0.20379527 0.20338735 0.20292804 0.20301132
 0.20319462 0.20310842 0.20340951 0.20339935 0.20354816 0.20408218
 0.20430398 0.20453109 0.20522033 0.20619144 0.20674896 0.20785257
 0.20839202 0.20800887 0.20750025 0.2067241  0.20628643 0.20559584
 0.20450649 0.20426269 0.20437093 0.20415823 0.20404598 0.20399064
 0.20408314 0.20419352 0.20447262 0.20461546 0.20479871 0.20508973
 0.20526697 0.20561525 0.2059688  0.20607151 0.20632854 0.20656244
 0.2061231  0.20554164 0.20499416 0.20416668 0.20339681 0.20264654
 0.20210998 0.2016156  0.20133084 0.20102654 0.20054805 0.20058322
 0.20055692 0.20016605 0.20045121 0.20049606 0.2001822  0.20058537
 0.20124553 0.20176065 0.20232865 0.20268376 0.20324096 0.20343734
 0.20310804 0.20289792 0.20245688 0.20171064 0.20105465 0.20026529
 0.19941317 0.19894075 0.19895169 0.19895747 0.19871256 0.19887155
 0.19895291 0.19875176 0.19892833 0.19873455 0.19860403 0.19893183
 0.19895588 0.199054   0.1993773  0.19949253 0.19986925 0.20010397
 0.1994711  0.19890743 0.19858694 0.19773996 0.19702545 0.19625494
 0.19548166 0.19516973 0.19501363 0.19474706 0.194667   0.19479592
 0.19467455 0.19457912 0.19439523 0.19440489 0.19440241 0.19439794
 0.1943602  0.19475932 0.19527629 0.19564883 0.19601443 0.19647475
 0.19612683 0.19558023 0.19499315 0.19385317 0.19296522 0.19218808
 0.19160457 0.19112413 0.19067186 0.19056375 0.19031432 0.19016558
 0.19019774 0.18988633 0.18964413 0.18964368 0.18941526 0.18975279
 0.19003206 0.19039583 0.1910203  0.19150598 0.19206269 0.19273798
 0.1925963  0.19209245 0.1911295  0.18980503 0.18909886 0.18839633
 0.18753062 0.18681104 0.18629475 0.18609066 0.18597189 0.18586369
 0.18575467 0.18559048 0.18551466 0.18523382 0.18508454 0.18529661
 0.18536127 0.18530187 0.18514892 0.18495847 0.1851104  0.18452646
 0.1820355  0.1796504  0.17787671 0.17598562 0.17442329 0.17294022
 0.17186151 0.17160246 0.17174768 0.17167065 0.17161132 0.17173791
 0.1720129  0.17176764 0.17157724 0.1717455  0.17176124 0.17202592
 0.17228353 0.17266665 0.17284825 0.17311676 0.17366168 0.17346942
 0.17282495 0.17189287 0.17075422 0.16984627 0.16845089 0.16713838
 0.16634808 0.16556196 0.16571687 0.16556188 0.16488948 0.16518344
 0.16549489 0.1651345  0.16542934 0.16438752 0.16464485 0.16502857
 0.16493864 0.16583194 0.16512676 0.16629834 0.16461927 0.16866224]
