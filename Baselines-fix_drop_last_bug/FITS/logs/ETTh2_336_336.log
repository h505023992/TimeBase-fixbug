Args in experiment:
Namespace(is_training=1, model_id='ETTh2_336_336', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=100, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_336_336_FITS_ETTh2_ftM_sl336_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7969
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=100, out_features=200, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17920000.0
params:  20200.0
Trainable parameters:  20200
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.609555959701538
Epoch: 1, Steps: 62 | Train Loss: 0.6691955 Vali Loss: 0.5076756 Test Loss: 0.4610167
Validation loss decreased (inf --> 0.507676).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.3289995193481445
Epoch: 2, Steps: 62 | Train Loss: 0.5402291 Vali Loss: 0.4638619 Test Loss: 0.4253543
Validation loss decreased (0.507676 --> 0.463862).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.075775146484375
Epoch: 3, Steps: 62 | Train Loss: 0.4756434 Vali Loss: 0.4430624 Test Loss: 0.4096691
Validation loss decreased (0.463862 --> 0.443062).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.6915030479431152
Epoch: 4, Steps: 62 | Train Loss: 0.4419305 Vali Loss: 0.4304848 Test Loss: 0.4023627
Validation loss decreased (0.443062 --> 0.430485).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.1980040073394775
Epoch: 5, Steps: 62 | Train Loss: 0.4200867 Vali Loss: 0.4201566 Test Loss: 0.3987098
Validation loss decreased (0.430485 --> 0.420157).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.248270034790039
Epoch: 6, Steps: 62 | Train Loss: 0.4047820 Vali Loss: 0.4168377 Test Loss: 0.3962962
Validation loss decreased (0.420157 --> 0.416838).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3218181133270264
Epoch: 7, Steps: 62 | Train Loss: 0.3936651 Vali Loss: 0.4134494 Test Loss: 0.3947664
Validation loss decreased (0.416838 --> 0.413449).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.340391159057617
Epoch: 8, Steps: 62 | Train Loss: 0.3843799 Vali Loss: 0.4090553 Test Loss: 0.3933382
Validation loss decreased (0.413449 --> 0.409055).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9832286834716797
Epoch: 9, Steps: 62 | Train Loss: 0.3769804 Vali Loss: 0.4086386 Test Loss: 0.3918487
Validation loss decreased (0.409055 --> 0.408639).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.1361701488494873
Epoch: 10, Steps: 62 | Train Loss: 0.3711334 Vali Loss: 0.4035069 Test Loss: 0.3906983
Validation loss decreased (0.408639 --> 0.403507).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.2280735969543457
Epoch: 11, Steps: 62 | Train Loss: 0.3658242 Vali Loss: 0.4032529 Test Loss: 0.3894476
Validation loss decreased (0.403507 --> 0.403253).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.314945220947266
Epoch: 12, Steps: 62 | Train Loss: 0.3612314 Vali Loss: 0.4032307 Test Loss: 0.3884318
Validation loss decreased (0.403253 --> 0.403231).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.214966297149658
Epoch: 13, Steps: 62 | Train Loss: 0.3572842 Vali Loss: 0.4013070 Test Loss: 0.3874792
Validation loss decreased (0.403231 --> 0.401307).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.2157039642333984
Epoch: 14, Steps: 62 | Train Loss: 0.3533538 Vali Loss: 0.4004410 Test Loss: 0.3864661
Validation loss decreased (0.401307 --> 0.400441).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.136627912521362
Epoch: 15, Steps: 62 | Train Loss: 0.3505170 Vali Loss: 0.3975938 Test Loss: 0.3856323
Validation loss decreased (0.400441 --> 0.397594).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.3710904121398926
Epoch: 16, Steps: 62 | Train Loss: 0.3483121 Vali Loss: 0.3954874 Test Loss: 0.3847527
Validation loss decreased (0.397594 --> 0.395487).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.5044634342193604
Epoch: 17, Steps: 62 | Train Loss: 0.3456950 Vali Loss: 0.3936557 Test Loss: 0.3841006
Validation loss decreased (0.395487 --> 0.393656).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.5673906803131104
Epoch: 18, Steps: 62 | Train Loss: 0.3432432 Vali Loss: 0.3960088 Test Loss: 0.3832384
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.3104541301727295
Epoch: 19, Steps: 62 | Train Loss: 0.3411671 Vali Loss: 0.3947224 Test Loss: 0.3826963
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.0281589031219482
Epoch: 20, Steps: 62 | Train Loss: 0.3399657 Vali Loss: 0.3938052 Test Loss: 0.3820530
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.777043104171753
Epoch: 21, Steps: 62 | Train Loss: 0.3377303 Vali Loss: 0.3935988 Test Loss: 0.3813139
Validation loss decreased (0.393656 --> 0.393599).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.2008056640625
Epoch: 22, Steps: 62 | Train Loss: 0.3360003 Vali Loss: 0.3904442 Test Loss: 0.3808397
Validation loss decreased (0.393599 --> 0.390444).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.2918131351470947
Epoch: 23, Steps: 62 | Train Loss: 0.3351288 Vali Loss: 0.3887193 Test Loss: 0.3804358
Validation loss decreased (0.390444 --> 0.388719).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.129542350769043
Epoch: 24, Steps: 62 | Train Loss: 0.3344321 Vali Loss: 0.3897913 Test Loss: 0.3798569
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2170960903167725
Epoch: 25, Steps: 62 | Train Loss: 0.3333477 Vali Loss: 0.3875683 Test Loss: 0.3794065
Validation loss decreased (0.388719 --> 0.387568).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.3546204566955566
Epoch: 26, Steps: 62 | Train Loss: 0.3323515 Vali Loss: 0.3900527 Test Loss: 0.3789490
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.4901046752929688
Epoch: 27, Steps: 62 | Train Loss: 0.3316956 Vali Loss: 0.3882842 Test Loss: 0.3785758
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.3626091480255127
Epoch: 28, Steps: 62 | Train Loss: 0.3309388 Vali Loss: 0.3887900 Test Loss: 0.3781637
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.460939884185791
Epoch: 29, Steps: 62 | Train Loss: 0.3303159 Vali Loss: 0.3880264 Test Loss: 0.3778346
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.345489263534546
Epoch: 30, Steps: 62 | Train Loss: 0.3287437 Vali Loss: 0.3875299 Test Loss: 0.3775280
Validation loss decreased (0.387568 --> 0.387530).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 7969
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=100, out_features=200, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17920000.0
params:  20200.0
Trainable parameters:  20200
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.1802968978881836
Epoch: 1, Steps: 62 | Train Loss: 0.6210130 Vali Loss: 0.3826430 Test Loss: 0.3708586
Validation loss decreased (inf --> 0.382643).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.3791329860687256
Epoch: 2, Steps: 62 | Train Loss: 0.6149771 Vali Loss: 0.3789425 Test Loss: 0.3676587
Validation loss decreased (0.382643 --> 0.378942).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.3916375637054443
Epoch: 3, Steps: 62 | Train Loss: 0.6103977 Vali Loss: 0.3767644 Test Loss: 0.3660704
Validation loss decreased (0.378942 --> 0.376764).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.125291109085083
Epoch: 4, Steps: 62 | Train Loss: 0.6104077 Vali Loss: 0.3740058 Test Loss: 0.3646390
Validation loss decreased (0.376764 --> 0.374006).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.434260368347168
Epoch: 5, Steps: 62 | Train Loss: 0.6078880 Vali Loss: 0.3751285 Test Loss: 0.3643730
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.316928386688232
Epoch: 6, Steps: 62 | Train Loss: 0.6067721 Vali Loss: 0.3742194 Test Loss: 0.3637354
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3986265659332275
Epoch: 7, Steps: 62 | Train Loss: 0.6053660 Vali Loss: 0.3717717 Test Loss: 0.3636647
Validation loss decreased (0.374006 --> 0.371772).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.3174889087677
Epoch: 8, Steps: 62 | Train Loss: 0.6067154 Vali Loss: 0.3727701 Test Loss: 0.3636485
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.3663859367370605
Epoch: 9, Steps: 62 | Train Loss: 0.6062646 Vali Loss: 0.3717150 Test Loss: 0.3635160
Validation loss decreased (0.371772 --> 0.371715).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.386878252029419
Epoch: 10, Steps: 62 | Train Loss: 0.6051924 Vali Loss: 0.3719401 Test Loss: 0.3634376
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.142483949661255
Epoch: 11, Steps: 62 | Train Loss: 0.6059394 Vali Loss: 0.3696378 Test Loss: 0.3632782
Validation loss decreased (0.371715 --> 0.369638).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.1911308765411377
Epoch: 12, Steps: 62 | Train Loss: 0.6060951 Vali Loss: 0.3735715 Test Loss: 0.3633894
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.2877705097198486
Epoch: 13, Steps: 62 | Train Loss: 0.6049925 Vali Loss: 0.3699093 Test Loss: 0.3634114
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.895970344543457
Epoch: 14, Steps: 62 | Train Loss: 0.6054987 Vali Loss: 0.3698916 Test Loss: 0.3633261
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.303654432296753
Epoch: 15, Steps: 62 | Train Loss: 0.6048201 Vali Loss: 0.3712903 Test Loss: 0.3633851
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.231431722640991
Epoch: 16, Steps: 62 | Train Loss: 0.6053518 Vali Loss: 0.3711985 Test Loss: 0.3634028
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh2_336_336_FITS_ETTh2_ftM_sl336_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3587808907032013, mae:0.3968431055545807, rse:0.47891032695770264, corr:[0.26405615 0.2663614  0.26428065 0.26500317 0.26339224 0.26132888
 0.26121765 0.26028612 0.25851277 0.25760508 0.25665963 0.25489125
 0.25351694 0.2524285  0.2515888  0.25119936 0.25045776 0.24919514
 0.24810629 0.24704146 0.24570207 0.24490482 0.24417171 0.24238205
 0.24007785 0.23841698 0.23700832 0.23571828 0.23506464 0.23451781
 0.23333333 0.23180951 0.23063882 0.22958204 0.2283238  0.22689195
 0.22560537 0.22506648 0.22464067 0.22340082 0.22209573 0.22159897
 0.22100572 0.21965663 0.21826018 0.21724704 0.2158745  0.2137529
 0.21167895 0.20970514 0.20779768 0.20620696 0.20497432 0.20344865
 0.2014422  0.19972324 0.19818342 0.19653423 0.1950974  0.19382101
 0.19305652 0.19291909 0.19323848 0.19288172 0.19197695 0.19128978
 0.1908666  0.1906441  0.19010843 0.18932474 0.18865527 0.18817474
 0.18691663 0.18553887 0.18502776 0.1846729  0.18369024 0.18284747
 0.18241242 0.18182068 0.18167362 0.18168347 0.18113989 0.18035585
 0.18034706 0.18047282 0.17973948 0.1788097  0.17840903 0.17794748
 0.17727582 0.17727937 0.17767276 0.17737341 0.17683412 0.17652549
 0.17655143 0.176763   0.17662656 0.17568275 0.1749083  0.17461786
 0.17394927 0.17283568 0.17328352 0.17440303 0.17476043 0.17444368
 0.1741626  0.173202   0.1719385  0.17181654 0.17172557 0.17096893
 0.17037515 0.17009155 0.16916376 0.16796319 0.16784245 0.16733676
 0.16539189 0.16369314 0.16333309 0.16291247 0.16213441 0.1614854
 0.1600838  0.15865135 0.15873688 0.15886551 0.15755837 0.15641099
 0.15618607 0.15590821 0.15553565 0.15568471 0.15515971 0.154172
 0.15430146 0.15469456 0.15400098 0.15287927 0.15242869 0.15075475
 0.14833894 0.1478975  0.14862913 0.14796598 0.14684245 0.14658885
 0.14580008 0.14456296 0.14477584 0.14518149 0.14447226 0.14403881
 0.14443652 0.14383972 0.14280345 0.14303482 0.14348732 0.14303128
 0.14219472 0.14222206 0.14299256 0.14368783 0.1431043  0.1413024
 0.1403862  0.140267   0.13939671 0.13859777 0.13886285 0.13800304
 0.13566475 0.13438524 0.13514154 0.1348417  0.13342836 0.13349676
 0.13373002 0.13234507 0.1310153  0.13184682 0.13265853 0.13237852
 0.13199434 0.13197036 0.13252895 0.13354978 0.1337295  0.1322969
 0.13113713 0.13095365 0.13095978 0.13146348 0.13221593 0.13181941
 0.13029948 0.12976117 0.13073844 0.13143554 0.13105835 0.13021098
 0.13000676 0.13012418 0.12981881 0.12934484 0.12974787 0.13096628
 0.13128306 0.13043462 0.13044725 0.13142237 0.13138081 0.12957603
 0.12780795 0.12697627 0.12717676 0.12724966 0.1262906  0.1254962
 0.12583204 0.12652414 0.12593785 0.12536621 0.12591909 0.12609498
 0.12511119 0.12349995 0.12263104 0.12323602 0.12416582 0.12449429
 0.12390372 0.123468   0.12378053 0.12442952 0.12462702 0.12402657
 0.12299781 0.12199263 0.12156553 0.12145762 0.12081947 0.12020126
 0.11989289 0.11974543 0.11970095 0.11949322 0.11924529 0.11926949
 0.11966125 0.11947595 0.11846987 0.11960631 0.12180237 0.12250958
 0.12171198 0.12209574 0.1231179  0.12325402 0.12371261 0.12430542
 0.1238068  0.1233798  0.12479462 0.12556288 0.12473905 0.12408058
 0.1242151  0.12381036 0.12352733 0.12431528 0.125147   0.12431289
 0.12331983 0.12308837 0.12321905 0.12369046 0.1243488  0.12406913
 0.12340458 0.12317359 0.12320966 0.12314896 0.12377226 0.12402986
 0.12318584 0.12255526 0.12246604 0.12146178 0.1198441  0.11964716
 0.11966847 0.11878636 0.11823687 0.11844877 0.11845744 0.11723687
 0.11663839 0.11696021 0.11679988 0.11683881 0.11704699 0.11788099
 0.11825304 0.11855301 0.11791942 0.11774582 0.11873055 0.11855703
 0.11668088 0.11536455 0.11469796 0.11393569 0.11393452 0.11414189
 0.11250793 0.11173071 0.11304659 0.11322326 0.11216788 0.11256337
 0.11320611 0.1122317  0.1118561  0.11246892 0.1120542  0.11232935
 0.1120517  0.11110238 0.1133303  0.11344423 0.11221331 0.12212692]
