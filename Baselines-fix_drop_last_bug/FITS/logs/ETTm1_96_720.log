Args in experiment:
Namespace(is_training=1, model_id='ETTm1_96_720', model='FITS', data='ETTm1', root_path='./dataset/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=40, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_96_720_FITS_ETTm1_ftM_sl96_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=40, out_features=340, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12185600.0
params:  13940.0
Trainable parameters:  13940
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8992332
	speed: 0.0857s/iter; left time: 667.8918s
	iters: 200, epoch: 1 | loss: 0.6621092
	speed: 0.0643s/iter; left time: 494.6188s
Epoch: 1 cost time: 19.30525279045105
Epoch: 1, Steps: 263 | Train Loss: 0.8828728 Vali Loss: 1.2929476 Test Loss: 0.7521456
Validation loss decreased (inf --> 1.292948).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5157762
	speed: 0.3343s/iter; left time: 2516.7026s
	iters: 200, epoch: 2 | loss: 0.5083885
	speed: 0.0694s/iter; left time: 515.6965s
Epoch: 2 cost time: 19.73438549041748
Epoch: 2, Steps: 263 | Train Loss: 0.4993328 Vali Loss: 1.1055849 Test Loss: 0.5754462
Validation loss decreased (1.292948 --> 1.105585).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4675107
	speed: 0.2856s/iter; left time: 2075.1376s
	iters: 200, epoch: 3 | loss: 0.4690460
	speed: 0.0567s/iter; left time: 406.0044s
Epoch: 3 cost time: 16.853450059890747
Epoch: 3, Steps: 263 | Train Loss: 0.4562803 Vali Loss: 1.0588238 Test Loss: 0.5337756
Validation loss decreased (1.105585 --> 1.058824).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4322261
	speed: 0.3042s/iter; left time: 2129.9110s
	iters: 200, epoch: 4 | loss: 0.4419915
	speed: 0.0652s/iter; left time: 449.9861s
Epoch: 4 cost time: 18.26938772201538
Epoch: 4, Steps: 263 | Train Loss: 0.4438945 Vali Loss: 1.0368322 Test Loss: 0.5132626
Validation loss decreased (1.058824 --> 1.036832).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4119392
	speed: 0.3049s/iter; left time: 2054.8453s
	iters: 200, epoch: 5 | loss: 0.4450144
	speed: 0.0655s/iter; left time: 434.9381s
Epoch: 5 cost time: 19.26555609703064
Epoch: 5, Steps: 263 | Train Loss: 0.4383707 Vali Loss: 1.0248845 Test Loss: 0.5025186
Validation loss decreased (1.036832 --> 1.024884).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4307017
	speed: 0.3028s/iter; left time: 1961.1188s
	iters: 200, epoch: 6 | loss: 0.4219052
	speed: 0.0751s/iter; left time: 479.1133s
Epoch: 6 cost time: 18.904393911361694
Epoch: 6, Steps: 263 | Train Loss: 0.4355717 Vali Loss: 1.0184004 Test Loss: 0.4968352
Validation loss decreased (1.024884 --> 1.018400).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4497854
	speed: 0.3044s/iter; left time: 1891.0653s
	iters: 200, epoch: 7 | loss: 0.4329635
	speed: 0.0647s/iter; left time: 395.6325s
Epoch: 7 cost time: 18.004401445388794
Epoch: 7, Steps: 263 | Train Loss: 0.4341831 Vali Loss: 1.0149419 Test Loss: 0.4935128
Validation loss decreased (1.018400 --> 1.014942).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4109567
	speed: 0.3033s/iter; left time: 1804.7710s
	iters: 200, epoch: 8 | loss: 0.3935523
	speed: 0.0685s/iter; left time: 400.4692s
Epoch: 8 cost time: 20.46160101890564
Epoch: 8, Steps: 263 | Train Loss: 0.4335755 Vali Loss: 1.0129825 Test Loss: 0.4915503
Validation loss decreased (1.014942 --> 1.012982).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4356452
	speed: 0.3157s/iter; left time: 1795.1137s
	iters: 200, epoch: 9 | loss: 0.3963788
	speed: 0.0610s/iter; left time: 340.7806s
Epoch: 9 cost time: 17.686999797821045
Epoch: 9, Steps: 263 | Train Loss: 0.4334621 Vali Loss: 1.0118971 Test Loss: 0.4907230
Validation loss decreased (1.012982 --> 1.011897).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4320680
	speed: 0.2906s/iter; left time: 1576.1199s
	iters: 200, epoch: 10 | loss: 0.4188586
	speed: 0.0581s/iter; left time: 309.5201s
Epoch: 10 cost time: 16.62325692176819
Epoch: 10, Steps: 263 | Train Loss: 0.4333085 Vali Loss: 1.0118681 Test Loss: 0.4898481
Validation loss decreased (1.011897 --> 1.011868).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4327272
	speed: 0.2713s/iter; left time: 1400.1552s
	iters: 200, epoch: 11 | loss: 0.4157000
	speed: 0.0585s/iter; left time: 296.1807s
Epoch: 11 cost time: 16.834540367126465
Epoch: 11, Steps: 263 | Train Loss: 0.4330108 Vali Loss: 1.0101382 Test Loss: 0.4894941
Validation loss decreased (1.011868 --> 1.010138).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4220327
	speed: 0.3319s/iter; left time: 1625.6956s
	iters: 200, epoch: 12 | loss: 0.4157003
	speed: 0.0733s/iter; left time: 351.6151s
Epoch: 12 cost time: 20.34694528579712
Epoch: 12, Steps: 263 | Train Loss: 0.4330746 Vali Loss: 1.0099558 Test Loss: 0.4893985
Validation loss decreased (1.010138 --> 1.009956).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4934351
	speed: 0.2642s/iter; left time: 1224.7155s
	iters: 200, epoch: 13 | loss: 0.4230532
	speed: 0.0585s/iter; left time: 265.3814s
Epoch: 13 cost time: 16.78800106048584
Epoch: 13, Steps: 263 | Train Loss: 0.4331183 Vali Loss: 1.0109320 Test Loss: 0.4895818
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4147928
	speed: 0.2635s/iter; left time: 1152.1457s
	iters: 200, epoch: 14 | loss: 0.4393463
	speed: 0.0707s/iter; left time: 301.8530s
Epoch: 14 cost time: 17.486454010009766
Epoch: 14, Steps: 263 | Train Loss: 0.4329944 Vali Loss: 1.0103582 Test Loss: 0.4893695
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4653757
	speed: 0.3021s/iter; left time: 1241.5142s
	iters: 200, epoch: 15 | loss: 0.4526814
	speed: 0.0712s/iter; left time: 285.3971s
Epoch: 15 cost time: 19.799502849578857
Epoch: 15, Steps: 263 | Train Loss: 0.4328277 Vali Loss: 1.0100914 Test Loss: 0.4892060
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4468886
	speed: 0.3103s/iter; left time: 1193.4151s
	iters: 200, epoch: 16 | loss: 0.4285201
	speed: 0.0562s/iter; left time: 210.6208s
Epoch: 16 cost time: 17.309691190719604
Epoch: 16, Steps: 263 | Train Loss: 0.4330346 Vali Loss: 1.0111215 Test Loss: 0.4893785
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4101904
	speed: 0.2687s/iter; left time: 962.6135s
	iters: 200, epoch: 17 | loss: 0.4391880
	speed: 0.0647s/iter; left time: 225.5198s
Epoch: 17 cost time: 16.737809896469116
Epoch: 17, Steps: 263 | Train Loss: 0.4329377 Vali Loss: 1.0109996 Test Loss: 0.4893046
EarlyStopping counter: 5 out of 5
Early stopping
train 33745
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=40, out_features=340, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12185600.0
params:  13940.0
Trainable parameters:  13940
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5031880
	speed: 0.0756s/iter; left time: 589.1295s
	iters: 200, epoch: 1 | loss: 0.4698074
	speed: 0.0618s/iter; left time: 475.6065s
Epoch: 1 cost time: 18.321157932281494
Epoch: 1, Steps: 263 | Train Loss: 0.4890981 Vali Loss: 1.0079689 Test Loss: 0.4878976
Validation loss decreased (inf --> 1.007969).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4522302
	speed: 0.3117s/iter; left time: 2346.3901s
	iters: 200, epoch: 2 | loss: 0.4760874
	speed: 0.0680s/iter; left time: 505.1767s
Epoch: 2 cost time: 17.918349266052246
Epoch: 2, Steps: 263 | Train Loss: 0.4888235 Vali Loss: 1.0082250 Test Loss: 0.4875839
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4514014
	speed: 0.2982s/iter; left time: 2166.2126s
	iters: 200, epoch: 3 | loss: 0.4886798
	speed: 0.0688s/iter; left time: 493.0770s
Epoch: 3 cost time: 18.960830450057983
Epoch: 3, Steps: 263 | Train Loss: 0.4887295 Vali Loss: 1.0081531 Test Loss: 0.4878543
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4881603
	speed: 0.2818s/iter; left time: 1973.3412s
	iters: 200, epoch: 4 | loss: 0.5047457
	speed: 0.0545s/iter; left time: 376.3089s
Epoch: 4 cost time: 16.426074981689453
Epoch: 4, Steps: 263 | Train Loss: 0.4886654 Vali Loss: 1.0078719 Test Loss: 0.4880660
Validation loss decreased (1.007969 --> 1.007872).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4743351
	speed: 0.2982s/iter; left time: 2009.8546s
	iters: 200, epoch: 5 | loss: 0.5373566
	speed: 0.0844s/iter; left time: 560.1485s
Epoch: 5 cost time: 20.81455683708191
Epoch: 5, Steps: 263 | Train Loss: 0.4886091 Vali Loss: 1.0087981 Test Loss: 0.4876380
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5017823
	speed: 0.3241s/iter; left time: 2098.9505s
	iters: 200, epoch: 6 | loss: 0.5183203
	speed: 0.0607s/iter; left time: 387.2430s
Epoch: 6 cost time: 17.354382753372192
Epoch: 6, Steps: 263 | Train Loss: 0.4885393 Vali Loss: 1.0083079 Test Loss: 0.4880928
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4874571
	speed: 0.2696s/iter; left time: 1675.0802s
	iters: 200, epoch: 7 | loss: 0.4991867
	speed: 0.0676s/iter; left time: 413.5117s
Epoch: 7 cost time: 17.7432804107666
Epoch: 7, Steps: 263 | Train Loss: 0.4887563 Vali Loss: 1.0088358 Test Loss: 0.4882748
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4839839
	speed: 0.2929s/iter; left time: 1743.0244s
	iters: 200, epoch: 8 | loss: 0.4933966
	speed: 0.0727s/iter; left time: 425.2691s
Epoch: 8 cost time: 21.184548377990723
Epoch: 8, Steps: 263 | Train Loss: 0.4886737 Vali Loss: 1.0080271 Test Loss: 0.4880781
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5019703
	speed: 0.3161s/iter; left time: 1797.4907s
	iters: 200, epoch: 9 | loss: 0.5099604
	speed: 0.0602s/iter; left time: 336.4551s
Epoch: 9 cost time: 16.416967630386353
Epoch: 9, Steps: 263 | Train Loss: 0.4884823 Vali Loss: 1.0077231 Test Loss: 0.4880229
Validation loss decreased (1.007872 --> 1.007723).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4918826
	speed: 0.2828s/iter; left time: 1533.7664s
	iters: 200, epoch: 10 | loss: 0.4763285
	speed: 0.0579s/iter; left time: 308.4905s
Epoch: 10 cost time: 16.006739139556885
Epoch: 10, Steps: 263 | Train Loss: 0.4884978 Vali Loss: 1.0080465 Test Loss: 0.4881988
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5016239
	speed: 0.2607s/iter; left time: 1345.2286s
	iters: 200, epoch: 11 | loss: 0.4600925
	speed: 0.0628s/iter; left time: 318.0553s
Epoch: 11 cost time: 19.11734628677368
Epoch: 11, Steps: 263 | Train Loss: 0.4884324 Vali Loss: 1.0082748 Test Loss: 0.4882991
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4812928
	speed: 0.3477s/iter; left time: 1702.9394s
	iters: 200, epoch: 12 | loss: 0.5001419
	speed: 0.0673s/iter; left time: 322.8458s
Epoch: 12 cost time: 18.758357524871826
Epoch: 12, Steps: 263 | Train Loss: 0.4884504 Vali Loss: 1.0082480 Test Loss: 0.4881331
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4566824
	speed: 0.2733s/iter; left time: 1266.8653s
	iters: 200, epoch: 13 | loss: 0.4693880
	speed: 0.0522s/iter; left time: 236.5082s
Epoch: 13 cost time: 17.102056980133057
Epoch: 13, Steps: 263 | Train Loss: 0.4883982 Vali Loss: 1.0091903 Test Loss: 0.4884653
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4795385
	speed: 0.2953s/iter; left time: 1291.2500s
	iters: 200, epoch: 14 | loss: 0.4421054
	speed: 0.0700s/iter; left time: 299.0183s
Epoch: 14 cost time: 18.425376415252686
Epoch: 14, Steps: 263 | Train Loss: 0.4883902 Vali Loss: 1.0087693 Test Loss: 0.4883955
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTm1_96_720_FITS_ETTm1_ftM_sl96_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4861225485801697, mae:0.44878292083740234, rse:0.6633513569831848, corr:[0.53163457 0.52837753 0.5278582  0.524802   0.52332073 0.5211241
 0.5189156  0.5175948  0.5153869  0.51460534 0.5130851  0.5117456
 0.5093938  0.50657666 0.5040505  0.5004831  0.49696296 0.49442038
 0.49198663 0.4888731  0.4860587  0.48352486 0.48068935 0.47716212
 0.47351158 0.47018555 0.46691912 0.4640119  0.46185786 0.46011326
 0.45872805 0.45818228 0.45819357 0.45876703 0.45948187 0.4595123
 0.4592679  0.45961124 0.45928347 0.45875537 0.45839876 0.4576367
 0.45808566 0.45834655 0.45817557 0.4579415  0.45742056 0.4583243
 0.45862988 0.45823237 0.458957   0.45919043 0.45958152 0.45932505
 0.45879027 0.45950624 0.46028697 0.4604858  0.46135324 0.461113
 0.46015662 0.45985717 0.45938757 0.45868984 0.45909008 0.4593529
 0.4588995  0.45904997 0.460114   0.4613007  0.46199855 0.46326938
 0.46460417 0.46542594 0.46708027 0.46785703 0.46835795 0.4688929
 0.4694274  0.47062814 0.47068834 0.47080904 0.4722089  0.47298345
 0.47382113 0.47422808 0.47526926 0.4768046  0.47745454 0.47868478
 0.47984484 0.48143777 0.4830971  0.48321977 0.48383415 0.48434535
 0.4834982  0.4828414  0.48175207 0.48015815 0.47801617 0.4766003
 0.47600627 0.47410095 0.4727218  0.4720424  0.47040027 0.46936533
 0.46817118 0.46642974 0.46575615 0.46471182 0.46343455 0.4627279
 0.46142414 0.460348   0.45917216 0.45753533 0.45641643 0.45491704
 0.45378563 0.45325977 0.4518545  0.45084697 0.4498737  0.44881976
 0.4485806  0.44779757 0.44751537 0.44808674 0.4484715  0.44857964
 0.4479166  0.44819766 0.4483279  0.44807476 0.44805816 0.4476413
 0.4478116  0.44768828 0.44733924 0.44776365 0.4478417  0.44804785
 0.4482787  0.44828176 0.44858477 0.4485259  0.44907296 0.44989312
 0.44991228 0.44966277 0.44967642 0.45008242 0.4499467  0.4499347
 0.45024252 0.4498939  0.4498355  0.44988167 0.4497283  0.45036903
 0.45109066 0.45126057 0.45163167 0.4527271  0.45395184 0.4546392
 0.4552832  0.4566699  0.4580387  0.45869228 0.45910433 0.46003118
 0.46049008 0.46093714 0.4616569  0.4626734  0.46379912 0.46402457
 0.4651954  0.46679622 0.46700868 0.46831548 0.46971518 0.47027636
 0.47211966 0.47397512 0.47537005 0.47620046 0.4770678  0.4792124
 0.48046002 0.48024878 0.4807448  0.47965485 0.47861052 0.4780313
 0.47642094 0.4751608  0.47372517 0.4713805  0.4699718  0.46854156
 0.46664724 0.4648435  0.46292886 0.4614039  0.45927653 0.4579252
 0.45684728 0.45468536 0.45312914 0.45130327 0.4491351  0.44770166
 0.44497198 0.44404072 0.44277036 0.44052935 0.44040743 0.4386386
 0.4373116  0.43808827 0.43789482 0.43857783 0.4382503  0.43753085
 0.4381305  0.43806553 0.43801266 0.43747714 0.43699244 0.43630126
 0.4359177  0.43684113 0.43694243 0.4368177  0.4366137  0.43583208
 0.43726465 0.438413   0.43789795 0.4389236  0.43845847 0.43792018
 0.43930262 0.43932533 0.43960512 0.4408991  0.43998626 0.43972105
 0.44063345 0.44078574 0.43967742 0.43954387 0.44090137 0.4413413
 0.44058588 0.44140738 0.44273826 0.44352335 0.4447721  0.44621542
 0.44751564 0.44821715 0.44948578 0.45040473 0.45065546 0.45137712
 0.45238262 0.45304415 0.45365572 0.45396414 0.45482424 0.45599556
 0.4568687  0.4575824  0.45875263 0.46025333 0.46057606 0.46085528
 0.46201965 0.46268114 0.4632268  0.46377695 0.46481147 0.46445376
 0.46255815 0.4619719  0.46109504 0.4587528  0.45682353 0.45579094
 0.45422465 0.45266643 0.45101643 0.44921565 0.44754064 0.4458868
 0.44397643 0.4418435  0.44033936 0.43942758 0.43808085 0.4369333
 0.43674967 0.43545428 0.4340521  0.43321016 0.43187076 0.43126595
 0.4307785  0.42966694 0.42831004 0.4274277  0.42689604 0.42621893
 0.42586944 0.4259397  0.4258121  0.42632353 0.42589396 0.42514476
 0.42549467 0.4249265  0.42481944 0.42466396 0.42378992 0.4238708
 0.42346293 0.42252678 0.42250764 0.42259824 0.42301974 0.42293474
 0.42342764 0.42390078 0.4234693  0.42383128 0.42454654 0.4241546
 0.42432493 0.4245126  0.42424557 0.4249042  0.42500728 0.42463195
 0.4245092  0.42404193 0.42457205 0.4249532  0.42470908 0.42484558
 0.4248339  0.42593172 0.42718923 0.4276153  0.42902502 0.43012577
 0.43121642 0.43305323 0.4341036  0.4345161  0.4352267  0.43650708
 0.43788835 0.43885845 0.44084257 0.44225115 0.44329336 0.44519904
 0.44646156 0.44841072 0.4502048  0.45144224 0.4533872  0.45354578
 0.45467955 0.45747405 0.4580677  0.45897207 0.46013936 0.46095875
 0.46084937 0.46027842 0.45981818 0.45933264 0.4580053  0.45656157
 0.45557746 0.45456514 0.4531062  0.4520351  0.4513034  0.4500872
 0.4487742  0.44741282 0.4457506  0.44423807 0.44270167 0.44181904
 0.44124544 0.43986747 0.43930182 0.43887562 0.4377656  0.43681353
 0.43486798 0.43367922 0.43283698 0.4312852  0.43120307 0.43080848
 0.4296535  0.42972568 0.4305725  0.43060496 0.4300774  0.43047708
 0.43094462 0.4310245  0.4310953  0.4301212  0.42988947 0.4301824
 0.42865345 0.42869526 0.42926505 0.42864794 0.42845282 0.42867205
 0.4290446  0.42936674 0.4295337  0.43012074 0.43027073 0.43048698
 0.43063954 0.43044868 0.430559   0.43101162 0.43098724 0.43099865
 0.4308195  0.43049908 0.43060812 0.4306246  0.4295071  0.43017974
 0.43191427 0.43179053 0.43222934 0.43313342 0.43364117 0.43550014
 0.43682826 0.4369735  0.4379856  0.43871215 0.4391165  0.4404777
 0.44139788 0.44268462 0.44354826 0.44359973 0.44487053 0.44594368
 0.44686317 0.44820178 0.44924656 0.4508214  0.45168102 0.45192868
 0.45273155 0.4529862  0.45407313 0.45481607 0.45416296 0.45358467
 0.4534086  0.45318782 0.45282096 0.4512887  0.45021662 0.44895992
 0.44779718 0.44635558 0.44456613 0.44375712 0.44189513 0.43975714
 0.43780884 0.43562275 0.4350268  0.43330055 0.4313266  0.43071887
 0.42875335 0.4271335  0.4260493  0.42486063 0.42388007 0.42180115
 0.42105374 0.4205898  0.41926786 0.41851348 0.41698748 0.41604
 0.4157636  0.41536584 0.41590884 0.41660056 0.41692564 0.41618434
 0.41526267 0.41628948 0.4167712  0.41551155 0.41486102 0.41542682
 0.41501933 0.41407573 0.414106   0.4145041  0.41448322 0.4146522
 0.41493633 0.4147155  0.41505444 0.416144   0.41640612 0.4158478
 0.4154652  0.41548377 0.4160975  0.4165578  0.41635722 0.41532874
 0.41508156 0.41565314 0.4149129  0.41421545 0.41461885 0.41469088
 0.4142393  0.4142163  0.4157201  0.41677845 0.41721943 0.4184715
 0.41927782 0.42042217 0.4214963  0.4220881  0.42367867 0.4242619
 0.42480066 0.4266563  0.42786565 0.42864767 0.42986828 0.43134117
 0.4322575  0.4330337  0.43475506 0.43579996 0.43606788 0.4374166
 0.43829378 0.4383331  0.439525   0.43964332 0.43820152 0.4386987
 0.4381406  0.43792704 0.43728432 0.4350223  0.4334327  0.43265894
 0.43102816 0.4288358  0.42771837 0.42719477 0.42552498 0.42403647
 0.42265478 0.4200189  0.41818932 0.41690943 0.41492394 0.41382784
 0.41321635 0.41156763 0.40991607 0.40912142 0.4077138  0.40642583
 0.4062334  0.40493143 0.403613   0.40314445 0.4019097  0.40136108
 0.40176    0.40206206 0.40136385 0.40100887 0.40171352 0.40166834
 0.40134162 0.40157077 0.40116787 0.401413   0.4020224  0.40104517
 0.39982447 0.39977267 0.40047577 0.4003802  0.4006728  0.40171176
 0.4016789  0.401596   0.40190014 0.4018475  0.40235233 0.40181848
 0.4011969  0.4021148  0.40273738 0.40270764 0.40316465 0.4030075
 0.40220827 0.40129238 0.40081713 0.40135956 0.40152523 0.40183955
 0.4025945  0.40231356 0.40244177 0.40358913 0.40386814 0.4048126
 0.4055536  0.40612358 0.40778068 0.40853056 0.409165   0.4105347
 0.41115984 0.41172916 0.41290537 0.41432765 0.41538626 0.4167583
 0.4182064  0.4192599  0.42076477 0.4217997  0.4226044  0.4236252
 0.42469278 0.42633623 0.4271408  0.42786276 0.42765197 0.4262531
 0.42659205 0.42607048 0.42480877 0.42397436 0.4227154  0.42113608
 0.41959795 0.4185747  0.4168261  0.41516975 0.4136597  0.4119098
 0.41127267 0.408818   0.40693966 0.40586787 0.4026368  0.40227485
 0.40151882 0.3998274  0.3993145  0.39644158 0.3959656  0.39420208
 0.39148378 0.39224187 0.39021307 0.38936862 0.38853142 0.3865436
 0.3868448  0.38606453 0.38609082 0.38517204 0.38537747 0.38612452
 0.38435853 0.38408723 0.38326865 0.3851019  0.3847998  0.38430378
 0.38526186 0.38493663 0.38775828 0.3876377  0.39301053 0.38709646]
