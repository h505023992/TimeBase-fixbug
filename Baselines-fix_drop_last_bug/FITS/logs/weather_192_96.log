Args in experiment:
Namespace(is_training=1, model_id='weather_192_96', model='FITS', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=96, individual=False, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=3, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=34, base_T=144, H_order=12)
Use GPU: cuda:0
>>>>>>>start training : weather_192_96_FITS_custom_ftM_sl192_ll48_pl96_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36600
val 5175
test 10444
Model(
  (freq_upsampler): Linear(in_features=34, out_features=51, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4660992.0
params:  1785.0
Trainable parameters:  1785
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4182538
	speed: 0.0968s/iter; left time: 818.2454s
	iters: 200, epoch: 1 | loss: 0.4219914
	speed: 0.0863s/iter; left time: 720.6455s
Epoch: 1 cost time: 25.590669631958008
Epoch: 1, Steps: 285 | Train Loss: 0.5223103 Vali Loss: 0.5307360 Test Loss: 0.2150537
Validation loss decreased (inf --> 0.530736).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2388880
	speed: 0.3137s/iter; left time: 2561.9259s
	iters: 200, epoch: 2 | loss: 0.2515406
	speed: 0.0709s/iter; left time: 571.6999s
Epoch: 2 cost time: 21.028785228729248
Epoch: 2, Steps: 285 | Train Loss: 0.3284599 Vali Loss: 0.4875376 Test Loss: 0.2006626
Validation loss decreased (0.530736 --> 0.487538).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3113607
	speed: 0.3183s/iter; left time: 2508.4960s
	iters: 200, epoch: 3 | loss: 0.2603800
	speed: 0.0842s/iter; left time: 654.9453s
Epoch: 3 cost time: 25.299838304519653
Epoch: 3, Steps: 285 | Train Loss: 0.2674690 Vali Loss: 0.4772062 Test Loss: 0.1957717
Validation loss decreased (0.487538 --> 0.477206).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1698206
	speed: 0.3502s/iter; left time: 2660.4026s
	iters: 200, epoch: 4 | loss: 0.2507831
	speed: 0.0873s/iter; left time: 654.1239s
Epoch: 4 cost time: 26.43672466278076
Epoch: 4, Steps: 285 | Train Loss: 0.2407957 Vali Loss: 0.4704618 Test Loss: 0.1926260
Validation loss decreased (0.477206 --> 0.470462).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1778142
	speed: 0.3407s/iter; left time: 2491.1936s
	iters: 200, epoch: 5 | loss: 0.1962132
	speed: 0.0880s/iter; left time: 634.6120s
Epoch: 5 cost time: 26.145586490631104
Epoch: 5, Steps: 285 | Train Loss: 0.2281690 Vali Loss: 0.4654857 Test Loss: 0.1906257
Validation loss decreased (0.470462 --> 0.465486).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2941795
	speed: 0.3497s/iter; left time: 2456.6434s
	iters: 200, epoch: 6 | loss: 0.2687172
	speed: 0.0906s/iter; left time: 627.1809s
Epoch: 6 cost time: 26.978219270706177
Epoch: 6, Steps: 285 | Train Loss: 0.2215391 Vali Loss: 0.4653361 Test Loss: 0.1896336
Validation loss decreased (0.465486 --> 0.465336).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2144998
	speed: 0.3528s/iter; left time: 2378.1223s
	iters: 200, epoch: 7 | loss: 0.1509317
	speed: 0.0866s/iter; left time: 575.3291s
Epoch: 7 cost time: 26.175148963928223
Epoch: 7, Steps: 285 | Train Loss: 0.2182354 Vali Loss: 0.4614434 Test Loss: 0.1894692
Validation loss decreased (0.465336 --> 0.461443).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1692991
	speed: 0.3621s/iter; left time: 2338.0225s
	iters: 200, epoch: 8 | loss: 0.1776095
	speed: 0.0876s/iter; left time: 556.9155s
Epoch: 8 cost time: 25.987550973892212
Epoch: 8, Steps: 285 | Train Loss: 0.2167145 Vali Loss: 0.4638928 Test Loss: 0.1890841
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1872389
	speed: 0.3465s/iter; left time: 2138.4453s
	iters: 200, epoch: 9 | loss: 0.3110672
	speed: 0.0897s/iter; left time: 544.7425s
Epoch: 9 cost time: 26.205192804336548
Epoch: 9, Steps: 285 | Train Loss: 0.2159032 Vali Loss: 0.4632434 Test Loss: 0.1890495
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1785432
	speed: 0.3400s/iter; left time: 2001.1286s
	iters: 200, epoch: 10 | loss: 0.2480076
	speed: 0.0815s/iter; left time: 471.3754s
Epoch: 10 cost time: 24.048203229904175
Epoch: 10, Steps: 285 | Train Loss: 0.2155046 Vali Loss: 0.4654072 Test Loss: 0.1893167
EarlyStopping counter: 3 out of 3
Early stopping
train 36600
val 5175
test 10444
Model(
  (freq_upsampler): Linear(in_features=34, out_features=51, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4660992.0
params:  1785.0
Trainable parameters:  1785
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3777991
	speed: 0.0931s/iter; left time: 786.3734s
	iters: 200, epoch: 1 | loss: 0.6082780
	speed: 0.0735s/iter; left time: 613.7886s
Epoch: 1 cost time: 24.416968822479248
Epoch: 1, Steps: 285 | Train Loss: 0.4884905 Vali Loss: 0.4601733 Test Loss: 0.1883279
Validation loss decreased (inf --> 0.460173).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6769695
	speed: 0.3484s/iter; left time: 2844.8226s
	iters: 200, epoch: 2 | loss: 0.3995395
	speed: 0.0922s/iter; left time: 743.6537s
Epoch: 2 cost time: 25.70611596107483
Epoch: 2, Steps: 285 | Train Loss: 0.4861469 Vali Loss: 0.4578196 Test Loss: 0.1874738
Validation loss decreased (0.460173 --> 0.457820).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3847508
	speed: 0.3545s/iter; left time: 2793.8426s
	iters: 200, epoch: 3 | loss: 0.3888260
	speed: 0.0905s/iter; left time: 704.1423s
Epoch: 3 cost time: 26.63851833343506
Epoch: 3, Steps: 285 | Train Loss: 0.4844129 Vali Loss: 0.4594034 Test Loss: 0.1871761
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6494499
	speed: 0.3465s/iter; left time: 2631.9069s
	iters: 200, epoch: 4 | loss: 0.4998625
	speed: 0.0890s/iter; left time: 667.2823s
Epoch: 4 cost time: 25.814382314682007
Epoch: 4, Steps: 285 | Train Loss: 0.4848059 Vali Loss: 0.4586634 Test Loss: 0.1871972
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 1.0806878
	speed: 0.3575s/iter; left time: 2613.6291s
	iters: 200, epoch: 5 | loss: 0.4322503
	speed: 0.0957s/iter; left time: 689.8722s
Epoch: 5 cost time: 28.78227949142456
Epoch: 5, Steps: 285 | Train Loss: 0.4844574 Vali Loss: 0.4597199 Test Loss: 0.1864953
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : weather_192_96_FITS_custom_ftM_sl192_ll48_pl96_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10444
mse:0.18819648027420044, mae:0.23327010869979858, rse:0.5716865658760071, corr:[0.4813642  0.4858583  0.48479345 0.48282018 0.48198932 0.4818306
 0.4810926  0.4795879  0.47792864 0.47665486 0.47580418 0.4749587
 0.47381148 0.47262824 0.471541   0.47048005 0.46919444 0.46762624
 0.46638516 0.46545628 0.4646522  0.46381438 0.46269542 0.4614024
 0.46025404 0.45919096 0.45804095 0.45636538 0.4544077  0.4526791
 0.45143622 0.45030656 0.4490997  0.44771618 0.44646758 0.44544023
 0.4446746  0.44377142 0.44265255 0.44138068 0.44035447 0.43975616
 0.43922785 0.43849167 0.43746907 0.43635383 0.4356137  0.43513626
 0.43448892 0.43355608 0.43271446 0.43200088 0.43162555 0.43135634
 0.43101394 0.43041494 0.42989355 0.42937115 0.4291259  0.4289484
 0.4287066  0.4284915  0.42842156 0.42840362 0.42841437 0.42832708
 0.4278272  0.42730293 0.42706084 0.42717692 0.42743346 0.42744282
 0.4272506  0.42715004 0.4272188  0.42743453 0.4275215  0.4275306
 0.42749724 0.42753252 0.42786342 0.4283241  0.42852324 0.42843565
 0.42826334 0.4282223  0.4284447  0.42859685 0.4286052  0.4284643
 0.4284611  0.42872006 0.4292258  0.4297006  0.42964336 0.4283908 ]
