Args in experiment:
Namespace(is_training=1, model_id='electricity_720_192', model='FITS', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=321, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=258, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : electricity_720_192_FITS_custom_ftM_sl720_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17501
val 2441
test 5069
Model(
  (freq_upsampler): Linear(in_features=258, out_features=326, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3455829504.0
params:  84434.0
Trainable parameters:  84434
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8085315
	speed: 0.2646s/iter; left time: 1053.5202s
Epoch: 1 cost time: 34.950480699539185
Epoch: 1, Steps: 136 | Train Loss: 0.9183882 Vali Loss: 0.6962503 Test Loss: 0.8025128
Validation loss decreased (inf --> 0.696250).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6826147
	speed: 0.5761s/iter; left time: 2215.0325s
Epoch: 2 cost time: 37.09013104438782
Epoch: 2, Steps: 136 | Train Loss: 0.6949346 Vali Loss: 0.6119574 Test Loss: 0.7060403
Validation loss decreased (0.696250 --> 0.611957).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5835459
	speed: 0.5930s/iter; left time: 2199.5950s
Epoch: 3 cost time: 36.82809376716614
Epoch: 3, Steps: 136 | Train Loss: 0.5974192 Vali Loss: 0.5469000 Test Loss: 0.6328415
Validation loss decreased (0.611957 --> 0.546900).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5031862
	speed: 0.6040s/iter; left time: 2158.2168s
Epoch: 4 cost time: 37.685706615448
Epoch: 4, Steps: 136 | Train Loss: 0.5197043 Vali Loss: 0.4946799 Test Loss: 0.5745022
Validation loss decreased (0.546900 --> 0.494680).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4460352
	speed: 0.6075s/iter; left time: 2087.9142s
Epoch: 5 cost time: 37.04092311859131
Epoch: 5, Steps: 136 | Train Loss: 0.4550044 Vali Loss: 0.4484123 Test Loss: 0.5228080
Validation loss decreased (0.494680 --> 0.448412).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3875127
	speed: 0.6069s/iter; left time: 2003.2475s
Epoch: 6 cost time: 38.254613637924194
Epoch: 6, Steps: 136 | Train Loss: 0.4005188 Vali Loss: 0.4067545 Test Loss: 0.4765142
Validation loss decreased (0.448412 --> 0.406754).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3419763
	speed: 0.5733s/iter; left time: 1814.3594s
Epoch: 7 cost time: 34.64163875579834
Epoch: 7, Steps: 136 | Train Loss: 0.3542357 Vali Loss: 0.3725807 Test Loss: 0.4379372
Validation loss decreased (0.406754 --> 0.372581).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3106794
	speed: 0.5989s/iter; left time: 1814.1795s
Epoch: 8 cost time: 40.0965051651001
Epoch: 8, Steps: 136 | Train Loss: 0.3146800 Vali Loss: 0.3422070 Test Loss: 0.4039931
Validation loss decreased (0.372581 --> 0.342207).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2742737
	speed: 0.5851s/iter; left time: 1692.6846s
Epoch: 9 cost time: 36.440184593200684
Epoch: 9, Steps: 136 | Train Loss: 0.2807065 Vali Loss: 0.3152207 Test Loss: 0.3735400
Validation loss decreased (0.342207 --> 0.315221).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2475828
	speed: 0.5869s/iter; left time: 1618.0972s
Epoch: 10 cost time: 36.533145904541016
Epoch: 10, Steps: 136 | Train Loss: 0.2513395 Vali Loss: 0.2920794 Test Loss: 0.3474385
Validation loss decreased (0.315221 --> 0.292079).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2229104
	speed: 0.5796s/iter; left time: 1519.1792s
Epoch: 11 cost time: 35.1364324092865
Epoch: 11, Steps: 136 | Train Loss: 0.2259050 Vali Loss: 0.2734064 Test Loss: 0.3263344
Validation loss decreased (0.292079 --> 0.273406).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1987219
	speed: 0.5762s/iter; left time: 1431.7728s
Epoch: 12 cost time: 37.73428726196289
Epoch: 12, Steps: 136 | Train Loss: 0.2038903 Vali Loss: 0.2569308 Test Loss: 0.3076950
Validation loss decreased (0.273406 --> 0.256931).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1821583
	speed: 0.5864s/iter; left time: 1377.4101s
Epoch: 13 cost time: 34.896409034729004
Epoch: 13, Steps: 136 | Train Loss: 0.1846549 Vali Loss: 0.2413992 Test Loss: 0.2900840
Validation loss decreased (0.256931 --> 0.241399).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1665101
	speed: 0.5599s/iter; left time: 1238.9759s
Epoch: 14 cost time: 36.50140142440796
Epoch: 14, Steps: 136 | Train Loss: 0.1679260 Vali Loss: 0.2276476 Test Loss: 0.2744465
Validation loss decreased (0.241399 --> 0.227648).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1525536
	speed: 0.5644s/iter; left time: 1172.3504s
Epoch: 15 cost time: 32.71073818206787
Epoch: 15, Steps: 136 | Train Loss: 0.1532860 Vali Loss: 0.2152200 Test Loss: 0.2601378
Validation loss decreased (0.227648 --> 0.215220).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1395115
	speed: 0.5472s/iter; left time: 1062.2107s
Epoch: 16 cost time: 35.19377779960632
Epoch: 16, Steps: 136 | Train Loss: 0.1404374 Vali Loss: 0.2058609 Test Loss: 0.2493642
Validation loss decreased (0.215220 --> 0.205861).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1268668
	speed: 0.5612s/iter; left time: 1012.8973s
Epoch: 17 cost time: 35.38335704803467
Epoch: 17, Steps: 136 | Train Loss: 0.1291568 Vali Loss: 0.1964829 Test Loss: 0.2385236
Validation loss decreased (0.205861 --> 0.196483).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1190202
	speed: 0.5507s/iter; left time: 919.1455s
Epoch: 18 cost time: 33.381736755371094
Epoch: 18, Steps: 136 | Train Loss: 0.1192549 Vali Loss: 0.1892688 Test Loss: 0.2301816
Validation loss decreased (0.196483 --> 0.189269).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1120007
	speed: 0.5378s/iter; left time: 824.4954s
Epoch: 19 cost time: 33.302947998046875
Epoch: 19, Steps: 136 | Train Loss: 0.1105336 Vali Loss: 0.1824183 Test Loss: 0.2222097
Validation loss decreased (0.189269 --> 0.182418).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1019327
	speed: 0.5350s/iter; left time: 747.3935s
Epoch: 20 cost time: 32.479389667510986
Epoch: 20, Steps: 136 | Train Loss: 0.1028375 Vali Loss: 0.1767849 Test Loss: 0.2155196
Validation loss decreased (0.182418 --> 0.176785).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.0939276
	speed: 0.5638s/iter; left time: 710.9349s
Epoch: 21 cost time: 37.36585593223572
Epoch: 21, Steps: 136 | Train Loss: 0.0960360 Vali Loss: 0.1703235 Test Loss: 0.2080384
Validation loss decreased (0.176785 --> 0.170323).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.0861836
	speed: 0.5989s/iter; left time: 673.7325s
Epoch: 22 cost time: 37.736124753952026
Epoch: 22, Steps: 136 | Train Loss: 0.0900290 Vali Loss: 0.1666935 Test Loss: 0.2037371
Validation loss decreased (0.170323 --> 0.166693).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.0817704
	speed: 0.5980s/iter; left time: 591.4060s
Epoch: 23 cost time: 36.355308532714844
Epoch: 23, Steps: 136 | Train Loss: 0.0847005 Vali Loss: 0.1619714 Test Loss: 0.1981459
Validation loss decreased (0.166693 --> 0.161971).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.0772665
	speed: 0.6094s/iter; left time: 519.8067s
Epoch: 24 cost time: 37.81863832473755
Epoch: 24, Steps: 136 | Train Loss: 0.0799626 Vali Loss: 0.1585083 Test Loss: 0.1940989
Validation loss decreased (0.161971 --> 0.158508).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.0754578
	speed: 0.6061s/iter; left time: 434.5711s
Epoch: 25 cost time: 36.83674383163452
Epoch: 25, Steps: 136 | Train Loss: 0.0757543 Vali Loss: 0.1550704 Test Loss: 0.1898959
Validation loss decreased (0.158508 --> 0.155070).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.0704116
	speed: 0.6097s/iter; left time: 354.2342s
Epoch: 26 cost time: 37.75704598426819
Epoch: 26, Steps: 136 | Train Loss: 0.0720295 Vali Loss: 0.1525482 Test Loss: 0.1869121
Validation loss decreased (0.155070 --> 0.152548).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.0670324
	speed: 0.6003s/iter; left time: 267.1429s
Epoch: 27 cost time: 37.05863618850708
Epoch: 27, Steps: 136 | Train Loss: 0.0687053 Vali Loss: 0.1497077 Test Loss: 0.1834701
Validation loss decreased (0.152548 --> 0.149708).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.0670110
	speed: 0.6055s/iter; left time: 187.0870s
Epoch: 28 cost time: 37.55666661262512
Epoch: 28, Steps: 136 | Train Loss: 0.0657446 Vali Loss: 0.1480231 Test Loss: 0.1814257
Validation loss decreased (0.149708 --> 0.148023).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.0617592
	speed: 0.6007s/iter; left time: 103.9233s
Epoch: 29 cost time: 36.993647813797
Epoch: 29, Steps: 136 | Train Loss: 0.0631133 Vali Loss: 0.1459446 Test Loss: 0.1788927
Validation loss decreased (0.148023 --> 0.145945).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.0605078
	speed: 0.6102s/iter; left time: 22.5774s
Epoch: 30 cost time: 37.22269153594971
Epoch: 30, Steps: 136 | Train Loss: 0.0607712 Vali Loss: 0.1435151 Test Loss: 0.1759984
Validation loss decreased (0.145945 --> 0.143515).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 17501
val 2441
test 5069
Model(
  (freq_upsampler): Linear(in_features=258, out_features=326, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3455829504.0
params:  84434.0
Trainable parameters:  84434
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.1404476
	speed: 0.2806s/iter; left time: 1117.1345s
Epoch: 1 cost time: 37.085246562957764
Epoch: 1, Steps: 136 | Train Loss: 0.1551287 Vali Loss: 0.1280832 Test Loss: 0.1598997
Validation loss decreased (inf --> 0.128083).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1534192
	speed: 0.5983s/iter; left time: 2300.6325s
Epoch: 2 cost time: 37.452738761901855
Epoch: 2, Steps: 136 | Train Loss: 0.1526422 Vali Loss: 0.1277353 Test Loss: 0.1596625
Validation loss decreased (0.128083 --> 0.127735).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1573984
	speed: 0.6123s/iter; left time: 2271.0410s
Epoch: 3 cost time: 36.618613481521606
Epoch: 3, Steps: 136 | Train Loss: 0.1524518 Vali Loss: 0.1277416 Test Loss: 0.1595445
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1618852
	speed: 0.6280s/iter; left time: 2243.7515s
Epoch: 4 cost time: 38.73713254928589
Epoch: 4, Steps: 136 | Train Loss: 0.1524072 Vali Loss: 0.1277545 Test Loss: 0.1594884
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1591448
	speed: 0.5949s/iter; left time: 2044.6419s
Epoch: 5 cost time: 35.71946382522583
Epoch: 5, Steps: 136 | Train Loss: 0.1523009 Vali Loss: 0.1277496 Test Loss: 0.1594870
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1580196
	speed: 0.6269s/iter; left time: 2069.3849s
Epoch: 6 cost time: 38.37132430076599
Epoch: 6, Steps: 136 | Train Loss: 0.1522661 Vali Loss: 0.1276166 Test Loss: 0.1593562
Validation loss decreased (0.127735 --> 0.127617).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1587136
	speed: 0.5957s/iter; left time: 1885.4937s
Epoch: 7 cost time: 36.572913646698
Epoch: 7, Steps: 136 | Train Loss: 0.1521382 Vali Loss: 0.1274753 Test Loss: 0.1593151
Validation loss decreased (0.127617 --> 0.127475).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1556274
	speed: 0.6324s/iter; left time: 1915.5636s
Epoch: 8 cost time: 39.2931227684021
Epoch: 8, Steps: 136 | Train Loss: 0.1521983 Vali Loss: 0.1274159 Test Loss: 0.1592584
Validation loss decreased (0.127475 --> 0.127416).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1484594
	speed: 0.6102s/iter; left time: 1765.3557s
Epoch: 9 cost time: 38.24482274055481
Epoch: 9, Steps: 136 | Train Loss: 0.1521570 Vali Loss: 0.1276285 Test Loss: 0.1593243
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1593544
	speed: 0.6280s/iter; left time: 1731.4660s
Epoch: 10 cost time: 38.38301730155945
Epoch: 10, Steps: 136 | Train Loss: 0.1521578 Vali Loss: 0.1274898 Test Loss: 0.1592431
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1507050
	speed: 0.5986s/iter; left time: 1568.8936s
Epoch: 11 cost time: 38.036346673965454
Epoch: 11, Steps: 136 | Train Loss: 0.1520339 Vali Loss: 0.1274814 Test Loss: 0.1592722
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1552044
	speed: 0.6433s/iter; left time: 1598.7023s
Epoch: 12 cost time: 38.616381883621216
Epoch: 12, Steps: 136 | Train Loss: 0.1520016 Vali Loss: 0.1274329 Test Loss: 0.1592464
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1679892
	speed: 0.6020s/iter; left time: 1414.0001s
Epoch: 13 cost time: 37.871195793151855
Epoch: 13, Steps: 136 | Train Loss: 0.1521333 Vali Loss: 0.1274352 Test Loss: 0.1591593
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : electricity_720_192_FITS_custom_ftM_sl720_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5069
mse:0.15938468146324158, mae:0.25608524549007416, rse:0.38685065507888794, corr:[0.46357116 0.46614456 0.46782213 0.46823248 0.4684207  0.4685382
 0.4684165  0.46840367 0.46814677 0.46807584 0.46796718 0.46784925
 0.46792418 0.46781376 0.46785524 0.46806765 0.467886   0.46776494
 0.46782884 0.46756724 0.46733084 0.4673639  0.46750933 0.46776026
 0.46795544 0.46822256 0.46838814 0.46832827 0.46827847 0.46814114
 0.46789253 0.46782207 0.46768093 0.46742865 0.4673284  0.4672876
 0.46712738 0.46696255 0.46693775 0.46697876 0.46685892 0.46660402
 0.46643507 0.4662863  0.46624526 0.4664007  0.4664964  0.4665204
 0.46663138 0.46676248 0.46682853 0.46687812 0.4668952  0.46680874
 0.46662945 0.466581   0.46660277 0.46646726 0.46638918 0.46639353
 0.46618822 0.46604007 0.46617675 0.46621165 0.46613955 0.466184
 0.4661533  0.46602294 0.46599996 0.4659476  0.46587634 0.46597084
 0.46610048 0.46609905 0.46604434 0.46618095 0.4662242  0.46608156
 0.46610582 0.46611694 0.46589205 0.46574175 0.46574652 0.4657439
 0.4657155  0.46561325 0.46548876 0.4654834  0.46553144 0.46553522
 0.46548033 0.46539006 0.46538344 0.46538633 0.4653308  0.46543273
 0.46562672 0.46573123 0.4657024  0.4657078  0.4657516  0.4656264
 0.46547264 0.4655333  0.46549565 0.46527594 0.46520445 0.46521056
 0.46507332 0.46495536 0.46488658 0.46486717 0.46495542 0.46499914
 0.4650943  0.465248   0.46516448 0.46506613 0.46527702 0.4656101
 0.4657765  0.4657293  0.46570158 0.46574557 0.46570975 0.46565267
 0.46569416 0.465784   0.46578044 0.46564558 0.46554634 0.4655691
 0.465407   0.46515754 0.46524265 0.46533313 0.4651318  0.46509367
 0.46519884 0.46502897 0.46497443 0.4650618  0.4649876  0.46499097
 0.46507543 0.4651186  0.4651458  0.46521872 0.46524444 0.4651405
 0.46502978 0.46497226 0.4648303  0.46473148 0.46476507 0.46467045
 0.4645889  0.46468982 0.4647269  0.464743   0.4648374  0.46478263
 0.46471354 0.4646255  0.46444023 0.4644218  0.46444282 0.46437266
 0.46433902 0.46449718 0.46451664 0.4643625  0.4641991  0.46417204
 0.4641446  0.4639036  0.46363345 0.46353573 0.4634845  0.46343276
 0.46323863 0.46321794 0.46339852 0.4632986  0.46334955 0.4635814
 0.4633754  0.46309307 0.46309718 0.46341377 0.4633172  0.4634834 ]
