Args in experiment:
Namespace(is_training=1, model_id='ETTh2_720_720', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=196, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.508519887924194
Epoch: 1, Steps: 56 | Train Loss: 0.8458604 Vali Loss: 0.8206619 Test Loss: 0.4699437
Validation loss decreased (inf --> 0.820662).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.666886568069458
Epoch: 2, Steps: 56 | Train Loss: 0.6778626 Vali Loss: 0.7646632 Test Loss: 0.4309906
Validation loss decreased (0.820662 --> 0.764663).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.7133545875549316
Epoch: 3, Steps: 56 | Train Loss: 0.6087543 Vali Loss: 0.7390333 Test Loss: 0.4154760
Validation loss decreased (0.764663 --> 0.739033).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.591334104537964
Epoch: 4, Steps: 56 | Train Loss: 0.5710200 Vali Loss: 0.7191840 Test Loss: 0.4084012
Validation loss decreased (0.739033 --> 0.719184).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.724790334701538
Epoch: 5, Steps: 56 | Train Loss: 0.5497141 Vali Loss: 0.7124240 Test Loss: 0.4045286
Validation loss decreased (0.719184 --> 0.712424).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.321385383605957
Epoch: 6, Steps: 56 | Train Loss: 0.5316382 Vali Loss: 0.7053845 Test Loss: 0.4020207
Validation loss decreased (0.712424 --> 0.705384).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.5792884826660156
Epoch: 7, Steps: 56 | Train Loss: 0.5184656 Vali Loss: 0.6988658 Test Loss: 0.4001078
Validation loss decreased (0.705384 --> 0.698866).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.589414596557617
Epoch: 8, Steps: 56 | Train Loss: 0.5066365 Vali Loss: 0.6999219 Test Loss: 0.3985343
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.5296034812927246
Epoch: 9, Steps: 56 | Train Loss: 0.4987916 Vali Loss: 0.6959570 Test Loss: 0.3972153
Validation loss decreased (0.698866 --> 0.695957).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.6335854530334473
Epoch: 10, Steps: 56 | Train Loss: 0.4911112 Vali Loss: 0.6859955 Test Loss: 0.3960389
Validation loss decreased (0.695957 --> 0.685995).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.839985132217407
Epoch: 11, Steps: 56 | Train Loss: 0.4835060 Vali Loss: 0.6871409 Test Loss: 0.3949354
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.7025132179260254
Epoch: 12, Steps: 56 | Train Loss: 0.4780127 Vali Loss: 0.6819202 Test Loss: 0.3939693
Validation loss decreased (0.685995 --> 0.681920).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.6477231979370117
Epoch: 13, Steps: 56 | Train Loss: 0.4719825 Vali Loss: 0.6806735 Test Loss: 0.3930884
Validation loss decreased (0.681920 --> 0.680673).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.560004949569702
Epoch: 14, Steps: 56 | Train Loss: 0.4678351 Vali Loss: 0.6809816 Test Loss: 0.3923274
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.298743724822998
Epoch: 15, Steps: 56 | Train Loss: 0.4630422 Vali Loss: 0.6824880 Test Loss: 0.3916280
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.2408502101898193
Epoch: 16, Steps: 56 | Train Loss: 0.4598331 Vali Loss: 0.6814409 Test Loss: 0.3909163
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.405628204345703
Epoch: 17, Steps: 56 | Train Loss: 0.4555578 Vali Loss: 0.6751665 Test Loss: 0.3903974
Validation loss decreased (0.680673 --> 0.675167).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.4018349647521973
Epoch: 18, Steps: 56 | Train Loss: 0.4542204 Vali Loss: 0.6798415 Test Loss: 0.3898170
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.375300168991089
Epoch: 19, Steps: 56 | Train Loss: 0.4500343 Vali Loss: 0.6750158 Test Loss: 0.3893236
Validation loss decreased (0.675167 --> 0.675016).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.5216550827026367
Epoch: 20, Steps: 56 | Train Loss: 0.4476327 Vali Loss: 0.6699758 Test Loss: 0.3888112
Validation loss decreased (0.675016 --> 0.669976).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.239141225814819
Epoch: 21, Steps: 56 | Train Loss: 0.4454531 Vali Loss: 0.6700784 Test Loss: 0.3884180
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.480978012084961
Epoch: 22, Steps: 56 | Train Loss: 0.4442353 Vali Loss: 0.6740632 Test Loss: 0.3880084
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.6766669750213623
Epoch: 23, Steps: 56 | Train Loss: 0.4420376 Vali Loss: 0.6717712 Test Loss: 0.3876507
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.6550116539001465
Epoch: 24, Steps: 56 | Train Loss: 0.4410315 Vali Loss: 0.6661023 Test Loss: 0.3873191
Validation loss decreased (0.669976 --> 0.666102).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2627315521240234
Epoch: 25, Steps: 56 | Train Loss: 0.4388276 Vali Loss: 0.6706569 Test Loss: 0.3870317
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.754763603210449
Epoch: 26, Steps: 56 | Train Loss: 0.4371529 Vali Loss: 0.6678308 Test Loss: 0.3867620
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.3655312061309814
Epoch: 27, Steps: 56 | Train Loss: 0.4361710 Vali Loss: 0.6699045 Test Loss: 0.3864525
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.3966124057769775
Epoch: 28, Steps: 56 | Train Loss: 0.4342150 Vali Loss: 0.6663144 Test Loss: 0.3862366
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.256810188293457
Epoch: 29, Steps: 56 | Train Loss: 0.4341422 Vali Loss: 0.6643304 Test Loss: 0.3860196
Validation loss decreased (0.666102 --> 0.664330).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.4687469005584717
Epoch: 30, Steps: 56 | Train Loss: 0.4332111 Vali Loss: 0.6692299 Test Loss: 0.3858244
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011296777049628277
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.731760263442993
Epoch: 1, Steps: 56 | Train Loss: 0.8174249 Vali Loss: 0.6601878 Test Loss: 0.3835618
Validation loss decreased (inf --> 0.660188).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.6213645935058594
Epoch: 2, Steps: 56 | Train Loss: 0.8092348 Vali Loss: 0.6514993 Test Loss: 0.3820988
Validation loss decreased (0.660188 --> 0.651499).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.574049234390259
Epoch: 3, Steps: 56 | Train Loss: 0.8046127 Vali Loss: 0.6478410 Test Loss: 0.3812383
Validation loss decreased (0.651499 --> 0.647841).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.5980405807495117
Epoch: 4, Steps: 56 | Train Loss: 0.8030568 Vali Loss: 0.6508006 Test Loss: 0.3805094
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.138197660446167
Epoch: 5, Steps: 56 | Train Loss: 0.8007231 Vali Loss: 0.6461250 Test Loss: 0.3800747
Validation loss decreased (0.647841 --> 0.646125).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.5538101196289062
Epoch: 6, Steps: 56 | Train Loss: 0.8017844 Vali Loss: 0.6517647 Test Loss: 0.3798417
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.614738702774048
Epoch: 7, Steps: 56 | Train Loss: 0.7980477 Vali Loss: 0.6434075 Test Loss: 0.3797377
Validation loss decreased (0.646125 --> 0.643407).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.291984796524048
Epoch: 8, Steps: 56 | Train Loss: 0.7983087 Vali Loss: 0.6434842 Test Loss: 0.3795236
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.3972578048706055
Epoch: 9, Steps: 56 | Train Loss: 0.7981433 Vali Loss: 0.6445850 Test Loss: 0.3795076
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.400038242340088
Epoch: 10, Steps: 56 | Train Loss: 0.7999013 Vali Loss: 0.6442641 Test Loss: 0.3794742
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.5767765045166016
Epoch: 11, Steps: 56 | Train Loss: 0.7995816 Vali Loss: 0.6459925 Test Loss: 0.3794171
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.717298984527588
Epoch: 12, Steps: 56 | Train Loss: 0.7986410 Vali Loss: 0.6451219 Test Loss: 0.3793771
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3783196806907654, mae:0.42309805750846863, rse:0.4916263818740845, corr:[ 0.22019291  0.2223154   0.22067541  0.22043324  0.22036847  0.2191826
  0.21830001  0.21799015  0.217039    0.2153145   0.21405032  0.21306655
  0.21173197  0.2102529   0.20915769  0.20846866  0.20783289  0.2071066
  0.20624258  0.20520544  0.20418182  0.20333232  0.20226961  0.2009402
  0.1995054   0.1982894   0.19720536  0.19618504  0.19552994  0.19511382
  0.19463098  0.19371484  0.19255292  0.19146627  0.19061391  0.18990423
  0.18912469  0.18835399  0.18752104  0.18659373  0.18577361  0.18532886
  0.18523286  0.18484503  0.18364562  0.18217887  0.18114856  0.18023375
  0.1791788   0.17791225  0.17667623  0.1756673   0.17492495  0.1741409
  0.17340402  0.17306487  0.17262699  0.17180902  0.17128544  0.17155573
  0.1718311   0.17140901  0.17082506  0.17077997  0.17101051  0.17162262
  0.17188686  0.17146997  0.17105275  0.1709804   0.17065042  0.16987257
  0.16954921  0.16980813  0.16959906  0.16838135  0.16759647  0.1679131
  0.16820666  0.16779801  0.16751009  0.16774479  0.1680607   0.16821657
  0.16823629  0.1682698   0.1683166   0.16837963  0.16822991  0.16790839
  0.16763394  0.16766515  0.1680556   0.16824381  0.16843729  0.16871662
  0.1688211   0.16862993  0.16859715  0.16890511  0.16883607  0.16776308
  0.16698718  0.16711515  0.16745982  0.16697668  0.16637789  0.16631332
  0.16612533  0.16569686  0.16571222  0.16615815  0.16612795  0.16589282
  0.1661582   0.1665366   0.16599642  0.16475816  0.16421549  0.16404481
  0.16340242  0.1624949   0.16195688  0.1613479   0.16047171  0.1601564
  0.16035204  0.15984347  0.15867785  0.15804002  0.15831171  0.15820506
  0.15731211  0.1562061   0.15548417  0.15501001  0.15447995  0.15413493
  0.15366255  0.15305324  0.15289783  0.1527714   0.15210335  0.15084845
  0.14952204  0.1486315   0.14780964  0.14722101  0.1471073   0.14706278
  0.14652073  0.14582129  0.14590882  0.14631377  0.14603277  0.14502668
  0.14432284  0.14454642  0.14490505  0.14423333  0.14277413  0.14252058
  0.14349437  0.14370021  0.1426484   0.14197576  0.14274788  0.14325538
  0.14247614  0.14148219  0.14141694  0.14131613  0.14051145  0.13991164
  0.14011289  0.13981329  0.13839023  0.13707757  0.13652086  0.13625534
  0.1357411   0.13540664  0.13567366  0.13594566  0.13569815  0.13561498
  0.13599668  0.1367843   0.1374046   0.13740821  0.1374145   0.13776349
  0.13826373  0.13835526  0.13792814  0.13810463  0.13907455  0.1394969
  0.13906325  0.13830376  0.13808298  0.13805978  0.13790089  0.13754423
  0.13753143  0.1380536   0.13847159  0.13838272  0.13785781  0.13788188
  0.13890597  0.14024475  0.14100698  0.14097399  0.14052439  0.13967766
  0.13879737  0.13831128  0.13817915  0.1380699   0.13805775  0.13895552
  0.1394865   0.13864027  0.13721326  0.13728592  0.138621    0.13903457
  0.13831414  0.13807724  0.13907431  0.14016253  0.14028205  0.14009695
  0.14054468  0.14166787  0.14298178  0.14388826  0.14435002  0.14455111
  0.1445357   0.14409718  0.14367618  0.14399505  0.14486341  0.14562503
  0.14617749  0.14671707  0.14731655  0.14701155  0.14609885  0.14596812
  0.14703129  0.1482894   0.1487768   0.14931792  0.15052427  0.15204757
  0.152807    0.1530807   0.15386489  0.15521285  0.15621495  0.15617125
  0.15603116  0.15634221  0.15675606  0.15683334  0.15678619  0.15727226
  0.15795046  0.15842788  0.1588187   0.15917423  0.15979484  0.16045627
  0.16072084  0.16069545  0.16087034  0.16184609  0.1630036   0.16350569
  0.16363655  0.16416483  0.16479598  0.16516672  0.16559507  0.16579227
  0.16506444  0.1637664   0.16371311  0.16545507  0.16674975  0.16621517
  0.16522908  0.16537982  0.16594407  0.16585332  0.16587779  0.16690235
  0.16795698  0.16797026  0.16743791  0.16765834  0.16843109  0.16889448
  0.1687536   0.16909629  0.16966222  0.1696935   0.16939794  0.16967906
  0.17059442  0.1709517   0.17048414  0.16964246  0.16915302  0.16892938
  0.16870329  0.1684453   0.16791376  0.16748098  0.16765553  0.16870569
  0.16921419  0.16819605  0.16731898  0.16848436  0.17085798  0.17142127
  0.16983737  0.16946046  0.17152612  0.17347084  0.17329226  0.17228685
  0.1722816   0.1725319   0.1715716   0.17039615  0.17125684  0.17322406
  0.17407185  0.1734605   0.17305607  0.17320417  0.17277677  0.17232081
  0.1728661   0.17425674  0.17486963  0.17454198  0.17487718  0.17666534
  0.17835647  0.17901586  0.17904611  0.17914121  0.17897752  0.17861463
  0.17906995  0.18029223  0.18075855  0.17952116  0.17801759  0.17817129
  0.17983648  0.18134332  0.18171331  0.1816826   0.18142195  0.18091303
  0.18041217  0.18054196  0.18105865  0.18136904  0.18157901  0.1819782
  0.18235233  0.18232924  0.18199924  0.18157503  0.18096088  0.18020749
  0.18002272  0.18097295  0.18202008  0.18197872  0.18139479  0.18174303
  0.18296844  0.18377233  0.18373798  0.1837441   0.18417041  0.1844173
  0.18429247  0.18382457  0.1837569   0.18391855  0.18360752  0.18299198
  0.1825099   0.1820909   0.1816103   0.18186885  0.18285424  0.18337263
  0.18273126  0.18205605  0.18213977  0.18206143  0.18128927  0.18067703
  0.18089814  0.18122478  0.18101572  0.18069607  0.18058363  0.18040012
  0.17990977  0.17978479  0.1794134   0.1779427   0.17634879  0.1760246
  0.17599455  0.17457101  0.17315738  0.1732956   0.17343292  0.1721912
  0.17074098  0.17065766  0.170872    0.16972439  0.16852523  0.16857524
  0.16883641  0.16831659  0.1672823   0.16673502  0.16631019  0.1661815
  0.16621713  0.16612226  0.16539072  0.1645672   0.16403565  0.16370256
  0.16350721  0.16326804  0.16244438  0.16156185  0.16145198  0.16176169
  0.16168664  0.16125008  0.16096255  0.16053243  0.15971072  0.15953928
  0.16030973  0.16037886  0.15916687  0.15910324  0.16035801  0.16039477
  0.15895256  0.15784307  0.1575957   0.15658358  0.15535027  0.15580378
  0.15691856  0.1561553   0.15432505  0.15444951  0.15580821  0.15581624
  0.15454495  0.15379435  0.1533606   0.15204978  0.15091573  0.15071967
  0.15061957  0.14987715  0.14918324  0.14862259  0.1477339   0.14660363
  0.14590606  0.14558539  0.14514856  0.14457184  0.14387189  0.14327005
  0.1429076   0.14262594  0.14191304  0.14077653  0.14040123  0.14086676
  0.14046675  0.13834989  0.13623874  0.13577086  0.13588217  0.13492599
  0.13347399  0.13325088  0.13341822  0.1323297   0.13073486  0.13008219
  0.13005637  0.12926976  0.12812747  0.12783696  0.12824263  0.12775491
  0.12561062  0.12372714  0.1237716   0.12476399  0.12476514  0.12342015
  0.12152278  0.1201811   0.11975576  0.11940636  0.11823989  0.11589172
  0.11396804  0.1138423   0.1143271   0.11346865  0.1115737   0.11039816
  0.10999552  0.10921799  0.10807393  0.10683824  0.10528798  0.10382875
  0.10283846  0.1024343   0.1018658   0.10114995  0.10130911  0.10118642
  0.09937135  0.09664363  0.0954091   0.09555466  0.09506101  0.09346388
  0.09215271  0.0916615   0.09141524  0.09040471  0.0892441   0.08855402
  0.08870388  0.08923665  0.08904031  0.08746253  0.08607648  0.08573694
  0.08540981  0.08404619  0.08256803  0.08227536  0.08214976  0.08074302
  0.07866688  0.07780846  0.07770449  0.07669792  0.07459435  0.072528
  0.0712545   0.07034993  0.06970698  0.06987641  0.07026815  0.06972285
  0.06811707  0.06720725  0.06740059  0.06701856  0.06582461  0.06530937
  0.06563216  0.06509244  0.06295151  0.06150201  0.06194898  0.06235583
  0.06083768  0.05868371  0.05739727  0.05674433  0.05533408  0.05380558
  0.05243707  0.05093134  0.0494459   0.0487867   0.04860206  0.04807997
  0.04803994  0.04854611  0.04824303  0.04676895  0.04573546  0.04609929
  0.04602131  0.04520812  0.04499717  0.04580482  0.04584679  0.04527589
  0.04503028  0.04490168  0.04395507  0.0424634   0.0414136   0.04090001
  0.04018183  0.03952541  0.03946319  0.03960714  0.03909037  0.03814494
  0.03694741  0.03617889  0.0367803   0.03743159  0.03592118  0.03377662
  0.03344889  0.03407467  0.03334926  0.03179243  0.03153805  0.03283309
  0.03285341  0.03101452  0.02966443  0.02984184  0.03033663  0.02948076
  0.02795272  0.02696753  0.02656301  0.02636845  0.0262562   0.02625895
  0.02617893  0.02630362  0.02677462  0.02703985  0.02721934  0.02705053
  0.02634355  0.02557577  0.02519117  0.02548438  0.02580071  0.02606579
  0.02589771  0.0247175   0.02288276  0.0218557   0.02283656  0.02339493
  0.02232561  0.02022869  0.01990282  0.02090211  0.02119667  0.02036493
  0.0195386   0.01890573  0.01897825  0.01888735  0.01840873  0.01853011
  0.01929283  0.0186517   0.01707719  0.01620637  0.01653278  0.01590824
  0.01380086  0.01205559  0.01193981  0.01156042  0.0106825   0.01025884
  0.01069036  0.00956899  0.0070759   0.00532764  0.00480864  0.00480031
  0.00414555  0.00312173  0.00377185  0.0046045   0.00375219  0.00217217
  0.00212544  0.00328371  0.0019717  -0.0006047   0.0017143   0.00769822]
