Args in experiment:
Namespace(is_training=1, model_id='weather_720_336', model='FITS', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=3, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=82, base_T=144, H_order=12)
Use GPU: cuda:0
>>>>>>>start training : weather_720_336_FITS_custom_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35832
val 4935
test 10204
Model(
  (freq_upsampler): Linear(in_features=82, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26449920.0
params:  9960.0
Trainable parameters:  9960
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5902390
	speed: 0.0925s/iter; left time: 765.1678s
	iters: 200, epoch: 1 | loss: 0.5080203
	speed: 0.0909s/iter; left time: 742.9382s
Epoch: 1 cost time: 25.81587529182434
Epoch: 1, Steps: 279 | Train Loss: 0.5773288 Vali Loss: 0.6337664 Test Loss: 0.2957543
Validation loss decreased (inf --> 0.633766).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3880009
	speed: 0.3686s/iter; left time: 2946.2034s
	iters: 200, epoch: 2 | loss: 0.3841800
	speed: 0.0932s/iter; left time: 735.8487s
Epoch: 2 cost time: 28.228137969970703
Epoch: 2, Steps: 279 | Train Loss: 0.3769344 Vali Loss: 0.5926303 Test Loss: 0.2799486
Validation loss decreased (0.633766 --> 0.592630).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3449760
	speed: 0.3871s/iter; left time: 2985.7357s
	iters: 200, epoch: 3 | loss: 0.3061431
	speed: 0.0884s/iter; left time: 673.0696s
Epoch: 3 cost time: 28.44297432899475
Epoch: 3, Steps: 279 | Train Loss: 0.3096001 Vali Loss: 0.5723375 Test Loss: 0.2724034
Validation loss decreased (0.592630 --> 0.572338).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2429466
	speed: 0.3752s/iter; left time: 2789.2146s
	iters: 200, epoch: 4 | loss: 0.2388951
	speed: 0.0992s/iter; left time: 727.8813s
Epoch: 4 cost time: 29.814151525497437
Epoch: 4, Steps: 279 | Train Loss: 0.2749337 Vali Loss: 0.5587366 Test Loss: 0.2674964
Validation loss decreased (0.572338 --> 0.558737).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2336807
	speed: 0.3946s/iter; left time: 2823.4447s
	iters: 200, epoch: 5 | loss: 0.2466502
	speed: 0.1059s/iter; left time: 747.3104s
Epoch: 5 cost time: 30.31518840789795
Epoch: 5, Steps: 279 | Train Loss: 0.2566455 Vali Loss: 0.5481147 Test Loss: 0.2646101
Validation loss decreased (0.558737 --> 0.548115).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2485216
	speed: 0.3691s/iter; left time: 2537.9185s
	iters: 200, epoch: 6 | loss: 0.2440688
	speed: 0.0995s/iter; left time: 674.1347s
Epoch: 6 cost time: 28.061107873916626
Epoch: 6, Steps: 279 | Train Loss: 0.2466713 Vali Loss: 0.5427256 Test Loss: 0.2627917
Validation loss decreased (0.548115 --> 0.542726).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2341290
	speed: 0.3565s/iter; left time: 2351.9396s
	iters: 200, epoch: 7 | loss: 0.2022634
	speed: 0.1057s/iter; left time: 686.9909s
Epoch: 7 cost time: 29.048897743225098
Epoch: 7, Steps: 279 | Train Loss: 0.2417727 Vali Loss: 0.5413370 Test Loss: 0.2615568
Validation loss decreased (0.542726 --> 0.541337).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2756117
	speed: 0.3958s/iter; left time: 2500.8625s
	iters: 200, epoch: 8 | loss: 0.2230335
	speed: 0.0933s/iter; left time: 580.3720s
Epoch: 8 cost time: 27.9436457157135
Epoch: 8, Steps: 279 | Train Loss: 0.2392797 Vali Loss: 0.5381421 Test Loss: 0.2612591
Validation loss decreased (0.541337 --> 0.538142).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2335741
	speed: 0.3751s/iter; left time: 2265.2951s
	iters: 200, epoch: 9 | loss: 0.1983333
	speed: 0.1134s/iter; left time: 673.4951s
Epoch: 9 cost time: 30.62458038330078
Epoch: 9, Steps: 279 | Train Loss: 0.2379998 Vali Loss: 0.5385523 Test Loss: 0.2612973
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2406438
	speed: 0.3845s/iter; left time: 2214.9673s
	iters: 200, epoch: 10 | loss: 0.2879331
	speed: 0.1083s/iter; left time: 613.0765s
Epoch: 10 cost time: 30.622163772583008
Epoch: 10, Steps: 279 | Train Loss: 0.2373659 Vali Loss: 0.5376456 Test Loss: 0.2611862
Validation loss decreased (0.538142 --> 0.537646).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2347672
	speed: 0.3722s/iter; left time: 2039.9262s
	iters: 200, epoch: 11 | loss: 0.2762046
	speed: 0.0928s/iter; left time: 499.5273s
Epoch: 11 cost time: 28.457040548324585
Epoch: 11, Steps: 279 | Train Loss: 0.2370984 Vali Loss: 0.5392504 Test Loss: 0.2609035
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2534651
	speed: 0.3893s/iter; left time: 2024.9425s
	iters: 200, epoch: 12 | loss: 0.1784568
	speed: 0.0963s/iter; left time: 491.1222s
Epoch: 12 cost time: 29.273712873458862
Epoch: 12, Steps: 279 | Train Loss: 0.2371757 Vali Loss: 0.5383087 Test Loss: 0.2610204
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2719955
	speed: 0.3852s/iter; left time: 1896.4439s
	iters: 200, epoch: 13 | loss: 0.2352999
	speed: 0.0855s/iter; left time: 412.5360s
Epoch: 13 cost time: 25.685410261154175
Epoch: 13, Steps: 279 | Train Loss: 0.2371091 Vali Loss: 0.5376658 Test Loss: 0.2609197
EarlyStopping counter: 3 out of 3
Early stopping
train 35832
val 4935
test 10204
Model(
  (freq_upsampler): Linear(in_features=82, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26449920.0
params:  9960.0
Trainable parameters:  9960
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4353805
	speed: 0.1038s/iter; left time: 858.6194s
	iters: 200, epoch: 1 | loss: 0.7178611
	speed: 0.1045s/iter; left time: 853.5296s
Epoch: 1 cost time: 27.81844997406006
Epoch: 1, Steps: 279 | Train Loss: 0.5337340 Vali Loss: 0.5376081 Test Loss: 0.2617515
Validation loss decreased (inf --> 0.537608).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4645744
	speed: 0.3334s/iter; left time: 2664.4473s
	iters: 200, epoch: 2 | loss: 0.5604839
	speed: 0.1037s/iter; left time: 818.3188s
Epoch: 2 cost time: 28.741477489471436
Epoch: 2, Steps: 279 | Train Loss: 0.5326277 Vali Loss: 0.5348715 Test Loss: 0.2608934
Validation loss decreased (0.537608 --> 0.534871).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4465319
	speed: 0.3559s/iter; left time: 2744.7629s
	iters: 200, epoch: 3 | loss: 0.6265575
	speed: 0.0849s/iter; left time: 646.2287s
Epoch: 3 cost time: 25.293938636779785
Epoch: 3, Steps: 279 | Train Loss: 0.5324916 Vali Loss: 0.5345227 Test Loss: 0.2607009
Validation loss decreased (0.534871 --> 0.534523).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5553309
	speed: 0.3682s/iter; left time: 2736.9748s
	iters: 200, epoch: 4 | loss: 0.5228858
	speed: 0.1047s/iter; left time: 767.8225s
Epoch: 4 cost time: 29.992796659469604
Epoch: 4, Steps: 279 | Train Loss: 0.5315496 Vali Loss: 0.5334058 Test Loss: 0.2603811
Validation loss decreased (0.534523 --> 0.533406).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4057692
	speed: 0.3772s/iter; left time: 2698.7363s
	iters: 200, epoch: 5 | loss: 0.6297357
	speed: 0.0888s/iter; left time: 626.7703s
Epoch: 5 cost time: 26.044063329696655
Epoch: 5, Steps: 279 | Train Loss: 0.5320244 Vali Loss: 0.5346853 Test Loss: 0.2601135
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4871037
	speed: 0.3697s/iter; left time: 2541.7199s
	iters: 200, epoch: 6 | loss: 0.4622437
	speed: 0.1021s/iter; left time: 692.0034s
Epoch: 6 cost time: 28.31857132911682
Epoch: 6, Steps: 279 | Train Loss: 0.5315335 Vali Loss: 0.5361974 Test Loss: 0.2603181
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5135334
	speed: 0.3358s/iter; left time: 2215.1519s
	iters: 200, epoch: 7 | loss: 0.6114630
	speed: 0.0944s/iter; left time: 613.1096s
Epoch: 7 cost time: 26.60519790649414
Epoch: 7, Steps: 279 | Train Loss: 0.5314473 Vali Loss: 0.5352846 Test Loss: 0.2602112
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : weather_720_336_FITS_custom_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10204
mse:0.2610086588859558, mae:0.29875313425064087, rse:0.6671180129051208, corr:[0.47892562 0.48314798 0.48461837 0.48448965 0.4836903  0.48299167
 0.48267123 0.4825921  0.48242888 0.4820082  0.48139545 0.48080993
 0.48038188 0.48015478 0.48000425 0.4797341  0.4793084  0.47856826
 0.4777552  0.47696087 0.476348   0.4759437  0.47567943 0.47530323
 0.4746928  0.47380385 0.4727186  0.47148943 0.4704071  0.4695153
 0.4688588  0.4682669  0.46765888 0.46685585 0.4659492  0.46496642
 0.4641116  0.46334186 0.46268928 0.46209875 0.46156657 0.46109164
 0.4605302  0.45987627 0.45918444 0.4584811  0.45788515 0.4574186
 0.45691022 0.45643726 0.45600852 0.4555239  0.4550818  0.4546059
 0.4540938  0.45352218 0.45299128 0.45247978 0.4520313  0.4516267
 0.45126048 0.45087087 0.45048907 0.4500949  0.44971466 0.44939417
 0.44902638 0.4487075  0.44841993 0.448179   0.44796187 0.44759881
 0.4471612  0.4465823  0.44583926 0.44509757 0.4444803  0.44410098
 0.44389552 0.4437856  0.44384143 0.44397223 0.4439646  0.44376445
 0.44337967 0.44282287 0.4423476  0.4419106  0.44162416 0.44138885
 0.44127384 0.44123888 0.44117236 0.44108263 0.4409025  0.44062382
 0.44030464 0.44000724 0.43975797 0.43955493 0.43936568 0.43916106
 0.43893752 0.43863836 0.438271   0.43787974 0.4374002  0.43696326
 0.4365509  0.4361988  0.43587223 0.43552566 0.43511406 0.4347769
 0.4344887  0.434206   0.43398166 0.43379673 0.43358    0.4332325
 0.43284687 0.43244103 0.432025   0.4316274  0.43125862 0.43090853
 0.430519   0.43007463 0.42960075 0.4291092  0.428595   0.42811012
 0.4277084  0.42738464 0.42716312 0.42694315 0.42667034 0.42639363
 0.42600638 0.42563426 0.42530525 0.42504364 0.4248148  0.42456067
 0.42423785 0.4238468  0.42340532 0.42292613 0.42235148 0.4216575
 0.42092767 0.4202077  0.41948906 0.41875297 0.41798714 0.4171986
 0.41659793 0.4159697  0.4153676  0.41469303 0.4140974  0.41346842
 0.41287062 0.41236976 0.41192997 0.41157734 0.4111417  0.41065413
 0.41014352 0.4095222  0.40883696 0.40816826 0.40756524 0.40716323
 0.4067723  0.40643954 0.4060617  0.40566424 0.405159   0.40458655
 0.40409946 0.4036161  0.40316132 0.40271476 0.40226397 0.4018243
 0.4013913  0.40092802 0.4005963  0.40036827 0.40024275 0.40014574
 0.40010923 0.39983803 0.3994281  0.39888984 0.3982981  0.39778653
 0.39747965 0.3974295  0.3975893  0.39780548 0.39787066 0.39762637
 0.39714026 0.39652368 0.3958908  0.3953228  0.3948368  0.39455268
 0.39430898 0.394047   0.39367887 0.39325798 0.39278167 0.39226508
 0.3918527  0.39164823 0.39154133 0.3913252  0.39100626 0.39063758
 0.39017773 0.38969547 0.38926372 0.38888296 0.3886591  0.38862506
 0.38867083 0.38869193 0.3885997  0.38842255 0.38812807 0.38776496
 0.38739666 0.38707072 0.38672736 0.38642398 0.38595653 0.3855076
 0.38515463 0.38495484 0.3848921  0.3849764  0.38518864 0.38537407
 0.38536784 0.3852486  0.3848648  0.38446903 0.3840733  0.38383707
 0.3836982  0.3836034  0.3835302  0.383325   0.38299727 0.38254035
 0.38217387 0.38196808 0.3817872  0.38174063 0.38175848 0.38167316
 0.38151154 0.38116676 0.3807552  0.3803861  0.38012847 0.38001537
 0.3799661  0.37988764 0.3797707  0.3793978  0.3788398  0.37817782
 0.3775108  0.37694672 0.37664175 0.37652743 0.37649333 0.37642118
 0.3762297  0.37576926 0.3751077  0.3743344  0.37357265 0.37287965
 0.37225074 0.37165824 0.3709792  0.3701424  0.36927086 0.36845168
 0.36771628 0.3671773  0.36679518 0.36648834 0.3660789  0.36562592
 0.36492184 0.36409602 0.3633162  0.36270568 0.3623083  0.3619886
 0.36165047 0.3610267  0.36033377 0.35955185 0.35866752 0.35788503
 0.35743466 0.35723248 0.35716867 0.35711262 0.35690206 0.35664877
 0.3565447  0.35639837 0.3561943  0.35593677 0.3553768  0.35427755
 0.35269713 0.35119036 0.35003948 0.3496481  0.3500486  0.3507851
 0.35132536 0.3511132  0.3499972  0.34838545 0.3471936  0.34784624]
