Args in experiment:
Namespace(is_training=1, model_id='weather_336_192', model='FITS', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=3, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=46, base_T=144, H_order=12)
Use GPU: cuda:0
>>>>>>>start training : weather_336_192_FITS_custom_ftM_sl336_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36360
val 5079
test 10348
Model(
  (freq_upsampler): Linear(in_features=46, out_features=72, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8902656.0
params:  3384.0
Trainable parameters:  3384
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5041378
	speed: 0.1083s/iter; left time: 912.2482s
	iters: 200, epoch: 1 | loss: 0.5228622
	speed: 0.0959s/iter; left time: 797.7686s
Epoch: 1 cost time: 27.993848085403442
Epoch: 1, Steps: 284 | Train Loss: 0.5080339 Vali Loss: 0.5734916 Test Loss: 0.2501605
Validation loss decreased (inf --> 0.573492).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2981630
	speed: 0.3701s/iter; left time: 3011.4996s
	iters: 200, epoch: 2 | loss: 0.2838331
	speed: 0.0983s/iter; left time: 789.9746s
Epoch: 2 cost time: 28.41022300720215
Epoch: 2, Steps: 284 | Train Loss: 0.3280715 Vali Loss: 0.5378612 Test Loss: 0.2363981
Validation loss decreased (0.573492 --> 0.537861).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3487745
	speed: 0.3629s/iter; left time: 2850.0823s
	iters: 200, epoch: 3 | loss: 0.2322872
	speed: 0.0872s/iter; left time: 676.3252s
Epoch: 3 cost time: 25.76567554473877
Epoch: 3, Steps: 284 | Train Loss: 0.2814414 Vali Loss: 0.5288036 Test Loss: 0.2309991
Validation loss decreased (0.537861 --> 0.528804).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3652435
	speed: 0.3557s/iter; left time: 2692.5760s
	iters: 200, epoch: 4 | loss: 0.2497891
	speed: 0.0862s/iter; left time: 644.0348s
Epoch: 4 cost time: 26.609204530715942
Epoch: 4, Steps: 284 | Train Loss: 0.2630989 Vali Loss: 0.5230371 Test Loss: 0.2272928
Validation loss decreased (0.528804 --> 0.523037).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2252653
	speed: 0.3567s/iter; left time: 2598.6640s
	iters: 200, epoch: 5 | loss: 0.2188573
	speed: 0.0872s/iter; left time: 626.5844s
Epoch: 5 cost time: 26.746217966079712
Epoch: 5, Steps: 284 | Train Loss: 0.2547770 Vali Loss: 0.5214328 Test Loss: 0.2251803
Validation loss decreased (0.523037 --> 0.521433).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2510853
	speed: 0.3434s/iter; left time: 2403.8265s
	iters: 200, epoch: 6 | loss: 0.3654525
	speed: 0.0815s/iter; left time: 562.2415s
Epoch: 6 cost time: 22.913479566574097
Epoch: 6, Steps: 284 | Train Loss: 0.2508397 Vali Loss: 0.5194319 Test Loss: 0.2239931
Validation loss decreased (0.521433 --> 0.519432).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2884697
	speed: 0.3081s/iter; left time: 2069.4734s
	iters: 200, epoch: 7 | loss: 0.3238129
	speed: 0.0874s/iter; left time: 578.2741s
Epoch: 7 cost time: 25.54889225959778
Epoch: 7, Steps: 284 | Train Loss: 0.2490141 Vali Loss: 0.5192792 Test Loss: 0.2230378
Validation loss decreased (0.519432 --> 0.519279).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2801549
	speed: 0.3246s/iter; left time: 2088.3489s
	iters: 200, epoch: 8 | loss: 0.1971187
	speed: 0.0943s/iter; left time: 597.2203s
Epoch: 8 cost time: 26.90573024749756
Epoch: 8, Steps: 284 | Train Loss: 0.2481727 Vali Loss: 0.5188366 Test Loss: 0.2228644
Validation loss decreased (0.519279 --> 0.518837).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1845191
	speed: 0.3580s/iter; left time: 2201.2137s
	iters: 200, epoch: 9 | loss: 0.2003125
	speed: 0.0924s/iter; left time: 559.0934s
Epoch: 9 cost time: 27.31811833381653
Epoch: 9, Steps: 284 | Train Loss: 0.2476607 Vali Loss: 0.5197075 Test Loss: 0.2226385
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2165672
	speed: 0.3543s/iter; left time: 2078.1478s
	iters: 200, epoch: 10 | loss: 0.2139833
	speed: 0.0934s/iter; left time: 538.6857s
Epoch: 10 cost time: 26.067047119140625
Epoch: 10, Steps: 284 | Train Loss: 0.2476230 Vali Loss: 0.5206491 Test Loss: 0.2226101
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1973937
	speed: 0.3459s/iter; left time: 1930.5767s
	iters: 200, epoch: 11 | loss: 0.2613097
	speed: 0.0963s/iter; left time: 528.0615s
Epoch: 11 cost time: 26.88719630241394
Epoch: 11, Steps: 284 | Train Loss: 0.2475634 Vali Loss: 0.5193701 Test Loss: 0.2223971
EarlyStopping counter: 3 out of 3
Early stopping
train 36360
val 5079
test 10348
Model(
  (freq_upsampler): Linear(in_features=46, out_features=72, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8902656.0
params:  3384.0
Trainable parameters:  3384
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4213278
	speed: 0.0971s/iter; left time: 817.9413s
	iters: 200, epoch: 1 | loss: 0.5266107
	speed: 0.0918s/iter; left time: 764.0474s
Epoch: 1 cost time: 26.173879146575928
Epoch: 1, Steps: 284 | Train Loss: 0.5165174 Vali Loss: 0.5099623 Test Loss: 0.2202728
Validation loss decreased (inf --> 0.509962).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4288714
	speed: 0.3475s/iter; left time: 2827.2582s
	iters: 200, epoch: 2 | loss: 0.4565042
	speed: 0.0881s/iter; left time: 708.4575s
Epoch: 2 cost time: 26.34758758544922
Epoch: 2, Steps: 284 | Train Loss: 0.5137257 Vali Loss: 0.5120385 Test Loss: 0.2200419
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4384899
	speed: 0.3651s/iter; left time: 2867.2934s
	iters: 200, epoch: 3 | loss: 0.3812250
	speed: 0.0950s/iter; left time: 736.4905s
Epoch: 3 cost time: 27.374234437942505
Epoch: 3, Steps: 284 | Train Loss: 0.5129701 Vali Loss: 0.5112388 Test Loss: 0.2194435
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5915759
	speed: 0.3529s/iter; left time: 2671.0652s
	iters: 200, epoch: 4 | loss: 0.4994994
	speed: 0.0851s/iter; left time: 635.6328s
Epoch: 4 cost time: 27.054842948913574
Epoch: 4, Steps: 284 | Train Loss: 0.5126390 Vali Loss: 0.5097573 Test Loss: 0.2191949
Validation loss decreased (0.509962 --> 0.509757).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.7806740
	speed: 0.3273s/iter; left time: 2384.2038s
	iters: 200, epoch: 5 | loss: 0.4069591
	speed: 0.0581s/iter; left time: 417.2038s
Epoch: 5 cost time: 18.93627119064331
Epoch: 5, Steps: 284 | Train Loss: 0.5124997 Vali Loss: 0.5077014 Test Loss: 0.2187868
Validation loss decreased (0.509757 --> 0.507701).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4104225
	speed: 0.2991s/iter; left time: 2094.2800s
	iters: 200, epoch: 6 | loss: 0.5912780
	speed: 0.0862s/iter; left time: 595.0626s
Epoch: 6 cost time: 25.488070964813232
Epoch: 6, Steps: 284 | Train Loss: 0.5124075 Vali Loss: 0.5101615 Test Loss: 0.2188710
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5531939
	speed: 0.3170s/iter; left time: 2129.3467s
	iters: 200, epoch: 7 | loss: 0.6024418
	speed: 0.0765s/iter; left time: 506.4230s
Epoch: 7 cost time: 23.778212547302246
Epoch: 7, Steps: 284 | Train Loss: 0.5122835 Vali Loss: 0.5094895 Test Loss: 0.2185706
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5695390
	speed: 0.3507s/iter; left time: 2255.7806s
	iters: 200, epoch: 8 | loss: 0.4081075
	speed: 0.0829s/iter; left time: 525.2314s
Epoch: 8 cost time: 25.852739572525024
Epoch: 8, Steps: 284 | Train Loss: 0.5122459 Vali Loss: 0.5082139 Test Loss: 0.2187990
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : weather_336_192_FITS_custom_ftM_sl336_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10348
mse:0.21895022690296173, mae:0.2623574435710907, rse:0.615943431854248, corr:[0.4825949  0.4855268  0.4849982  0.48357597 0.48268336 0.4824152
 0.48221233 0.48160025 0.48042437 0.47908708 0.47811896 0.47771186
 0.47754753 0.4772531  0.47655305 0.47566545 0.47473595 0.47385818
 0.47332478 0.47288802 0.4723371  0.47158563 0.4707353  0.46982062
 0.4690491  0.46828738 0.4674487  0.46630833 0.46502835 0.46369007
 0.46255153 0.46161872 0.4608796  0.4601062  0.45929113 0.45831022
 0.45732462 0.45636097 0.4555429  0.45476437 0.45402685 0.45342377
 0.4527704  0.4520601  0.45130572 0.4505639  0.44988897 0.44937095
 0.44873968 0.4480674  0.44739524 0.44675958 0.4463067  0.4459255
 0.4455816  0.44504917 0.44442558 0.44375628 0.44317573 0.44275868
 0.44259098 0.44252384 0.4423777  0.4421333  0.44179666 0.44150856
 0.44106865 0.44076374 0.4405145  0.44032007 0.44011635 0.43988004
 0.43951878 0.43910986 0.4386665  0.43828914 0.43787745 0.4375543
 0.43731526 0.4371191  0.43705353 0.43705642 0.43705863 0.4369829
 0.43691602 0.43674436 0.436718   0.43663603 0.4366363  0.4366683
 0.4366857  0.43655914 0.43634737 0.4361747  0.43603837 0.43587494
 0.43571866 0.43557656 0.43540984 0.43519923 0.43498793 0.43481302
 0.43470564 0.43464407 0.43454796 0.43432623 0.4339571  0.4335218
 0.43303412 0.43262556 0.4323227  0.4321633  0.43203947 0.43190604
 0.4317115  0.4314195  0.4310724  0.4307243  0.43038195 0.4300618
 0.42975262 0.42947945 0.42917913 0.42885485 0.42858362 0.42832473
 0.42809883 0.4278134  0.42743537 0.42696342 0.4264341  0.42591128
 0.42547587 0.42513195 0.42488375 0.42464685 0.42436758 0.42405483
 0.4237023  0.4233498  0.4230422  0.4227214  0.42242372 0.42210335
 0.42173073 0.42135578 0.42101192 0.420609   0.42014688 0.41957167
 0.41901028 0.4184583  0.4179575  0.41751444 0.4169268  0.41620186
 0.4155302  0.4148806  0.41425842 0.41354623 0.41300052 0.41249552
 0.4120431  0.41164473 0.41126418 0.41094226 0.41050756 0.40991
 0.4092314  0.4083976  0.40758795 0.40692553 0.40626898 0.4058375
 0.4052383  0.40460545 0.40389147 0.4033736  0.40307012 0.40279502
 0.40244925 0.4019227  0.40123263 0.4004243  0.39963794 0.39916363
 0.39887047 0.39853713 0.39838806 0.39824852 0.39776057 0.39627814]
