Args in experiment:
Namespace(is_training=1, model_id='ETTh2_96_96', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=40, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_96_96_FITS_ETTh2_ftM_sl96_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8449
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=40, out_features=80, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2867200.0
params:  3280.0
Trainable parameters:  3280
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.986346960067749
Epoch: 1, Steps: 66 | Train Loss: 0.4647456 Vali Loss: 0.3063536 Test Loss: 0.4042702
Validation loss decreased (inf --> 0.306354).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.8278427124023438
Epoch: 2, Steps: 66 | Train Loss: 0.3875618 Vali Loss: 0.2840461 Test Loss: 0.3709452
Validation loss decreased (0.306354 --> 0.284046).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.7335543632507324
Epoch: 3, Steps: 66 | Train Loss: 0.3431882 Vali Loss: 0.2671969 Test Loss: 0.3512123
Validation loss decreased (0.284046 --> 0.267197).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.0255956649780273
Epoch: 4, Steps: 66 | Train Loss: 0.3149546 Vali Loss: 0.2569157 Test Loss: 0.3390104
Validation loss decreased (0.267197 --> 0.256916).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.035865306854248
Epoch: 5, Steps: 66 | Train Loss: 0.2960391 Vali Loss: 0.2504975 Test Loss: 0.3308789
Validation loss decreased (0.256916 --> 0.250498).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.107269287109375
Epoch: 6, Steps: 66 | Train Loss: 0.2828469 Vali Loss: 0.2461187 Test Loss: 0.3252690
Validation loss decreased (0.250498 --> 0.246119).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.0647969245910645
Epoch: 7, Steps: 66 | Train Loss: 0.2731899 Vali Loss: 0.2426437 Test Loss: 0.3212763
Validation loss decreased (0.246119 --> 0.242644).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.781451225280762
Epoch: 8, Steps: 66 | Train Loss: 0.2658448 Vali Loss: 0.2406518 Test Loss: 0.3181907
Validation loss decreased (0.242644 --> 0.240652).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.3421225547790527
Epoch: 9, Steps: 66 | Train Loss: 0.2600770 Vali Loss: 0.2384844 Test Loss: 0.3157764
Validation loss decreased (0.240652 --> 0.238484).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2867650985717773
Epoch: 10, Steps: 66 | Train Loss: 0.2553939 Vali Loss: 0.2347866 Test Loss: 0.3137790
Validation loss decreased (0.238484 --> 0.234787).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.2575881481170654
Epoch: 11, Steps: 66 | Train Loss: 0.2515257 Vali Loss: 0.2339802 Test Loss: 0.3120871
Validation loss decreased (0.234787 --> 0.233980).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.2931458950042725
Epoch: 12, Steps: 66 | Train Loss: 0.2481848 Vali Loss: 0.2332584 Test Loss: 0.3105278
Validation loss decreased (0.233980 --> 0.233258).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.186445474624634
Epoch: 13, Steps: 66 | Train Loss: 0.2454417 Vali Loss: 0.2320108 Test Loss: 0.3092606
Validation loss decreased (0.233258 --> 0.232011).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.8233752250671387
Epoch: 14, Steps: 66 | Train Loss: 0.2429873 Vali Loss: 0.2299379 Test Loss: 0.3081243
Validation loss decreased (0.232011 --> 0.229938).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.7422502040863037
Epoch: 15, Steps: 66 | Train Loss: 0.2408300 Vali Loss: 0.2290157 Test Loss: 0.3070385
Validation loss decreased (0.229938 --> 0.229016).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.155935049057007
Epoch: 16, Steps: 66 | Train Loss: 0.2389505 Vali Loss: 0.2276142 Test Loss: 0.3060925
Validation loss decreased (0.229016 --> 0.227614).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.236661195755005
Epoch: 17, Steps: 66 | Train Loss: 0.2372282 Vali Loss: 0.2264613 Test Loss: 0.3051534
Validation loss decreased (0.227614 --> 0.226461).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.1459734439849854
Epoch: 18, Steps: 66 | Train Loss: 0.2357478 Vali Loss: 0.2271372 Test Loss: 0.3043687
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.1238255500793457
Epoch: 19, Steps: 66 | Train Loss: 0.2343651 Vali Loss: 0.2256410 Test Loss: 0.3036328
Validation loss decreased (0.226461 --> 0.225641).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.7010953426361084
Epoch: 20, Steps: 66 | Train Loss: 0.2331686 Vali Loss: 0.2256866 Test Loss: 0.3029627
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.0900063514709473
Epoch: 21, Steps: 66 | Train Loss: 0.2320886 Vali Loss: 0.2241257 Test Loss: 0.3023283
Validation loss decreased (0.225641 --> 0.224126).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.2068421840667725
Epoch: 22, Steps: 66 | Train Loss: 0.2310849 Vali Loss: 0.2241635 Test Loss: 0.3017986
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.92372989654541
Epoch: 23, Steps: 66 | Train Loss: 0.2300823 Vali Loss: 0.2230368 Test Loss: 0.3012628
Validation loss decreased (0.224126 --> 0.223037).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.840665817260742
Epoch: 24, Steps: 66 | Train Loss: 0.2293393 Vali Loss: 0.2235884 Test Loss: 0.3007726
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.892406940460205
Epoch: 25, Steps: 66 | Train Loss: 0.2285131 Vali Loss: 0.2213397 Test Loss: 0.3003105
Validation loss decreased (0.223037 --> 0.221340).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.91475772857666
Epoch: 26, Steps: 66 | Train Loss: 0.2278886 Vali Loss: 0.2223621 Test Loss: 0.2999149
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.3368375301361084
Epoch: 27, Steps: 66 | Train Loss: 0.2272450 Vali Loss: 0.2214658 Test Loss: 0.2995160
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.810117721557617
Epoch: 28, Steps: 66 | Train Loss: 0.2266684 Vali Loss: 0.2209471 Test Loss: 0.2991440
Validation loss decreased (0.221340 --> 0.220947).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.670083999633789
Epoch: 29, Steps: 66 | Train Loss: 0.2261086 Vali Loss: 0.2212251 Test Loss: 0.2988429
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.6907706260681152
Epoch: 30, Steps: 66 | Train Loss: 0.2256122 Vali Loss: 0.2203565 Test Loss: 0.2985242
Validation loss decreased (0.220947 --> 0.220357).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 8449
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=40, out_features=80, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2867200.0
params:  3280.0
Trainable parameters:  3280
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.911011219024658
Epoch: 1, Steps: 66 | Train Loss: 0.4288426 Vali Loss: 0.2168443 Test Loss: 0.2945420
Validation loss decreased (inf --> 0.216844).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.5109124183654785
Epoch: 2, Steps: 66 | Train Loss: 0.4243707 Vali Loss: 0.2146660 Test Loss: 0.2924056
Validation loss decreased (0.216844 --> 0.214666).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.585444450378418
Epoch: 3, Steps: 66 | Train Loss: 0.4219932 Vali Loss: 0.2116576 Test Loss: 0.2912543
Validation loss decreased (0.214666 --> 0.211658).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.5494067668914795
Epoch: 4, Steps: 66 | Train Loss: 0.4205654 Vali Loss: 0.2122897 Test Loss: 0.2906345
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.6487109661102295
Epoch: 5, Steps: 66 | Train Loss: 0.4197125 Vali Loss: 0.2115380 Test Loss: 0.2903084
Validation loss decreased (0.211658 --> 0.211538).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.73852276802063
Epoch: 6, Steps: 66 | Train Loss: 0.4191107 Vali Loss: 0.2110456 Test Loss: 0.2901955
Validation loss decreased (0.211538 --> 0.211046).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.598601818084717
Epoch: 7, Steps: 66 | Train Loss: 0.4186059 Vali Loss: 0.2116655 Test Loss: 0.2900600
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.6115801334381104
Epoch: 8, Steps: 66 | Train Loss: 0.4182584 Vali Loss: 0.2104244 Test Loss: 0.2899604
Validation loss decreased (0.211046 --> 0.210424).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.638364315032959
Epoch: 9, Steps: 66 | Train Loss: 0.4179579 Vali Loss: 0.2106494 Test Loss: 0.2898740
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.565166711807251
Epoch: 10, Steps: 66 | Train Loss: 0.4177669 Vali Loss: 0.2115780 Test Loss: 0.2897909
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.7089009284973145
Epoch: 11, Steps: 66 | Train Loss: 0.4175456 Vali Loss: 0.2113391 Test Loss: 0.2898131
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.7940971851348877
Epoch: 12, Steps: 66 | Train Loss: 0.4174147 Vali Loss: 0.2100623 Test Loss: 0.2897775
Validation loss decreased (0.210424 --> 0.210062).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.661693811416626
Epoch: 13, Steps: 66 | Train Loss: 0.4171894 Vali Loss: 0.2098304 Test Loss: 0.2897378
Validation loss decreased (0.210062 --> 0.209830).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.5008373260498047
Epoch: 14, Steps: 66 | Train Loss: 0.4170754 Vali Loss: 0.2097053 Test Loss: 0.2897250
Validation loss decreased (0.209830 --> 0.209705).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.5861146450042725
Epoch: 15, Steps: 66 | Train Loss: 0.4169671 Vali Loss: 0.2090913 Test Loss: 0.2897210
Validation loss decreased (0.209705 --> 0.209091).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5794899463653564
Epoch: 16, Steps: 66 | Train Loss: 0.4168722 Vali Loss: 0.2102064 Test Loss: 0.2896847
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.60684871673584
Epoch: 17, Steps: 66 | Train Loss: 0.4167846 Vali Loss: 0.2105499 Test Loss: 0.2897294
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.6198055744171143
Epoch: 18, Steps: 66 | Train Loss: 0.4166427 Vali Loss: 0.2104445 Test Loss: 0.2896706
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.660762071609497
Epoch: 19, Steps: 66 | Train Loss: 0.4165925 Vali Loss: 0.2104801 Test Loss: 0.2896238
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.6400046348571777
Epoch: 20, Steps: 66 | Train Loss: 0.4165522 Vali Loss: 0.2098776 Test Loss: 0.2896737
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh2_96_96_FITS_ETTh2_ftM_sl96_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.29025450348854065, mae:0.3390117287635803, rse:0.43418097496032715, corr:[0.27759442 0.2772685  0.27597094 0.27568427 0.2739516  0.27338076
 0.27187398 0.27138013 0.27008054 0.26917908 0.26791337 0.26639974
 0.26449215 0.26263905 0.26171482 0.2609688  0.260862   0.26056492
 0.26003462 0.25913158 0.25819075 0.2573495  0.25623676 0.2545889
 0.2516701  0.24953458 0.24780649 0.2462655  0.24480574 0.24319108
 0.24192524 0.24068363 0.2389111  0.23693237 0.23590274 0.23467359
 0.23292434 0.2315021  0.23068176 0.2296143  0.22856979 0.22873151
 0.2281847  0.22732835 0.22656198 0.22521307 0.22467208 0.22278115
 0.21964763 0.21686523 0.21526647 0.21362247 0.21173504 0.21081945
 0.20821908 0.20655905 0.20505044 0.20261103 0.20146896 0.20001143
 0.19952437 0.19849698 0.19828053 0.19854127 0.19753444 0.19751255
 0.19675578 0.19623296 0.19648637 0.19541945 0.19513594 0.19393158
 0.19184625 0.19061524 0.18993352 0.18901825 0.18691616 0.18646577
 0.18579362 0.1848744  0.18398789 0.18335766 0.18318398 0.1828638
 0.18219376 0.18103017 0.18267804 0.18194611 0.1807455  0.18111742
 0.18011352 0.18029138 0.1783825  0.17944707 0.17801815 0.18124385]
