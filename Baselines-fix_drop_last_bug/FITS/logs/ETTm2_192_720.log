Args in experiment:
Namespace(is_training=1, model_id='ETTm2_192_720', model='FITS', data='ETTm2', root_path='./dataset/', data_path='ETTm2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=64, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_192_720_FITS_ETTm2_ftM_sl192_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=64, out_features=304, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17432576.0
params:  19760.0
Trainable parameters:  19760
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4676675
	speed: 0.0652s/iter; left time: 505.8003s
	iters: 200, epoch: 1 | loss: 0.4538487
	speed: 0.0568s/iter; left time: 434.9509s
Epoch: 1 cost time: 15.885910749435425
Epoch: 1, Steps: 262 | Train Loss: 0.5603599 Vali Loss: 0.3037108 Test Loss: 0.4183012
Validation loss decreased (inf --> 0.303711).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5066002
	speed: 0.2539s/iter; left time: 1903.8108s
	iters: 200, epoch: 2 | loss: 0.4180580
	speed: 0.0544s/iter; left time: 402.3134s
Epoch: 2 cost time: 16.093944311141968
Epoch: 2, Steps: 262 | Train Loss: 0.4612651 Vali Loss: 0.2874680 Test Loss: 0.3992228
Validation loss decreased (0.303711 --> 0.287468).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4896172
	speed: 0.2457s/iter; left time: 1778.4004s
	iters: 200, epoch: 3 | loss: 0.4350443
	speed: 0.0556s/iter; left time: 396.9472s
Epoch: 3 cost time: 14.877170085906982
Epoch: 3, Steps: 262 | Train Loss: 0.4490181 Vali Loss: 0.2823200 Test Loss: 0.3934462
Validation loss decreased (0.287468 --> 0.282320).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4753599
	speed: 0.2389s/iter; left time: 1666.3128s
	iters: 200, epoch: 4 | loss: 0.4003975
	speed: 0.0521s/iter; left time: 358.3482s
Epoch: 4 cost time: 14.729095935821533
Epoch: 4, Steps: 262 | Train Loss: 0.4446706 Vali Loss: 0.2797823 Test Loss: 0.3905704
Validation loss decreased (0.282320 --> 0.279782).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4369736
	speed: 0.2501s/iter; left time: 1679.1818s
	iters: 200, epoch: 5 | loss: 0.3889715
	speed: 0.0572s/iter; left time: 378.0951s
Epoch: 5 cost time: 15.913665771484375
Epoch: 5, Steps: 262 | Train Loss: 0.4426916 Vali Loss: 0.2781953 Test Loss: 0.3889743
Validation loss decreased (0.279782 --> 0.278195).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5739611
	speed: 0.2339s/iter; left time: 1508.8467s
	iters: 200, epoch: 6 | loss: 0.3819379
	speed: 0.0538s/iter; left time: 341.8951s
Epoch: 6 cost time: 14.227062702178955
Epoch: 6, Steps: 262 | Train Loss: 0.4407407 Vali Loss: 0.2772484 Test Loss: 0.3880900
Validation loss decreased (0.278195 --> 0.277248).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4195900
	speed: 0.2336s/iter; left time: 1445.5880s
	iters: 200, epoch: 7 | loss: 0.5708533
	speed: 0.0473s/iter; left time: 288.0611s
Epoch: 7 cost time: 13.313036918640137
Epoch: 7, Steps: 262 | Train Loss: 0.4401365 Vali Loss: 0.2767867 Test Loss: 0.3875499
Validation loss decreased (0.277248 --> 0.276787).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4259624
	speed: 0.2192s/iter; left time: 1298.9989s
	iters: 200, epoch: 8 | loss: 0.4099849
	speed: 0.0433s/iter; left time: 252.1090s
Epoch: 8 cost time: 12.620041847229004
Epoch: 8, Steps: 262 | Train Loss: 0.4396228 Vali Loss: 0.2767660 Test Loss: 0.3871083
Validation loss decreased (0.276787 --> 0.276766).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5291684
	speed: 0.1930s/iter; left time: 1093.3826s
	iters: 200, epoch: 9 | loss: 0.3648002
	speed: 0.0559s/iter; left time: 311.0118s
Epoch: 9 cost time: 14.368101835250854
Epoch: 9, Steps: 262 | Train Loss: 0.4392333 Vali Loss: 0.2763185 Test Loss: 0.3868928
Validation loss decreased (0.276766 --> 0.276318).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5179068
	speed: 0.2456s/iter; left time: 1326.7476s
	iters: 200, epoch: 10 | loss: 0.3811447
	speed: 0.0532s/iter; left time: 282.1056s
Epoch: 10 cost time: 15.041576623916626
Epoch: 10, Steps: 262 | Train Loss: 0.4385832 Vali Loss: 0.2763966 Test Loss: 0.3867201
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3471720
	speed: 0.2446s/iter; left time: 1257.3623s
	iters: 200, epoch: 11 | loss: 0.4336002
	speed: 0.0573s/iter; left time: 288.6015s
Epoch: 11 cost time: 15.276405096054077
Epoch: 11, Steps: 262 | Train Loss: 0.4379392 Vali Loss: 0.2760528 Test Loss: 0.3865209
Validation loss decreased (0.276318 --> 0.276053).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4703050
	speed: 0.2479s/iter; left time: 1209.3039s
	iters: 200, epoch: 12 | loss: 0.4031426
	speed: 0.0520s/iter; left time: 248.2988s
Epoch: 12 cost time: 14.684165954589844
Epoch: 12, Steps: 262 | Train Loss: 0.4385228 Vali Loss: 0.2760881 Test Loss: 0.3863944
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5097752
	speed: 0.2376s/iter; left time: 1096.8943s
	iters: 200, epoch: 13 | loss: 0.4283791
	speed: 0.0498s/iter; left time: 225.1714s
Epoch: 13 cost time: 14.04983901977539
Epoch: 13, Steps: 262 | Train Loss: 0.4382979 Vali Loss: 0.2759160 Test Loss: 0.3862602
Validation loss decreased (0.276053 --> 0.275916).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5217924
	speed: 0.2190s/iter; left time: 953.5450s
	iters: 200, epoch: 14 | loss: 0.3372073
	speed: 0.0505s/iter; left time: 214.8994s
Epoch: 14 cost time: 13.667929410934448
Epoch: 14, Steps: 262 | Train Loss: 0.4376014 Vali Loss: 0.2758229 Test Loss: 0.3861717
Validation loss decreased (0.275916 --> 0.275823).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5188932
	speed: 0.2193s/iter; left time: 897.7287s
	iters: 200, epoch: 15 | loss: 0.3668545
	speed: 0.0452s/iter; left time: 180.5639s
Epoch: 15 cost time: 13.327580213546753
Epoch: 15, Steps: 262 | Train Loss: 0.4377150 Vali Loss: 0.2761414 Test Loss: 0.3860815
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4957707
	speed: 0.2293s/iter; left time: 878.4292s
	iters: 200, epoch: 16 | loss: 0.3831354
	speed: 0.0486s/iter; left time: 181.3180s
Epoch: 16 cost time: 13.63431715965271
Epoch: 16, Steps: 262 | Train Loss: 0.4376581 Vali Loss: 0.2759123 Test Loss: 0.3860373
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4132221
	speed: 0.2122s/iter; left time: 757.2557s
	iters: 200, epoch: 17 | loss: 0.4266885
	speed: 0.0528s/iter; left time: 183.2098s
Epoch: 17 cost time: 13.860708475112915
Epoch: 17, Steps: 262 | Train Loss: 0.4373397 Vali Loss: 0.2757863 Test Loss: 0.3859800
Validation loss decreased (0.275823 --> 0.275786).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5020183
	speed: 0.2321s/iter; left time: 767.4240s
	iters: 200, epoch: 18 | loss: 0.3591746
	speed: 0.0480s/iter; left time: 153.7941s
Epoch: 18 cost time: 13.78237509727478
Epoch: 18, Steps: 262 | Train Loss: 0.4374539 Vali Loss: 0.2757264 Test Loss: 0.3859301
Validation loss decreased (0.275786 --> 0.275726).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4103882
	speed: 0.2252s/iter; left time: 685.7912s
	iters: 200, epoch: 19 | loss: 0.4917935
	speed: 0.0481s/iter; left time: 141.6898s
Epoch: 19 cost time: 13.944347381591797
Epoch: 19, Steps: 262 | Train Loss: 0.4376469 Vali Loss: 0.2760318 Test Loss: 0.3858841
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5163152
	speed: 0.2301s/iter; left time: 640.2579s
	iters: 200, epoch: 20 | loss: 0.6632097
	speed: 0.0478s/iter; left time: 128.2935s
Epoch: 20 cost time: 13.293893098831177
Epoch: 20, Steps: 262 | Train Loss: 0.4372102 Vali Loss: 0.2756585 Test Loss: 0.3858267
Validation loss decreased (0.275726 --> 0.275658).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3118927
	speed: 0.2384s/iter; left time: 601.0021s
	iters: 200, epoch: 21 | loss: 0.4727362
	speed: 0.0477s/iter; left time: 115.4821s
Epoch: 21 cost time: 13.9530508518219
Epoch: 21, Steps: 262 | Train Loss: 0.4370975 Vali Loss: 0.2756900 Test Loss: 0.3858140
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3754283
	speed: 0.2364s/iter; left time: 533.9573s
	iters: 200, epoch: 22 | loss: 0.4966404
	speed: 0.0498s/iter; left time: 107.5535s
Epoch: 22 cost time: 14.1824471950531
Epoch: 22, Steps: 262 | Train Loss: 0.4373858 Vali Loss: 0.2758832 Test Loss: 0.3857541
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3442889
	speed: 0.2103s/iter; left time: 419.9214s
	iters: 200, epoch: 23 | loss: 0.4607331
	speed: 0.0466s/iter; left time: 88.3172s
Epoch: 23 cost time: 12.043130159378052
Epoch: 23, Steps: 262 | Train Loss: 0.4374478 Vali Loss: 0.2757281 Test Loss: 0.3857619
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4827877
	speed: 0.1821s/iter; left time: 315.8978s
	iters: 200, epoch: 24 | loss: 0.4960951
	speed: 0.0372s/iter; left time: 60.7805s
Epoch: 24 cost time: 10.707440376281738
Epoch: 24, Steps: 262 | Train Loss: 0.4375022 Vali Loss: 0.2757904 Test Loss: 0.3857503
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.6468433
	speed: 0.1892s/iter; left time: 278.7570s
	iters: 200, epoch: 25 | loss: 0.3881786
	speed: 0.0424s/iter; left time: 58.1559s
Epoch: 25 cost time: 12.18796157836914
Epoch: 25, Steps: 262 | Train Loss: 0.4367049 Vali Loss: 0.2759057 Test Loss: 0.3857269
EarlyStopping counter: 5 out of 5
Early stopping
train 33649
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=64, out_features=304, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17432576.0
params:  19760.0
Trainable parameters:  19760
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5231369
	speed: 0.0627s/iter; left time: 486.5891s
	iters: 200, epoch: 1 | loss: 0.8132808
	speed: 0.0488s/iter; left time: 373.9964s
Epoch: 1 cost time: 14.047025918960571
Epoch: 1, Steps: 262 | Train Loss: 0.5520260 Vali Loss: 0.2759895 Test Loss: 0.3855376
Validation loss decreased (inf --> 0.275990).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5452960
	speed: 0.2261s/iter; left time: 1695.4217s
	iters: 200, epoch: 2 | loss: 0.6833789
	speed: 0.0488s/iter; left time: 360.7263s
Epoch: 2 cost time: 14.028345108032227
Epoch: 2, Steps: 262 | Train Loss: 0.5519979 Vali Loss: 0.2752666 Test Loss: 0.3853230
Validation loss decreased (0.275990 --> 0.275267).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3833235
	speed: 0.2297s/iter; left time: 1662.0948s
	iters: 200, epoch: 3 | loss: 0.6382306
	speed: 0.0468s/iter; left time: 333.9012s
Epoch: 3 cost time: 13.725817918777466
Epoch: 3, Steps: 262 | Train Loss: 0.5517827 Vali Loss: 0.2754626 Test Loss: 0.3853115
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5708914
	speed: 0.2500s/iter; left time: 1744.0055s
	iters: 200, epoch: 4 | loss: 0.4224454
	speed: 0.0583s/iter; left time: 400.9872s
Epoch: 4 cost time: 15.7806875705719
Epoch: 4, Steps: 262 | Train Loss: 0.5521191 Vali Loss: 0.2755560 Test Loss: 0.3852135
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6237099
	speed: 0.2485s/iter; left time: 1668.4037s
	iters: 200, epoch: 5 | loss: 0.4775648
	speed: 0.0555s/iter; left time: 367.2993s
Epoch: 5 cost time: 15.554134607315063
Epoch: 5, Steps: 262 | Train Loss: 0.5515438 Vali Loss: 0.2753634 Test Loss: 0.3852531
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4837019
	speed: 0.2505s/iter; left time: 1616.2218s
	iters: 200, epoch: 6 | loss: 0.4950772
	speed: 0.0578s/iter; left time: 366.8347s
Epoch: 6 cost time: 15.268953084945679
Epoch: 6, Steps: 262 | Train Loss: 0.5511789 Vali Loss: 0.2754783 Test Loss: 0.3851903
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5517561
	speed: 0.2477s/iter; left time: 1532.7612s
	iters: 200, epoch: 7 | loss: 0.7654679
	speed: 0.0558s/iter; left time: 339.8904s
Epoch: 7 cost time: 15.865517139434814
Epoch: 7, Steps: 262 | Train Loss: 0.5517031 Vali Loss: 0.2752302 Test Loss: 0.3851927
Validation loss decreased (0.275267 --> 0.275230).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6588201
	speed: 0.2541s/iter; left time: 1506.1559s
	iters: 200, epoch: 8 | loss: 0.4821182
	speed: 0.0574s/iter; left time: 334.6436s
Epoch: 8 cost time: 15.544196367263794
Epoch: 8, Steps: 262 | Train Loss: 0.5516302 Vali Loss: 0.2756174 Test Loss: 0.3852383
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5661331
	speed: 0.2464s/iter; left time: 1395.7609s
	iters: 200, epoch: 9 | loss: 0.4719133
	speed: 0.0553s/iter; left time: 307.6459s
Epoch: 9 cost time: 15.094859600067139
Epoch: 9, Steps: 262 | Train Loss: 0.5511633 Vali Loss: 0.2754458 Test Loss: 0.3850857
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4566125
	speed: 0.2539s/iter; left time: 1372.0314s
	iters: 200, epoch: 10 | loss: 0.5373755
	speed: 0.0542s/iter; left time: 287.3445s
Epoch: 10 cost time: 15.482182025909424
Epoch: 10, Steps: 262 | Train Loss: 0.5508038 Vali Loss: 0.2754637 Test Loss: 0.3850474
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5193651
	speed: 0.2551s/iter; left time: 1311.4301s
	iters: 200, epoch: 11 | loss: 0.5169775
	speed: 0.0556s/iter; left time: 280.1625s
Epoch: 11 cost time: 15.721766710281372
Epoch: 11, Steps: 262 | Train Loss: 0.5512636 Vali Loss: 0.2754776 Test Loss: 0.3850826
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.7616696
	speed: 0.2631s/iter; left time: 1283.5776s
	iters: 200, epoch: 12 | loss: 0.5255257
	speed: 0.0522s/iter; left time: 249.6166s
Epoch: 12 cost time: 15.431461811065674
Epoch: 12, Steps: 262 | Train Loss: 0.5507439 Vali Loss: 0.2753767 Test Loss: 0.3850866
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTm2_192_720_FITS_ETTm2_ftM_sl192_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.38246580958366394, mae:0.3870040774345398, rse:0.4970964789390564, corr:[0.5532786  0.5475139  0.5444589  0.54427767 0.5414291  0.5405471
 0.53916204 0.53798133 0.5380841  0.53698057 0.53599    0.5356525
 0.5346816  0.53437287 0.5343245  0.5333594  0.5326898  0.53206325
 0.5311797  0.5309217  0.530387   0.52932084 0.5288627  0.52833426
 0.5274143  0.52718145 0.52699566 0.5264076  0.5259144  0.5250666
 0.5241328  0.5237054  0.5229841  0.52215    0.52182764 0.5214416
 0.520842   0.52047735 0.51980877 0.51900786 0.5186047  0.5181285
 0.5173609  0.5169001  0.51644117 0.5157664  0.51524055 0.51459646
 0.5136502  0.5128363  0.5119924  0.51115507 0.5104776  0.5095577
 0.5088506  0.50850266 0.5078419  0.5071133  0.5066841  0.50625634
 0.50594723 0.50581414 0.505535   0.5051877  0.50516754 0.50503075
 0.50466615 0.50452125 0.5042245  0.5038388  0.503775   0.5037724
 0.50351083 0.50323594 0.50282633 0.50225306 0.50187844 0.50161505
 0.5012992  0.50087035 0.5005441  0.5003146  0.49994785 0.4993164
 0.4987082  0.4983913  0.4982499  0.497978   0.49737078 0.49684557
 0.49659333 0.4961991  0.49563667 0.49518618 0.4942635  0.49255458
 0.49074945 0.48922798 0.48780552 0.4864761  0.48505023 0.483533
 0.4823072  0.48117313 0.47964355 0.478035   0.4766165  0.47536358
 0.47428274 0.47316197 0.47166896 0.47028828 0.46926433 0.4682133
 0.4671367  0.46618915 0.464815   0.4632663  0.4623929  0.46170092
 0.46076655 0.45964792 0.4585689  0.45752203 0.45661226 0.45567232
 0.45474234 0.45392665 0.45297366 0.45188403 0.4509046  0.45000353
 0.44926906 0.44862297 0.4474953  0.4462959  0.4457413  0.4454196
 0.44467947 0.44400433 0.4433418  0.44265202 0.44237247 0.44190326
 0.44081023 0.43985942 0.4392379  0.43835852 0.4374706  0.4365454
 0.43551585 0.43502092 0.43495417 0.43431246 0.43345264 0.43299678
 0.4325919  0.43220696 0.43203762 0.43191215 0.43165424 0.43145147
 0.43147555 0.4312855  0.43102932 0.43078944 0.43051857 0.43027648
 0.4303258  0.43047604 0.4301704  0.42988425 0.43026367 0.43047222
 0.43021452 0.43040773 0.43066263 0.4300667  0.4295813  0.42992854
 0.4299113  0.42953986 0.42971706 0.4299717  0.429874   0.42977977
 0.42972827 0.42955494 0.42950335 0.42926762 0.42838055 0.42694363
 0.42544568 0.4243572  0.42350096 0.422153   0.4204083  0.4193068
 0.41851166 0.41719553 0.4156067  0.4141339  0.412745   0.4114001
 0.4099709  0.40887573 0.40836546 0.40749264 0.40585196 0.4047048
 0.4040671  0.40315124 0.40203163 0.4008524  0.3995863  0.39867428
 0.39811492 0.397302   0.39626026 0.39518383 0.3942484  0.39342886
 0.3920937  0.390507   0.3897485  0.38945332 0.3888193  0.38764524
 0.38671145 0.38586402 0.3847962  0.38395357 0.3832503  0.3822714
 0.38158455 0.3817084  0.38192016 0.38169292 0.38162926 0.38135356
 0.38069114 0.38006923 0.3795362  0.3791987  0.37900096 0.3788025
 0.37846416 0.37835175 0.37849364 0.3781879  0.3780057  0.3780537
 0.37769377 0.37730384 0.3771056  0.37726054 0.37756982 0.37749523
 0.37682635 0.37657145 0.37706456 0.37728015 0.376951   0.3768938
 0.37720817 0.37733543 0.37752068 0.37783927 0.37812105 0.37837532
 0.37841156 0.37844157 0.3786967  0.37872013 0.37831613 0.3782135
 0.37841502 0.37841514 0.37816608 0.37816322 0.37841436 0.37866345
 0.3784598  0.3780247  0.37793425 0.37785667 0.37722254 0.3761185
 0.37477943 0.37364    0.37305573 0.37249103 0.37142006 0.37046438
 0.36977932 0.3687482  0.36766624 0.36677635 0.36572975 0.36464205
 0.3637174  0.36266747 0.36177057 0.36113155 0.35987213 0.35843584
 0.35772765 0.3571045  0.35606128 0.35531256 0.3547518  0.3539091
 0.35327876 0.35286802 0.3520357  0.35133258 0.3509605  0.35002097
 0.34864444 0.34780636 0.34740597 0.34679615 0.3462939  0.34605902
 0.34557548 0.3448575  0.34393448 0.34301326 0.34252226 0.34243616
 0.3423763  0.34232643 0.34220085 0.3416366  0.3407167  0.33980286
 0.33906803 0.3389556  0.33868548 0.33758578 0.33683476 0.3369303
 0.3367569  0.33672598 0.33723673 0.3370856  0.33643872 0.3364188
 0.33642814 0.33598265 0.33604732 0.33633345 0.33612612 0.3361782
 0.33647254 0.33676058 0.33749214 0.33795977 0.33734718 0.3370567
 0.33760428 0.33750224 0.33714285 0.33768135 0.3380593  0.33775946
 0.3377586  0.3379063  0.3379458  0.33852214 0.33907053 0.3383163
 0.33745944 0.33841753 0.340146   0.34031805 0.33908966 0.33875513
 0.33964577 0.33987677 0.3392799  0.33898795 0.33839077 0.33684123
 0.33580554 0.33616188 0.33638516 0.33555424 0.33446002 0.3336324
 0.3325998  0.33108187 0.32970852 0.3290613  0.32840502 0.3269025
 0.32555544 0.32522467 0.32488543 0.32384807 0.32277203 0.3223215
 0.32188582 0.32056388 0.31897816 0.31835118 0.31770316 0.31604442
 0.31462783 0.31450123 0.31433398 0.31337875 0.31234762 0.31156248
 0.31118762 0.3113919  0.3112755  0.310251   0.30925074 0.30925545
 0.30906123 0.30794558 0.30685756 0.30689934 0.30698735 0.30639353
 0.30649704 0.3072261  0.30687916 0.30606344 0.30664417 0.30738977
 0.30649573 0.30509856 0.30424744 0.3035072  0.30298305 0.3033424
 0.304147   0.30351812 0.30122203 0.30069888 0.30242306 0.30246294
 0.30049875 0.30033445 0.30105    0.30031154 0.29990244 0.30016282
 0.29935214 0.29883477 0.2992057  0.2989485  0.29885414 0.2997224
 0.30014154 0.30037364 0.30101964 0.3013235  0.30155164 0.30145833
 0.30067274 0.30107254 0.30222246 0.30157453 0.30110672 0.30248475
 0.30234557 0.30075958 0.30127236 0.3020455  0.30105242 0.30105388
 0.3022507  0.30210227 0.30123264 0.30049103 0.29924244 0.29829773
 0.2980664  0.29732656 0.29659608 0.2965923  0.29613173 0.2949701
 0.29368868 0.29195833 0.29047337 0.290178   0.28964564 0.28790405
 0.2866527  0.286431   0.28556186 0.28403732 0.28310135 0.28234038
 0.28121245 0.2803594  0.2797779  0.2787598  0.27751297 0.27651167
 0.27560487 0.2749836  0.27459496 0.27402687 0.2732544  0.27239913
 0.27175313 0.27121794 0.27009335 0.2687648  0.26835087 0.2683827
 0.2678341  0.2667215  0.26590255 0.26575089 0.26581573 0.26525605
 0.26483598 0.26518187 0.26517102 0.26439533 0.26387542 0.26301473
 0.2619418  0.26169375 0.2614538  0.26046145 0.25987288 0.25962815
 0.25884187 0.2587447  0.25936732 0.2591756  0.2587805  0.25834936
 0.25699785 0.25640672 0.25765675 0.25822818 0.25765464 0.2576246
 0.25759873 0.25657913 0.2555957  0.25590733 0.2572057  0.25768584
 0.25684327 0.2571132  0.2582235  0.25768512 0.2571956  0.25862378
 0.25884342 0.25739124 0.25712088 0.2573887  0.2570182  0.25725237
 0.2575626  0.25736445 0.2579173  0.25808635 0.25700215 0.25688145
 0.25752428 0.2570732  0.25679398 0.2569092  0.25577602 0.25448176
 0.2539264  0.25263244 0.25131118 0.25098804 0.25002137 0.2481842
 0.24697958 0.24605794 0.24501912 0.244297   0.24309692 0.24144796
 0.24082567 0.24056417 0.23936248 0.2381216  0.2371349  0.23556195
 0.23419788 0.23347965 0.232204   0.23090832 0.2307144  0.23025984
 0.22930653 0.22926188 0.22935201 0.22858538 0.22796673 0.22760528
 0.22674169 0.22583538 0.22494785 0.22425705 0.22418508 0.22391234
 0.2230475  0.22283413 0.22346605 0.22314483 0.22232732 0.22214462
 0.22216994 0.22237563 0.22279194 0.22297128 0.22266732 0.22229306
 0.22246303 0.2229008  0.22244409 0.22118579 0.22087267 0.22100586
 0.21995303 0.21911184 0.21964635 0.22004516 0.21997179 0.219581
 0.21844727 0.21816494 0.21934374 0.21982162 0.21938767 0.21931894
 0.21933    0.21982194 0.22114477 0.22129047 0.2210229  0.22220737
 0.22306265 0.22282629 0.22352259 0.22369583 0.222204   0.22263834
 0.22508761 0.22594579 0.22581314 0.22649732 0.22698016 0.22720541
 0.22782902 0.22828488 0.22866948 0.22910897 0.2292073  0.22940359
 0.22982182 0.2296567  0.22922374 0.22904587 0.22846526 0.22724918
 0.22600387 0.22528498 0.22488266 0.22394663 0.2225394  0.22193268
 0.22157496 0.22001958 0.21850528 0.21821737 0.2176633  0.21643838
 0.21564667 0.21483408 0.21365145 0.21246907 0.21076351 0.2092634
 0.20914271 0.20874502 0.20705073 0.2060586  0.20516971 0.20305353
 0.20156427 0.20076834 0.1993001  0.19856603 0.19814342 0.19621842
 0.1949109  0.19450185 0.1926352  0.19125095 0.19146717 0.19081484
 0.18977864 0.18938398 0.18821584 0.187871   0.18776228 0.18664266
 0.18727769 0.18821017 0.18861616 0.19238544 0.19139393 0.19557355]
