Args in experiment:
Namespace(is_training=1, model_id='ETTh1_192_336', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=64, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_192_336_FITS_ETTh1_ftM_sl192_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8113
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=64, out_features=176, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10092544.0
params:  11440.0
Trainable parameters:  11440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.183697462081909
Epoch: 1, Steps: 63 | Train Loss: 0.8001022 Vali Loss: 1.9454355 Test Loss: 0.9398255
Validation loss decreased (inf --> 1.945436).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.089960813522339
Epoch: 2, Steps: 63 | Train Loss: 0.6200542 Vali Loss: 1.7220021 Test Loss: 0.7819272
Validation loss decreased (1.945436 --> 1.722002).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.41862154006958
Epoch: 3, Steps: 63 | Train Loss: 0.5269597 Vali Loss: 1.6039543 Test Loss: 0.7012947
Validation loss decreased (1.722002 --> 1.603954).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.3819053173065186
Epoch: 4, Steps: 63 | Train Loss: 0.4770614 Vali Loss: 1.5421104 Test Loss: 0.6576630
Validation loss decreased (1.603954 --> 1.542110).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.110395669937134
Epoch: 5, Steps: 63 | Train Loss: 0.4479917 Vali Loss: 1.5022240 Test Loss: 0.6315178
Validation loss decreased (1.542110 --> 1.502224).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.4051482677459717
Epoch: 6, Steps: 63 | Train Loss: 0.4290474 Vali Loss: 1.4717804 Test Loss: 0.6138141
Validation loss decreased (1.502224 --> 1.471780).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.802123785018921
Epoch: 7, Steps: 63 | Train Loss: 0.4159308 Vali Loss: 1.4504062 Test Loss: 0.6004350
Validation loss decreased (1.471780 --> 1.450406).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.193368673324585
Epoch: 8, Steps: 63 | Train Loss: 0.4055062 Vali Loss: 1.4310437 Test Loss: 0.5892326
Validation loss decreased (1.450406 --> 1.431044).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.9535317420959473
Epoch: 9, Steps: 63 | Train Loss: 0.3968096 Vali Loss: 1.4185050 Test Loss: 0.5798191
Validation loss decreased (1.431044 --> 1.418505).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.6335902214050293
Epoch: 10, Steps: 63 | Train Loss: 0.3898548 Vali Loss: 1.4127777 Test Loss: 0.5718468
Validation loss decreased (1.418505 --> 1.412778).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.5762202739715576
Epoch: 11, Steps: 63 | Train Loss: 0.3832758 Vali Loss: 1.3896580 Test Loss: 0.5639373
Validation loss decreased (1.412778 --> 1.389658).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.590721607208252
Epoch: 12, Steps: 63 | Train Loss: 0.3778986 Vali Loss: 1.3848664 Test Loss: 0.5572497
Validation loss decreased (1.389658 --> 1.384866).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.053666114807129
Epoch: 13, Steps: 63 | Train Loss: 0.3729997 Vali Loss: 1.3766712 Test Loss: 0.5509768
Validation loss decreased (1.384866 --> 1.376671).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.396322011947632
Epoch: 14, Steps: 63 | Train Loss: 0.3686636 Vali Loss: 1.3682014 Test Loss: 0.5453850
Validation loss decreased (1.376671 --> 1.368201).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.041052579879761
Epoch: 15, Steps: 63 | Train Loss: 0.3649111 Vali Loss: 1.3557396 Test Loss: 0.5401515
Validation loss decreased (1.368201 --> 1.355740).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.5557174682617188
Epoch: 16, Steps: 63 | Train Loss: 0.3613747 Vali Loss: 1.3562174 Test Loss: 0.5355075
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.488694429397583
Epoch: 17, Steps: 63 | Train Loss: 0.3581886 Vali Loss: 1.3441126 Test Loss: 0.5312089
Validation loss decreased (1.355740 --> 1.344113).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.4294676780700684
Epoch: 18, Steps: 63 | Train Loss: 0.3554106 Vali Loss: 1.3426851 Test Loss: 0.5274537
Validation loss decreased (1.344113 --> 1.342685).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.8618037700653076
Epoch: 19, Steps: 63 | Train Loss: 0.3527589 Vali Loss: 1.3323410 Test Loss: 0.5237056
Validation loss decreased (1.342685 --> 1.332341).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.2980289459228516
Epoch: 20, Steps: 63 | Train Loss: 0.3503131 Vali Loss: 1.3290925 Test Loss: 0.5203477
Validation loss decreased (1.332341 --> 1.329093).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.102972507476807
Epoch: 21, Steps: 63 | Train Loss: 0.3482956 Vali Loss: 1.3268902 Test Loss: 0.5172009
Validation loss decreased (1.329093 --> 1.326890).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.5055947303771973
Epoch: 22, Steps: 63 | Train Loss: 0.3462631 Vali Loss: 1.3201782 Test Loss: 0.5144933
Validation loss decreased (1.326890 --> 1.320178).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.4751858711242676
Epoch: 23, Steps: 63 | Train Loss: 0.3443309 Vali Loss: 1.3168463 Test Loss: 0.5118380
Validation loss decreased (1.320178 --> 1.316846).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.0505473613739014
Epoch: 24, Steps: 63 | Train Loss: 0.3425498 Vali Loss: 1.3074367 Test Loss: 0.5093104
Validation loss decreased (1.316846 --> 1.307437).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.9875316619873047
Epoch: 25, Steps: 63 | Train Loss: 0.3410766 Vali Loss: 1.3070046 Test Loss: 0.5070847
Validation loss decreased (1.307437 --> 1.307005).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.0913634300231934
Epoch: 26, Steps: 63 | Train Loss: 0.3397270 Vali Loss: 1.3072495 Test Loss: 0.5049156
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.0776193141937256
Epoch: 27, Steps: 63 | Train Loss: 0.3382489 Vali Loss: 1.3036258 Test Loss: 0.5030088
Validation loss decreased (1.307005 --> 1.303626).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.376955509185791
Epoch: 28, Steps: 63 | Train Loss: 0.3371834 Vali Loss: 1.2999212 Test Loss: 0.5012425
Validation loss decreased (1.303626 --> 1.299921).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.4656524658203125
Epoch: 29, Steps: 63 | Train Loss: 0.3358272 Vali Loss: 1.3013800 Test Loss: 0.4995611
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.1269187927246094
Epoch: 30, Steps: 63 | Train Loss: 0.3346221 Vali Loss: 1.3005271 Test Loss: 0.4978857
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00011296777049628277
train 8113
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=64, out_features=176, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10092544.0
params:  11440.0
Trainable parameters:  11440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0923826694488525
Epoch: 1, Steps: 63 | Train Loss: 0.4908881 Vali Loss: 1.2640061 Test Loss: 0.4759645
Validation loss decreased (inf --> 1.264006).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.102891683578491
Epoch: 2, Steps: 63 | Train Loss: 0.4787260 Vali Loss: 1.2445755 Test Loss: 0.4634604
Validation loss decreased (1.264006 --> 1.244576).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.202009439468384
Epoch: 3, Steps: 63 | Train Loss: 0.4717994 Vali Loss: 1.2382407 Test Loss: 0.4565342
Validation loss decreased (1.244576 --> 1.238241).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.5783474445343018
Epoch: 4, Steps: 63 | Train Loss: 0.4673505 Vali Loss: 1.2288584 Test Loss: 0.4529471
Validation loss decreased (1.238241 --> 1.228858).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.5757992267608643
Epoch: 5, Steps: 63 | Train Loss: 0.4649613 Vali Loss: 1.2207719 Test Loss: 0.4514495
Validation loss decreased (1.228858 --> 1.220772).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.128143548965454
Epoch: 6, Steps: 63 | Train Loss: 0.4639905 Vali Loss: 1.2192322 Test Loss: 0.4508792
Validation loss decreased (1.220772 --> 1.219232).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.2402942180633545
Epoch: 7, Steps: 63 | Train Loss: 0.4628939 Vali Loss: 1.2246923 Test Loss: 0.4508456
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.6851956844329834
Epoch: 8, Steps: 63 | Train Loss: 0.4624147 Vali Loss: 1.2215840 Test Loss: 0.4510769
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.5598626136779785
Epoch: 9, Steps: 63 | Train Loss: 0.4621040 Vali Loss: 1.2185236 Test Loss: 0.4513194
Validation loss decreased (1.219232 --> 1.218524).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.7985875606536865
Epoch: 10, Steps: 63 | Train Loss: 0.4620226 Vali Loss: 1.2229103 Test Loss: 0.4515234
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.7820188999176025
Epoch: 11, Steps: 63 | Train Loss: 0.4620910 Vali Loss: 1.2167724 Test Loss: 0.4518530
Validation loss decreased (1.218524 --> 1.216772).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3173418045043945
Epoch: 12, Steps: 63 | Train Loss: 0.4616556 Vali Loss: 1.2160136 Test Loss: 0.4518393
Validation loss decreased (1.216772 --> 1.216014).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.6261236667633057
Epoch: 13, Steps: 63 | Train Loss: 0.4618880 Vali Loss: 1.2157830 Test Loss: 0.4520835
Validation loss decreased (1.216014 --> 1.215783).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.826692819595337
Epoch: 14, Steps: 63 | Train Loss: 0.4616660 Vali Loss: 1.2157406 Test Loss: 0.4521058
Validation loss decreased (1.215783 --> 1.215741).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.2226521968841553
Epoch: 15, Steps: 63 | Train Loss: 0.4619973 Vali Loss: 1.2201352 Test Loss: 0.4522893
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.005756616592407
Epoch: 16, Steps: 63 | Train Loss: 0.4613392 Vali Loss: 1.2230039 Test Loss: 0.4522448
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.431689977645874
Epoch: 17, Steps: 63 | Train Loss: 0.4615518 Vali Loss: 1.2198297 Test Loss: 0.4523053
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.5071563720703125
Epoch: 18, Steps: 63 | Train Loss: 0.4618399 Vali Loss: 1.2160618 Test Loss: 0.4523156
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.6872963905334473
Epoch: 19, Steps: 63 | Train Loss: 0.4612766 Vali Loss: 1.2210847 Test Loss: 0.4523612
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_192_336_FITS_ETTh1_ftM_sl192_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4513528048992157, mae:0.4333743453025818, rse:0.6396024823188782, corr:[0.25458524 0.2583497  0.2571842  0.25872624 0.25565478 0.25291672
 0.25237748 0.2516809  0.25155732 0.25181183 0.25105682 0.250854
 0.25132257 0.2508261  0.25041088 0.25059688 0.25051942 0.25060138
 0.2503203  0.249743   0.24967405 0.24980065 0.24986391 0.25024268
 0.24977657 0.24913284 0.2488864  0.248302   0.24752572 0.24735445
 0.24713415 0.24651392 0.24617177 0.24590449 0.24580944 0.24593797
 0.24590985 0.24587701 0.246208   0.2464766  0.24657862 0.24686913
 0.2470793  0.24699126 0.24692851 0.24696217 0.24731459 0.24783674
 0.24772775 0.24693984 0.24577896 0.2446303  0.24369903 0.24261993
 0.24169296 0.24117737 0.24088079 0.24063012 0.2403149  0.24031682
 0.24039763 0.24045546 0.24045867 0.24061279 0.24066724 0.24074446
 0.24112198 0.24111764 0.2407883  0.24075006 0.24092343 0.24073172
 0.23986843 0.23887298 0.23810858 0.23757339 0.23721822 0.23692575
 0.23651956 0.23594394 0.2356427  0.23535949 0.2350857  0.23498662
 0.23479901 0.23466587 0.23476148 0.23467577 0.23461378 0.2347112
 0.23469172 0.23456003 0.23437808 0.23456372 0.23518361 0.23582922
 0.2357845  0.23529394 0.23494352 0.23452474 0.23406045 0.23363931
 0.23348571 0.2335107  0.2332844  0.23308335 0.23303652 0.23289919
 0.23260508 0.23255314 0.23257592 0.23273563 0.2329394  0.23323667
 0.2334685  0.23354292 0.23347262 0.2335402  0.23373441 0.23383395
 0.23359874 0.23309559 0.2323495  0.23129995 0.23049317 0.23011577
 0.2298004  0.22983749 0.22974737 0.22934371 0.22907318 0.22903231
 0.22890058 0.22878912 0.2290772  0.22913079 0.2290743  0.22922572
 0.22943054 0.22947173 0.22945249 0.22954077 0.22978084 0.22967328
 0.2290948  0.22868499 0.22830278 0.22702928 0.225881   0.22536977
 0.22503169 0.22467592 0.2247677  0.22500218 0.2249336  0.22484589
 0.22486947 0.22488603 0.2248741  0.22471125 0.22466622 0.22488569
 0.22503737 0.22499956 0.22515199 0.22549303 0.22568017 0.2259442
 0.22595562 0.22554845 0.2253746  0.22533262 0.22483425 0.22438931
 0.22439927 0.22457947 0.22440034 0.22427338 0.22452833 0.22480077
 0.22470127 0.22461861 0.22472747 0.22479339 0.22478321 0.22509567
 0.2255997  0.22580564 0.22595794 0.22632827 0.22669949 0.22680429
 0.22636968 0.22577755 0.2252148  0.22449182 0.22371723 0.22317806
 0.22276688 0.22274496 0.22274065 0.22252396 0.22235586 0.2222492
 0.22197032 0.222101   0.22234909 0.22213945 0.22195299 0.22203721
 0.22228967 0.22200504 0.22187382 0.22210473 0.22222781 0.22224653
 0.22219701 0.22194183 0.22164367 0.22118801 0.2208409  0.22053699
 0.22002599 0.21975575 0.21993405 0.2199414  0.2193954  0.21918054
 0.2191658  0.21912217 0.21934114 0.21961856 0.21959047 0.21964452
 0.21992649 0.22003393 0.2199274  0.22006282 0.22048278 0.22079122
 0.22086413 0.22073667 0.22050564 0.22011453 0.21977909 0.2194642
 0.21925998 0.21943678 0.21987574 0.21988434 0.21970469 0.21954557
 0.21913482 0.21902843 0.21943864 0.21946092 0.21936513 0.21980163
 0.22021548 0.22004336 0.22007006 0.22044134 0.22071643 0.22057936
 0.22031336 0.21994604 0.2197109  0.21945542 0.21903478 0.21854934
 0.2181606  0.21800521 0.21788143 0.21778904 0.21780369 0.2176526
 0.2171374  0.21692367 0.2171528  0.21723281 0.2170701  0.21736485
 0.2175905  0.21750712 0.21772017 0.21792719 0.21789175 0.21822987
 0.21870875 0.21892762 0.21911359 0.21897386 0.21862707 0.21853489
 0.2185123  0.21854155 0.21861795 0.21864742 0.21903652 0.21919203
 0.21853116 0.21829595 0.21882631 0.21879908 0.21887043 0.21938425
 0.21966058 0.21955621 0.2199082  0.22012313 0.22001505 0.22024165
 0.2205977  0.2204295  0.22037289 0.22008318 0.21945079 0.21872571
 0.21861602 0.21872933 0.21842037 0.21816106 0.21790865 0.21775025
 0.21728891 0.21716732 0.21733557 0.21749221 0.21737851 0.21812999
 0.21811971 0.21726853 0.21808948 0.21746244 0.2173495  0.21944806]
