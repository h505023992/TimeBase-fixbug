Args in experiment:
Namespace(is_training=1, model_id='traffic_96_192', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=34, base_T=36, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_96_192_FITS_custom_ftM_sl96_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11993
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=34, out_features=102, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  382645248.0
params:  3570.0
Trainable parameters:  3570
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 17.69513463973999
Epoch: 1, Steps: 93 | Train Loss: 1.3807239 Vali Loss: 1.4646097 Test Loss: 1.7282825
Validation loss decreased (inf --> 1.464610).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 18.00583004951477
Epoch: 2, Steps: 93 | Train Loss: 0.9527336 Vali Loss: 1.1289160 Test Loss: 1.3346312
Validation loss decreased (1.464610 --> 1.128916).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 17.95902729034424
Epoch: 3, Steps: 93 | Train Loss: 0.7457010 Vali Loss: 0.9648090 Test Loss: 1.1417251
Validation loss decreased (1.128916 --> 0.964809).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 17.512322902679443
Epoch: 4, Steps: 93 | Train Loss: 0.6385158 Vali Loss: 0.8740194 Test Loss: 1.0363443
Validation loss decreased (0.964809 --> 0.874019).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 17.25767159461975
Epoch: 5, Steps: 93 | Train Loss: 0.5743219 Vali Loss: 0.8161574 Test Loss: 0.9694610
Validation loss decreased (0.874019 --> 0.816157).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 18.496943473815918
Epoch: 6, Steps: 93 | Train Loss: 0.5306970 Vali Loss: 0.7734391 Test Loss: 0.9208041
Validation loss decreased (0.816157 --> 0.773439).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 18.68914222717285
Epoch: 7, Steps: 93 | Train Loss: 0.4975515 Vali Loss: 0.7423292 Test Loss: 0.8838980
Validation loss decreased (0.773439 --> 0.742329).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 17.856855154037476
Epoch: 8, Steps: 93 | Train Loss: 0.4707294 Vali Loss: 0.7144298 Test Loss: 0.8518144
Validation loss decreased (0.742329 --> 0.714430).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 17.448506355285645
Epoch: 9, Steps: 93 | Train Loss: 0.4479415 Vali Loss: 0.6912377 Test Loss: 0.8246002
Validation loss decreased (0.714430 --> 0.691238).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 17.95849919319153
Epoch: 10, Steps: 93 | Train Loss: 0.4284614 Vali Loss: 0.6702994 Test Loss: 0.8011630
Validation loss decreased (0.691238 --> 0.670299).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 18.095889806747437
Epoch: 11, Steps: 93 | Train Loss: 0.4115047 Vali Loss: 0.6523305 Test Loss: 0.7810351
Validation loss decreased (0.670299 --> 0.652330).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 18.09818720817566
Epoch: 12, Steps: 93 | Train Loss: 0.3966442 Vali Loss: 0.6379813 Test Loss: 0.7634766
Validation loss decreased (0.652330 --> 0.637981).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 17.90169334411621
Epoch: 13, Steps: 93 | Train Loss: 0.3834483 Vali Loss: 0.6237386 Test Loss: 0.7471804
Validation loss decreased (0.637981 --> 0.623739).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 18.24142575263977
Epoch: 14, Steps: 93 | Train Loss: 0.3718639 Vali Loss: 0.6119083 Test Loss: 0.7336817
Validation loss decreased (0.623739 --> 0.611908).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 17.81827187538147
Epoch: 15, Steps: 93 | Train Loss: 0.3616325 Vali Loss: 0.6005492 Test Loss: 0.7211092
Validation loss decreased (0.611908 --> 0.600549).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 17.993959426879883
Epoch: 16, Steps: 93 | Train Loss: 0.3524968 Vali Loss: 0.5913773 Test Loss: 0.7103224
Validation loss decreased (0.600549 --> 0.591377).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 18.855133056640625
Epoch: 17, Steps: 93 | Train Loss: 0.3441694 Vali Loss: 0.5827622 Test Loss: 0.7003006
Validation loss decreased (0.591377 --> 0.582762).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 19.01364040374756
Epoch: 18, Steps: 93 | Train Loss: 0.3369309 Vali Loss: 0.5757194 Test Loss: 0.6920968
Validation loss decreased (0.582762 --> 0.575719).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 17.729429244995117
Epoch: 19, Steps: 93 | Train Loss: 0.3303582 Vali Loss: 0.5683065 Test Loss: 0.6837863
Validation loss decreased (0.575719 --> 0.568307).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 17.67004919052124
Epoch: 20, Steps: 93 | Train Loss: 0.3243770 Vali Loss: 0.5624880 Test Loss: 0.6770333
Validation loss decreased (0.568307 --> 0.562488).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 17.822415828704834
Epoch: 21, Steps: 93 | Train Loss: 0.3190684 Vali Loss: 0.5567229 Test Loss: 0.6707645
Validation loss decreased (0.562488 --> 0.556723).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 18.240806818008423
Epoch: 22, Steps: 93 | Train Loss: 0.3142761 Vali Loss: 0.5522177 Test Loss: 0.6652098
Validation loss decreased (0.556723 --> 0.552218).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 17.90746235847473
Epoch: 23, Steps: 93 | Train Loss: 0.3099002 Vali Loss: 0.5466249 Test Loss: 0.6599649
Validation loss decreased (0.552218 --> 0.546625).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 18.090906381607056
Epoch: 24, Steps: 93 | Train Loss: 0.3060244 Vali Loss: 0.5432589 Test Loss: 0.6554205
Validation loss decreased (0.546625 --> 0.543259).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 17.627309799194336
Epoch: 25, Steps: 93 | Train Loss: 0.3023898 Vali Loss: 0.5391635 Test Loss: 0.6512959
Validation loss decreased (0.543259 --> 0.539164).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 18.495744705200195
Epoch: 26, Steps: 93 | Train Loss: 0.2991848 Vali Loss: 0.5363977 Test Loss: 0.6476423
Validation loss decreased (0.539164 --> 0.536398).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 19.141834497451782
Epoch: 27, Steps: 93 | Train Loss: 0.2961341 Vali Loss: 0.5334675 Test Loss: 0.6441700
Validation loss decreased (0.536398 --> 0.533468).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 18.916206121444702
Epoch: 28, Steps: 93 | Train Loss: 0.2933715 Vali Loss: 0.5303224 Test Loss: 0.6410872
Validation loss decreased (0.533468 --> 0.530322).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 19.016080856323242
Epoch: 29, Steps: 93 | Train Loss: 0.2909228 Vali Loss: 0.5282496 Test Loss: 0.6382529
Validation loss decreased (0.530322 --> 0.528250).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 18.27696704864502
Epoch: 30, Steps: 93 | Train Loss: 0.2885412 Vali Loss: 0.5257353 Test Loss: 0.6357155
Validation loss decreased (0.528250 --> 0.525735).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11993
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=34, out_features=102, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  382645248.0
params:  3570.0
Trainable parameters:  3570
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 17.902500867843628
Epoch: 1, Steps: 93 | Train Loss: 0.3891786 Vali Loss: 0.5005934 Test Loss: 0.6073529
Validation loss decreased (inf --> 0.500593).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 17.540677547454834
Epoch: 2, Steps: 93 | Train Loss: 0.3754608 Vali Loss: 0.4947813 Test Loss: 0.6027803
Validation loss decreased (0.500593 --> 0.494781).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 17.681854248046875
Epoch: 3, Steps: 93 | Train Loss: 0.3724666 Vali Loss: 0.4944925 Test Loss: 0.6028646
Validation loss decreased (0.494781 --> 0.494493).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 17.62878656387329
Epoch: 4, Steps: 93 | Train Loss: 0.3718994 Vali Loss: 0.4937966 Test Loss: 0.6030915
Validation loss decreased (0.494493 --> 0.493797).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 17.60654091835022
Epoch: 5, Steps: 93 | Train Loss: 0.3718903 Vali Loss: 0.4952210 Test Loss: 0.6031506
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 18.057753801345825
Epoch: 6, Steps: 93 | Train Loss: 0.3717412 Vali Loss: 0.4948222 Test Loss: 0.6031884
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 18.228460550308228
Epoch: 7, Steps: 93 | Train Loss: 0.3718707 Vali Loss: 0.4944590 Test Loss: 0.6031732
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 18.699527740478516
Epoch: 8, Steps: 93 | Train Loss: 0.3717257 Vali Loss: 0.4942370 Test Loss: 0.6031860
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 22.833176374435425
Epoch: 9, Steps: 93 | Train Loss: 0.3717191 Vali Loss: 0.4949237 Test Loss: 0.6031430
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_96_192_FITS_custom_ftM_sl96_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.6028547286987305, mae:0.36560705304145813, rse:0.6408190131187439, corr:[0.27274668 0.2887673  0.2889203  0.28757563 0.28574324 0.28593498
 0.28574666 0.28562182 0.28654957 0.28666756 0.28640458 0.2869121
 0.28615516 0.28500316 0.2852084  0.28532097 0.2851526  0.28567693
 0.285491   0.28497007 0.28513384 0.28484172 0.28507087 0.2916321
 0.30380812 0.30536535 0.3027864  0.30132067 0.30039808 0.30004364
 0.30015177 0.29961655 0.29901618 0.29960194 0.29999495 0.29969352
 0.29996887 0.30001664 0.299391   0.2992007  0.29886633 0.29843572
 0.29960844 0.30080616 0.30066577 0.3018575  0.30393675 0.30222246
 0.2917195  0.2865896  0.2864232  0.28610268 0.28617907 0.2859843
 0.28549528 0.28529269 0.28505605 0.28489888 0.28515005 0.2852164
 0.2851724  0.2856301  0.28595856 0.28572434 0.2853399  0.28542385
 0.28608736 0.2871261  0.28849468 0.29039833 0.29202417 0.28952473
 0.2821865  0.2803664  0.28184196 0.28303376 0.2833686  0.28343615
 0.28322172 0.28266978 0.28236943 0.28219447 0.28199956 0.282145
 0.2824397  0.2824461  0.2824207  0.28247783 0.28220385 0.28189385
 0.2820239  0.2822403  0.28256693 0.2831835  0.2834752  0.28313252
 0.28216207 0.28255445 0.2829004  0.28301805 0.28336766 0.28322268
 0.2827588  0.28264627 0.28262013 0.28240523 0.2825429  0.28292385
 0.2831287  0.28317416 0.2829775  0.2826726  0.2827326  0.2829487
 0.28280917 0.28278127 0.2831071  0.28302363 0.28262475 0.28265685
 0.28287548 0.28275272 0.28264225 0.2826698  0.2824737  0.28240162
 0.28247005 0.28235716 0.2823597  0.28237748 0.28211823 0.2821139
 0.28245705 0.28249666 0.28253376 0.28288195 0.28282118 0.2825764
 0.28299502 0.28345433 0.28321847 0.28297263 0.2826741  0.28220326
 0.28295183 0.28320664 0.28291523 0.28298914 0.28335425 0.2831467
 0.28266132 0.28276983 0.28273308 0.28223848 0.28222942 0.28251347
 0.28250316 0.28278872 0.2832469  0.28320447 0.28312027 0.2831491
 0.2830933  0.2833495  0.28327036 0.28204298 0.28090477 0.28263032
 0.28936026 0.29227346 0.2912642  0.28951663 0.2878209  0.28699955
 0.2864222  0.2858055  0.28615034 0.28680584 0.2865793  0.28667238
 0.28683776 0.2860368  0.2859575  0.28643212 0.2856022  0.28546557
 0.2857937  0.28431156 0.2845209  0.28618887 0.2841873  0.29168472]
