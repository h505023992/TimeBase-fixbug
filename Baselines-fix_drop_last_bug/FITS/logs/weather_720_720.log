Args in experiment:
Namespace(is_training=1, model_id='weather_720_720', model='FITS', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=3, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=82, base_T=144, H_order=12)
Use GPU: cuda:0
>>>>>>>start training : weather_720_720_FITS_custom_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): Linear(in_features=82, out_features=164, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  36148224.0
params:  13612.0
Trainable parameters:  13612
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7282791
	speed: 0.1084s/iter; left time: 886.8137s
	iters: 200, epoch: 1 | loss: 0.6004771
	speed: 0.0891s/iter; left time: 720.1057s
Epoch: 1 cost time: 26.532499313354492
Epoch: 1, Steps: 276 | Train Loss: 0.6934457 Vali Loss: 0.7362807 Test Loss: 0.3527757
Validation loss decreased (inf --> 0.736281).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5390862
	speed: 0.3453s/iter; left time: 2729.4507s
	iters: 200, epoch: 2 | loss: 0.5385554
	speed: 0.0848s/iter; left time: 661.7106s
Epoch: 2 cost time: 26.73885226249695
Epoch: 2, Steps: 276 | Train Loss: 0.5042974 Vali Loss: 0.6928217 Test Loss: 0.3407215
Validation loss decreased (0.736281 --> 0.692822).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4836181
	speed: 0.3500s/iter; left time: 2670.4052s
	iters: 200, epoch: 3 | loss: 0.4586909
	speed: 0.1018s/iter; left time: 766.7984s
Epoch: 3 cost time: 27.249581813812256
Epoch: 3, Steps: 276 | Train Loss: 0.4441897 Vali Loss: 0.6630888 Test Loss: 0.3336117
Validation loss decreased (0.692822 --> 0.663089).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4083637
	speed: 0.3410s/iter; left time: 2507.0721s
	iters: 200, epoch: 4 | loss: 0.4288630
	speed: 0.0955s/iter; left time: 692.8089s
Epoch: 4 cost time: 26.430743932724
Epoch: 4, Steps: 276 | Train Loss: 0.4087009 Vali Loss: 0.6441438 Test Loss: 0.3291351
Validation loss decreased (0.663089 --> 0.644144).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3946056
	speed: 0.3418s/iter; left time: 2419.2488s
	iters: 200, epoch: 5 | loss: 0.3716471
	speed: 0.0847s/iter; left time: 591.1131s
Epoch: 5 cost time: 26.398173093795776
Epoch: 5, Steps: 276 | Train Loss: 0.3863160 Vali Loss: 0.6339521 Test Loss: 0.3261134
Validation loss decreased (0.644144 --> 0.633952).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3475882
	speed: 0.3528s/iter; left time: 2399.6664s
	iters: 200, epoch: 6 | loss: 0.3788186
	speed: 0.0860s/iter; left time: 576.0677s
Epoch: 6 cost time: 25.24692416191101
Epoch: 6, Steps: 276 | Train Loss: 0.3718951 Vali Loss: 0.6246566 Test Loss: 0.3244240
Validation loss decreased (0.633952 --> 0.624657).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3366684
	speed: 0.3277s/iter; left time: 2137.9201s
	iters: 200, epoch: 7 | loss: 0.3517488
	speed: 0.0865s/iter; left time: 555.5459s
Epoch: 7 cost time: 26.04910397529602
Epoch: 7, Steps: 276 | Train Loss: 0.3623143 Vali Loss: 0.6203630 Test Loss: 0.3232159
Validation loss decreased (0.624657 --> 0.620363).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3773459
	speed: 0.3501s/iter; left time: 2187.6340s
	iters: 200, epoch: 8 | loss: 0.3240895
	speed: 0.0914s/iter; left time: 562.1845s
Epoch: 8 cost time: 27.259907722473145
Epoch: 8, Steps: 276 | Train Loss: 0.3555437 Vali Loss: 0.6163549 Test Loss: 0.3235116
Validation loss decreased (0.620363 --> 0.616355).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3193283
	speed: 0.3351s/iter; left time: 2001.7797s
	iters: 200, epoch: 9 | loss: 0.3537083
	speed: 0.0855s/iter; left time: 502.3939s
Epoch: 9 cost time: 24.82452416419983
Epoch: 9, Steps: 276 | Train Loss: 0.3510894 Vali Loss: 0.6168517 Test Loss: 0.3233864
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3397932
	speed: 0.3368s/iter; left time: 1918.6960s
	iters: 200, epoch: 10 | loss: 0.3323431
	speed: 0.0840s/iter; left time: 469.8874s
Epoch: 10 cost time: 24.13347029685974
Epoch: 10, Steps: 276 | Train Loss: 0.3482348 Vali Loss: 0.6154137 Test Loss: 0.3231480
Validation loss decreased (0.616355 --> 0.615414).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3881058
	speed: 0.3485s/iter; left time: 1889.4892s
	iters: 200, epoch: 11 | loss: 0.2924131
	speed: 0.0859s/iter; left time: 457.0856s
Epoch: 11 cost time: 25.125221490859985
Epoch: 11, Steps: 276 | Train Loss: 0.3460834 Vali Loss: 0.6148436 Test Loss: 0.3233691
Validation loss decreased (0.615414 --> 0.614844).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3537976
	speed: 0.3297s/iter; left time: 1696.2239s
	iters: 200, epoch: 12 | loss: 0.3567006
	speed: 0.0878s/iter; left time: 442.7218s
Epoch: 12 cost time: 26.27436375617981
Epoch: 12, Steps: 276 | Train Loss: 0.3451484 Vali Loss: 0.6155813 Test Loss: 0.3235687
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3673211
	speed: 0.3161s/iter; left time: 1538.8812s
	iters: 200, epoch: 13 | loss: 0.3096956
	speed: 0.0891s/iter; left time: 424.9314s
Epoch: 13 cost time: 24.277546405792236
Epoch: 13, Steps: 276 | Train Loss: 0.3443622 Vali Loss: 0.6155145 Test Loss: 0.3235951
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3259961
	speed: 0.3065s/iter; left time: 1407.7712s
	iters: 200, epoch: 14 | loss: 0.3628959
	speed: 0.0764s/iter; left time: 343.0709s
Epoch: 14 cost time: 22.278751373291016
Epoch: 14, Steps: 276 | Train Loss: 0.3439451 Vali Loss: 0.6157808 Test Loss: 0.3239012
EarlyStopping counter: 3 out of 3
Early stopping
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): Linear(in_features=82, out_features=164, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  36148224.0
params:  13612.0
Trainable parameters:  13612
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5720575
	speed: 0.0905s/iter; left time: 740.3571s
	iters: 200, epoch: 1 | loss: 0.6412395
	speed: 0.0751s/iter; left time: 607.2861s
Epoch: 1 cost time: 22.98919129371643
Epoch: 1, Steps: 276 | Train Loss: 0.5894917 Vali Loss: 0.6182917 Test Loss: 0.3230555
Validation loss decreased (inf --> 0.618292).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5037737
	speed: 0.3284s/iter; left time: 2596.2783s
	iters: 200, epoch: 2 | loss: 0.6076102
	speed: 0.0810s/iter; left time: 632.5699s
Epoch: 2 cost time: 24.099116325378418
Epoch: 2, Steps: 276 | Train Loss: 0.5887280 Vali Loss: 0.6168774 Test Loss: 0.3238257
Validation loss decreased (0.618292 --> 0.616877).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4928489
	speed: 0.3180s/iter; left time: 2425.7425s
	iters: 200, epoch: 3 | loss: 0.5362607
	speed: 0.0863s/iter; left time: 649.8024s
Epoch: 3 cost time: 24.86719298362732
Epoch: 3, Steps: 276 | Train Loss: 0.5882872 Vali Loss: 0.6162664 Test Loss: 0.3239028
Validation loss decreased (0.616877 --> 0.616266).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4917882
	speed: 0.3235s/iter; left time: 2371.0541s
	iters: 200, epoch: 4 | loss: 0.6470546
	speed: 0.0836s/iter; left time: 606.6478s
Epoch: 4 cost time: 24.45041036605835
Epoch: 4, Steps: 276 | Train Loss: 0.5883361 Vali Loss: 0.6163619 Test Loss: 0.3237515
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5902429
	speed: 0.2973s/iter; left time: 2104.0563s
	iters: 200, epoch: 5 | loss: 0.5694597
	speed: 0.0662s/iter; left time: 461.9398s
Epoch: 5 cost time: 20.61808466911316
Epoch: 5, Steps: 276 | Train Loss: 0.5880850 Vali Loss: 0.6151939 Test Loss: 0.3234850
Validation loss decreased (0.616266 --> 0.615194).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5917420
	speed: 0.3254s/iter; left time: 2213.2766s
	iters: 200, epoch: 6 | loss: 0.5423904
	speed: 0.0879s/iter; left time: 589.2397s
Epoch: 6 cost time: 25.766993761062622
Epoch: 6, Steps: 276 | Train Loss: 0.5886115 Vali Loss: 0.6154263 Test Loss: 0.3236090
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5745720
	speed: 0.3452s/iter; left time: 2252.5763s
	iters: 200, epoch: 7 | loss: 0.5923719
	speed: 0.0856s/iter; left time: 550.2621s
Epoch: 7 cost time: 25.399743795394897
Epoch: 7, Steps: 276 | Train Loss: 0.5883631 Vali Loss: 0.6156513 Test Loss: 0.3233615
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5052834
	speed: 0.3434s/iter; left time: 2146.2020s
	iters: 200, epoch: 8 | loss: 0.5976387
	speed: 0.0862s/iter; left time: 530.2829s
Epoch: 8 cost time: 24.917747020721436
Epoch: 8, Steps: 276 | Train Loss: 0.5880627 Vali Loss: 0.6170143 Test Loss: 0.3237263
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : weather_720_720_FITS_custom_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3245904960632324, mae:0.34615501832962036, rse:0.7450876832008362, corr:[0.47941202 0.4819775  0.48172122 0.48047265 0.4793959  0.47910312
 0.47946718 0.4799943  0.48025206 0.4800469  0.47947806 0.47880077
 0.47813758 0.477657   0.47729212 0.47687966 0.4763609  0.4755987
 0.4748036  0.4740599  0.47344488 0.47294012 0.47254086 0.47200984
 0.47131953 0.47041067 0.46937564 0.46824968 0.46724486 0.46637377
 0.46568775 0.46502092 0.46433237 0.46347988 0.46257073 0.4615941
 0.4607305  0.45991087 0.45915666 0.45844543 0.45779815 0.4572597
 0.45670378 0.45610827 0.45549995 0.45487025 0.45427752 0.45372698
 0.45303902 0.45234564 0.45172098 0.45113415 0.4506707  0.45028973
 0.44995162 0.44955778 0.44914278 0.4486181  0.44808173 0.44753775
 0.44705352 0.44659054 0.4462362  0.44592246 0.44564903 0.44543293
 0.44512808 0.44478497 0.4444274  0.44410807 0.4438604  0.44353583
 0.4432365  0.44288084 0.44239482 0.44186392 0.44140235 0.44109482
 0.4408928  0.44070777 0.4406312  0.44062278 0.44050652 0.44028077
 0.43995464 0.43950456 0.43913448 0.43877327 0.4384904  0.4381834
 0.43794444 0.43776715 0.43760452 0.4374999  0.4373946  0.4372401
 0.43704292 0.4367889  0.43649122 0.436157   0.4357874  0.43540332
 0.4350645  0.4347695  0.43450654 0.43429488 0.43402433 0.43380085
 0.43355194 0.43329978 0.43302253 0.4326723  0.43221304 0.43181682
 0.43146056 0.4311032  0.43078744 0.4304847  0.43014884 0.42969558
 0.42923182 0.42881933 0.4284415  0.42813718 0.42790258 0.42768636
 0.4274046  0.42702606 0.42656878 0.426038   0.425428   0.4248084
 0.4242639  0.42381012 0.4234935  0.4232309  0.42296916 0.4227291
 0.42237908 0.4220103  0.42163408 0.42126918 0.42089275 0.4204866
 0.42005125 0.41960415 0.4191701  0.41876915 0.4183132  0.41774327
 0.41709098 0.41638437 0.4155965  0.4146978  0.41370258 0.41264716
 0.41180775 0.4110567  0.4104689  0.40996572 0.40963978 0.40932006
 0.40898952 0.40864158 0.40819708 0.40770435 0.4070438  0.40630317
 0.40558916 0.4048592  0.4041533  0.40352988 0.40296915 0.40257266
 0.40214252 0.40173057 0.40126723 0.40079612 0.40027905 0.39974067
 0.39932472 0.3989462  0.39862993 0.39832813 0.39802495 0.39769256
 0.39731368 0.39681032 0.39632487 0.39585862 0.3954364  0.39507356
 0.39486995 0.39458445 0.39428464 0.39390257 0.39343417 0.3929386
 0.39244658 0.39206803 0.39185026 0.39174652 0.39163783 0.39139292
 0.39107022 0.39068386 0.3902515  0.38978335 0.3892735  0.3888715
 0.38852286 0.3882022  0.38785288 0.38750812 0.38712752 0.38665056
 0.38616255 0.3857719  0.38544196 0.38504776 0.3846714  0.38438678
 0.3841331  0.38390505 0.38369057 0.38343775 0.38323173 0.38315192
 0.3831331  0.38311762 0.38303772 0.38288897 0.3825883  0.3821763
 0.38170767 0.38125372 0.38081428 0.3805023  0.3801446  0.37983647
 0.37959915 0.37940133 0.3791387  0.37885734 0.37861976 0.37839946
 0.37817985 0.37805668 0.3778725  0.3777369  0.37754604 0.3773096
 0.37697834 0.37653968 0.3760927  0.37562785 0.3752145  0.37491947
 0.37484488 0.37494457 0.3750319  0.3750943  0.37505236 0.3748267
 0.3744402  0.37389624 0.37328154 0.37275156 0.37237623 0.37212828
 0.3719604  0.3718167  0.37166145 0.37129226 0.37073192 0.3700096
 0.3691659  0.36827973 0.3675259  0.36694214 0.36652055 0.3662447
 0.36605215 0.36576542 0.36537224 0.3648555  0.36423784 0.3635457
 0.36285934 0.36224413 0.36167425 0.3611151  0.36062595 0.36022297
 0.35981783 0.3594076  0.3589432  0.35837328 0.35760415 0.35671425
 0.35552984 0.3541443  0.35267875 0.3513178  0.3502575  0.3495395
 0.34917825 0.34894922 0.34888527 0.34875724 0.34838003 0.34779456
 0.34717    0.34661102 0.3462082  0.34600687 0.3459279  0.34592807
 0.34595013 0.34568587 0.34508818 0.3442789  0.34331307 0.34222388
 0.3411824  0.3404895  0.34005424 0.33985466 0.33977786 0.33966717
 0.339549   0.33937377 0.3391808  0.33908433 0.33900738 0.3390376
 0.33901164 0.3388599  0.33868447 0.33827204 0.3377667  0.33725595
 0.33688194 0.33670214 0.336759   0.33703938 0.3373724  0.33758062
 0.3375619  0.3372901  0.33685276 0.33621344 0.33563435 0.3351147
 0.33473083 0.3345464  0.33454302 0.33456412 0.3345533  0.33450907
 0.33433983 0.33410725 0.3339537  0.33379066 0.33364746 0.33355856
 0.33345813 0.33335936 0.33329266 0.33312935 0.33292532 0.33277094
 0.33269256 0.33265582 0.3326247  0.33265868 0.3327025  0.33267146
 0.33251923 0.33226684 0.33183268 0.3313245  0.3308681  0.3303451
 0.32987493 0.32953852 0.32937935 0.32937676 0.32945803 0.32958648
 0.3296212  0.32969087 0.3297216  0.32976642 0.32983795 0.32996947
 0.3301141  0.3302149  0.33033365 0.33027184 0.33014387 0.32984382
 0.32961163 0.329252   0.32892594 0.32853928 0.32814667 0.32784858
 0.32763702 0.32733905 0.3269329  0.3264302  0.32597053 0.32560226
 0.3253327  0.32514402 0.32508254 0.32513446 0.32517377 0.32513508
 0.32487372 0.3245515  0.32404602 0.32353777 0.32394136 0.32339014
 0.3238383  0.3233696  0.32094622 0.32036248 0.3196746  0.31900457
 0.31833717 0.31771806 0.31722707 0.3168228  0.31646988 0.31606883
 0.31563348 0.31515658 0.31465206 0.31409517 0.31351107 0.31271192
 0.3118483  0.31091547 0.30994767 0.30906722 0.3082945  0.3077277
 0.30738375 0.3071042  0.30674416 0.30631214 0.3057546  0.30499476
 0.30430943 0.30363503 0.30303234 0.30256975 0.30217484 0.3019168
 0.30168074 0.3014413  0.30117348 0.30083078 0.3003666  0.2998781
 0.29941446 0.29911584 0.29892293 0.2987765  0.29866666 0.29854512
 0.29841074 0.2982981  0.2981957  0.29816988 0.29823253 0.29829246
 0.2983897  0.29842997 0.29835898 0.29818475 0.29794618 0.29778337
 0.29768044 0.2977302  0.2980283  0.29844436 0.29876643 0.29875356
 0.29839897 0.29777014 0.29717076 0.2967031  0.29634184 0.29613027
 0.29613513 0.29629815 0.29647994 0.29670888 0.29693362 0.29715982
 0.2973713  0.2976333  0.29794312 0.29812753 0.29832804 0.29829577
 0.29811302 0.29784754 0.29755425 0.29708046 0.29653096 0.29609415
 0.29575893 0.29552242 0.29538718 0.29537612 0.29545254 0.29541454
 0.29539925 0.2953794  0.29528847 0.29522425 0.2950312  0.29475614
 0.29430842 0.2938114  0.29312667 0.29253733 0.2921     0.29184514
 0.29167038 0.29152066 0.29125857 0.290983   0.2906339  0.29020363
 0.2897076  0.28925368 0.28891712 0.288723   0.2885735  0.2885021
 0.2884721  0.28848994 0.28850472 0.28841403 0.2882247  0.28784043
 0.28733212 0.28673056 0.2861346  0.28551412 0.28504506 0.28466818
 0.28433904 0.28405884 0.2838017  0.28354764 0.2830897  0.2824137
 0.2815987  0.28069398 0.2798111  0.27893016 0.2781707  0.27758205
 0.2771092  0.27672336 0.2763543  0.27592668 0.27537602 0.27469558
 0.27402216 0.2731362  0.271939   0.2710998  0.27026352 0.26947916
 0.26876003 0.26808742 0.267417   0.266778   0.26611567 0.2655008
 0.26490366 0.2643291  0.26377076 0.26326808 0.26281866 0.26246116
 0.26215804 0.26190972 0.26174918 0.26167998 0.2616946  0.26172444
 0.26176873 0.2616819  0.26142555 0.26095006 0.26028243 0.259605
 0.25887746 0.25831625 0.25792006 0.2576359  0.25747257 0.2573376
 0.2571306  0.25687072 0.25652698 0.25602496 0.25556624 0.255074
 0.254683   0.25452995 0.25443858 0.25437737 0.2543074  0.2541769
 0.2539075  0.25359684 0.25331694 0.2531602  0.25326988 0.25351572
 0.25387704 0.25416338 0.2543367  0.2542817  0.25407583 0.25378942
 0.25359148 0.25348842 0.25347954 0.25344008 0.25342914 0.2533646
 0.25322962 0.2530555  0.25290093 0.25273955 0.25271317 0.2527538
 0.25291225 0.2529832  0.2529946  0.2527839  0.2525333  0.25223568
 0.25208452 0.2521116  0.25233358 0.2526623  0.25296757 0.25316712
 0.25324792 0.25305802 0.25269404 0.25220346 0.25172994 0.25138545
 0.25110078 0.25086442 0.250626   0.25034413 0.2500358  0.24966182
 0.24933334 0.24914926 0.24902825 0.24899387 0.24892019 0.24883516
 0.248689   0.24847168 0.24825327 0.24803628 0.2478965  0.24778585
 0.24764924 0.24746735 0.24727365 0.24702904 0.24679431 0.24658126
 0.24641651 0.24621503 0.24604657 0.2458754  0.24563871 0.24528988
 0.24491726 0.24457687 0.24430119 0.24410461 0.24400452 0.2439022
 0.24378534 0.24361183 0.24336007 0.24307282 0.24283214 0.24260852
 0.24240154 0.2421263  0.24174768 0.2412775  0.24059036 0.23979849
 0.23882025 0.23796923 0.23695976 0.23563653 0.23355637 0.23077597]
