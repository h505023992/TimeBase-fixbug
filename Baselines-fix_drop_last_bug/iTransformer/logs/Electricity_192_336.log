Args in experiment:
Namespace(is_training=1, model_id='ECL_192_336', model='iTransformer', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=336, enc_in=321, dec_in=321, c_out=321, d_model=512, n_heads=8, e_layers=3, d_layers=1, d_ff=512, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=64, patience=3, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=2, use_multi_gpu=False, devices='0,1,2,3', exp_name='MTSF', channel_independence=False, inverse=False, class_strategy='projection', target_root_path='./data/electricity/', target_data_path='electricity.csv', efficient_training=False, use_norm=True, partial_start_index=0)
Use GPU: cuda:2
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.
MACs: 1636067328.0
Params: 5006160.0
1.64G MACs
>>>>>>>start training : ECL_192_336_iTransformer_custom_M_ft192_sl48_ll336_pl512_dm8_nh3_el1_dl512_df1_fctimeF_ebTrue_dtExp_projection_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17885
val 2297
test 4925
	iters: 100, epoch: 1 | loss: 0.2637679
	speed: 0.1452s/iter; left time: 390.6475s
	iters: 200, epoch: 1 | loss: 0.2124722
	speed: 0.1331s/iter; left time: 344.9069s
Max Memory (MB): 3138.1748046875
Epoch: 1 cost time: 37.98223328590393
Epoch: 1, Steps: 279 | Train Loss: 0.2827796 Vali Loss: 0.1804924 Test Loss: 0.2035602
Validation loss decreased (inf --> 0.180492).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.1981268
	speed: 1.1819s/iter; left time: 2850.6290s
	iters: 200, epoch: 2 | loss: 0.2109631
	speed: 0.1231s/iter; left time: 284.5474s
Max Memory (MB): 3138.1748046875
Epoch: 2 cost time: 36.406567096710205
Epoch: 2, Steps: 279 | Train Loss: 0.2117982 Vali Loss: 0.1680368 Test Loss: 0.1896338
Validation loss decreased (0.180492 --> 0.168037).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2020524
	speed: 1.2830s/iter; left time: 2736.6695s
	iters: 200, epoch: 3 | loss: 0.1997752
	speed: 0.1220s/iter; left time: 248.0823s
Max Memory (MB): 3138.1748046875
Epoch: 3 cost time: 35.21260476112366
Epoch: 3, Steps: 279 | Train Loss: 0.2015612 Vali Loss: 0.1630891 Test Loss: 0.1839668
Validation loss decreased (0.168037 --> 0.163089).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1878721
	speed: 1.0810s/iter; left time: 2004.0903s
	iters: 200, epoch: 4 | loss: 0.1987029
	speed: 0.1225s/iter; left time: 214.7968s
Max Memory (MB): 3138.1748046875
Epoch: 4 cost time: 35.6155948638916
Epoch: 4, Steps: 279 | Train Loss: 0.1976653 Vali Loss: 0.1612338 Test Loss: 0.1819629
Validation loss decreased (0.163089 --> 0.161234).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.2348495
	speed: 1.2949s/iter; left time: 2039.4926s
	iters: 200, epoch: 5 | loss: 0.1801487
	speed: 0.1292s/iter; left time: 190.5026s
Max Memory (MB): 3138.1748046875
Epoch: 5 cost time: 37.32133865356445
Epoch: 5, Steps: 279 | Train Loss: 0.1956599 Vali Loss: 0.1603557 Test Loss: 0.1807152
Validation loss decreased (0.161234 --> 0.160356).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.1957903
	speed: 1.1465s/iter; left time: 1485.8972s
	iters: 200, epoch: 6 | loss: 0.1954923
	speed: 0.1190s/iter; left time: 142.3071s
Max Memory (MB): 3138.1748046875
Epoch: 6 cost time: 35.50749588012695
Epoch: 6, Steps: 279 | Train Loss: 0.1946665 Vali Loss: 0.1598090 Test Loss: 0.1801839
Validation loss decreased (0.160356 --> 0.159809).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.1964076
	speed: 1.1060s/iter; left time: 1124.7693s
	iters: 200, epoch: 7 | loss: 0.2048365
	speed: 0.1257s/iter; left time: 115.2409s
Max Memory (MB): 3138.1748046875
Epoch: 7 cost time: 36.88659644126892
Epoch: 7, Steps: 279 | Train Loss: 0.1941364 Vali Loss: 0.1597665 Test Loss: 0.1799579
Validation loss decreased (0.159809 --> 0.159766).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.1931803
	speed: 1.1323s/iter; left time: 835.6625s
	iters: 200, epoch: 8 | loss: 0.1704421
	speed: 0.1199s/iter; left time: 76.4798s
Max Memory (MB): 3138.1748046875
Epoch: 8 cost time: 35.44434332847595
Epoch: 8, Steps: 279 | Train Loss: 0.1938976 Vali Loss: 0.1594142 Test Loss: 0.1799940
Validation loss decreased (0.159766 --> 0.159414).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.1936853
	speed: 1.2000s/iter; left time: 550.7933s
	iters: 200, epoch: 9 | loss: 0.2023807
	speed: 0.1323s/iter; left time: 47.5073s
Max Memory (MB): 3138.1748046875
Epoch: 9 cost time: 37.81468939781189
Epoch: 9, Steps: 279 | Train Loss: 0.1937588 Vali Loss: 0.1593508 Test Loss: 0.1798610
Validation loss decreased (0.159414 --> 0.159351).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.1952779
	speed: 1.2989s/iter; left time: 233.8086s
	iters: 200, epoch: 10 | loss: 0.2061982
	speed: 0.1219s/iter; left time: 9.7554s
Max Memory (MB): 3138.1748046875
Epoch: 10 cost time: 35.979926347732544
Epoch: 10, Steps: 279 | Train Loss: 0.1936902 Vali Loss: 0.1593224 Test Loss: 0.1798113
Validation loss decreased (0.159351 --> 0.159322).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : ECL_192_336_iTransformer_custom_M_ft192_sl48_ll336_pl512_dm8_nh3_el1_dl512_df1_fctimeF_ebTrue_dtExp_projection_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
test shape: (4925, 1, 336, 321) (4925, 1, 336, 321)
test shape: (4925, 336, 321) (4925, 336, 321)
mse:0.17981134355068207, mae:0.2757670283317566
