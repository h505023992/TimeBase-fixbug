Args in experiment:
Namespace(is_training=1, model_id='traffic_720_192', model='LightTimeBaseTST', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=192, period_len=24, basis_num=8, use_period_norm=1, use_orthogonal=1, orthogonal_weight=0.04, fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=7, individual=0, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='learned', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.03, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=3, use_multi_gpu=0, devices='0,1', test_flop=False)
Use GPU: cuda:3
320
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
MACs: 6289152.0
Params: 320.0
6.29M MACs
>>>>>>>start training : traffic_720_192_LightTimeBaseTST_custom_ftM_sl720_pl192_test_0_seed2023>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Max Memory (MB): 1379.57373046875
Epoch: 1 cost time: 27.308029413223267
Epoch: 1, Steps: 89 | Train Loss: 0.4034616 Vali Loss: 0.5124511 Test Loss: 0.6178853
Validation loss decreased (inf --> 0.512451).  Saving model ...
Updating learning rate to 0.03
Max Memory (MB): 1654.2939453125
Epoch: 2 cost time: 27.662131786346436
Epoch: 2, Steps: 89 | Train Loss: 0.3007318 Vali Loss: 0.3575781 Test Loss: 0.4354386
Validation loss decreased (0.512451 --> 0.357578).  Saving model ...
Updating learning rate to 0.03
Max Memory (MB): 1654.2939453125
Epoch: 3 cost time: 27.544479846954346
Epoch: 3, Steps: 89 | Train Loss: 0.2573846 Vali Loss: 0.3468275 Test Loss: 0.4201468
Validation loss decreased (0.357578 --> 0.346828).  Saving model ...
Updating learning rate to 0.03
Max Memory (MB): 1654.2939453125
Epoch: 4 cost time: 27.757182359695435
Epoch: 4, Steps: 89 | Train Loss: 0.2518907 Vali Loss: 0.3515239 Test Loss: 0.4300823
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.024
Max Memory (MB): 1654.2939453125
Epoch: 5 cost time: 27.777063131332397
Epoch: 5, Steps: 89 | Train Loss: 0.2493154 Vali Loss: 0.3425989 Test Loss: 0.4156426
Validation loss decreased (0.346828 --> 0.342599).  Saving model ...
Updating learning rate to 0.019200000000000002
Max Memory (MB): 1654.2939453125
Epoch: 6 cost time: 27.41749095916748
Epoch: 6, Steps: 89 | Train Loss: 0.2473885 Vali Loss: 0.3419122 Test Loss: 0.4167662
Validation loss decreased (0.342599 --> 0.341912).  Saving model ...
Updating learning rate to 0.015360000000000004
Max Memory (MB): 1654.2939453125
Epoch: 7 cost time: 27.884382486343384
Epoch: 7, Steps: 89 | Train Loss: 0.2457586 Vali Loss: 0.3372879 Test Loss: 0.4090523
Validation loss decreased (0.341912 --> 0.337288).  Saving model ...
Updating learning rate to 0.012288000000000002
Max Memory (MB): 1654.2939453125
Epoch: 8 cost time: 28.128177642822266
Epoch: 8, Steps: 89 | Train Loss: 0.2453657 Vali Loss: 0.3346858 Test Loss: 0.4096495
Validation loss decreased (0.337288 --> 0.334686).  Saving model ...
Updating learning rate to 0.009830400000000001
Max Memory (MB): 1654.2939453125
Epoch: 9 cost time: 27.68177318572998
Epoch: 9, Steps: 89 | Train Loss: 0.2447075 Vali Loss: 0.3386922 Test Loss: 0.4089779
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.007864320000000003
Max Memory (MB): 1654.2939453125
Epoch: 10 cost time: 27.594695806503296
Epoch: 10, Steps: 89 | Train Loss: 0.2442932 Vali Loss: 0.3350838 Test Loss: 0.4085593
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.006291456000000002
Max Memory (MB): 1654.2939453125
Epoch: 11 cost time: 27.79360032081604
Epoch: 11, Steps: 89 | Train Loss: 0.2434888 Vali Loss: 0.3360416 Test Loss: 0.4065350
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.005033164800000003
Max Memory (MB): 1654.2939453125
Epoch: 12 cost time: 27.66825532913208
Epoch: 12, Steps: 89 | Train Loss: 0.2432126 Vali Loss: 0.3376980 Test Loss: 0.4071234
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.004026531840000002
Max Memory (MB): 1654.2939453125
Epoch: 13 cost time: 27.76952838897705
Epoch: 13, Steps: 89 | Train Loss: 0.2429205 Vali Loss: 0.3338641 Test Loss: 0.4057948
Validation loss decreased (0.334686 --> 0.333864).  Saving model ...
Updating learning rate to 0.0032212254720000015
Max Memory (MB): 1654.2939453125
Epoch: 14 cost time: 27.817750692367554
Epoch: 14, Steps: 89 | Train Loss: 0.2426369 Vali Loss: 0.3334888 Test Loss: 0.4063514
Validation loss decreased (0.333864 --> 0.333489).  Saving model ...
Updating learning rate to 0.0025769803776000016
Max Memory (MB): 1654.2939453125
Epoch: 15 cost time: 27.433677434921265
Epoch: 15, Steps: 89 | Train Loss: 0.2425114 Vali Loss: 0.3345830 Test Loss: 0.4056425
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.002061584302080001
Max Memory (MB): 1654.2939453125
Epoch: 16 cost time: 27.238584518432617
Epoch: 16, Steps: 89 | Train Loss: 0.2425238 Vali Loss: 0.3330579 Test Loss: 0.4056234
Validation loss decreased (0.333489 --> 0.333058).  Saving model ...
Updating learning rate to 0.001649267441664001
Max Memory (MB): 1654.2939453125
Epoch: 17 cost time: 27.84579110145569
Epoch: 17, Steps: 89 | Train Loss: 0.2422402 Vali Loss: 0.3335752 Test Loss: 0.4052556
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.001319413953331201
Max Memory (MB): 1654.2939453125
Epoch: 18 cost time: 27.7344708442688
Epoch: 18, Steps: 89 | Train Loss: 0.2421796 Vali Loss: 0.3343371 Test Loss: 0.4045787
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0010555311626649609
Max Memory (MB): 1654.2939453125
Epoch: 19 cost time: 27.764772176742554
Epoch: 19, Steps: 89 | Train Loss: 0.2421035 Vali Loss: 0.3335773 Test Loss: 0.4055796
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0008444249301319687
Max Memory (MB): 1654.2939453125
Epoch: 20 cost time: 27.630069971084595
Epoch: 20, Steps: 89 | Train Loss: 0.2420460 Vali Loss: 0.3317651 Test Loss: 0.4047424
Validation loss decreased (0.333058 --> 0.331765).  Saving model ...
Updating learning rate to 0.000675539944105575
Max Memory (MB): 1654.2939453125
Epoch: 21 cost time: 27.310546875
Epoch: 21, Steps: 89 | Train Loss: 0.2419942 Vali Loss: 0.3321852 Test Loss: 0.4047413
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00054043195528446
Max Memory (MB): 1654.2939453125
Epoch: 22 cost time: 27.653470993041992
Epoch: 22, Steps: 89 | Train Loss: 0.2419494 Vali Loss: 0.3332107 Test Loss: 0.4047942
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00043234556422756804
Max Memory (MB): 1654.2939453125
Epoch: 23 cost time: 29.30018162727356
Epoch: 23, Steps: 89 | Train Loss: 0.2418667 Vali Loss: 0.3329349 Test Loss: 0.4046490
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00034587645138205446
Max Memory (MB): 1654.2939453125
Epoch: 24 cost time: 27.930233001708984
Epoch: 24, Steps: 89 | Train Loss: 0.2418149 Vali Loss: 0.3329125 Test Loss: 0.4047000
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002767011611056436
Max Memory (MB): 1654.2939453125
Epoch: 25 cost time: 27.7203152179718
Epoch: 25, Steps: 89 | Train Loss: 0.2418389 Vali Loss: 0.3324522 Test Loss: 0.4045155
EarlyStopping counter: 5 out of 5
Early stopping
Final Max Memory (MB): 1654.2939453125
>>>>>>>testing : traffic_720_192_LightTimeBaseTST_custom_ftM_sl720_pl192_test_0_seed2023<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4037693738937378, mae:0.27400532364845276, rse:0.5244398713111877
