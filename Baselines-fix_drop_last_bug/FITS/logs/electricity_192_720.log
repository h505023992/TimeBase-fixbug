Args in experiment:
Namespace(is_training=1, model_id='electricity_192_720', model='FITS', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=321, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=82, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : electricity_192_720_FITS_custom_ftM_sl192_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17501
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=82, out_features=389, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1310625024.0
params:  32287.0
Trainable parameters:  32287
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.0823888
	speed: 0.2708s/iter; left time: 1077.9895s
Epoch: 1 cost time: 36.39214563369751
Epoch: 1, Steps: 136 | Train Loss: 1.3391688 Vali Loss: 0.7670857 Test Loss: 0.8842828
Validation loss decreased (inf --> 0.767086).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6683486
	speed: 0.6261s/iter; left time: 2407.3770s
Epoch: 2 cost time: 36.44386839866638
Epoch: 2, Steps: 136 | Train Loss: 0.7293938 Vali Loss: 0.5637270 Test Loss: 0.6610971
Validation loss decreased (0.767086 --> 0.563727).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5934026
	speed: 0.6084s/iter; left time: 2256.4968s
Epoch: 3 cost time: 36.226552963256836
Epoch: 3, Steps: 136 | Train Loss: 0.5954228 Vali Loss: 0.4972337 Test Loss: 0.5865409
Validation loss decreased (0.563727 --> 0.497234).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5213839
	speed: 0.6292s/iter; left time: 2248.1616s
Epoch: 4 cost time: 35.61235857009888
Epoch: 4, Steps: 136 | Train Loss: 0.5306144 Vali Loss: 0.4501470 Test Loss: 0.5328046
Validation loss decreased (0.497234 --> 0.450147).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4834965
	speed: 0.5862s/iter; left time: 2014.7575s
Epoch: 5 cost time: 35.73698043823242
Epoch: 5, Steps: 136 | Train Loss: 0.4802686 Vali Loss: 0.4117722 Test Loss: 0.4888526
Validation loss decreased (0.450147 --> 0.411772).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4259830
	speed: 0.6155s/iter; left time: 2031.9269s
Epoch: 6 cost time: 37.39960980415344
Epoch: 6, Steps: 136 | Train Loss: 0.4380849 Vali Loss: 0.3784476 Test Loss: 0.4510306
Validation loss decreased (0.411772 --> 0.378448).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4055496
	speed: 0.6127s/iter; left time: 1939.3000s
Epoch: 7 cost time: 35.65660357475281
Epoch: 7, Steps: 136 | Train Loss: 0.4028473 Vali Loss: 0.3511147 Test Loss: 0.4193948
Validation loss decreased (0.378448 --> 0.351115).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3684537
	speed: 0.6258s/iter; left time: 1895.4759s
Epoch: 8 cost time: 38.36115598678589
Epoch: 8, Steps: 136 | Train Loss: 0.3727295 Vali Loss: 0.3278552 Test Loss: 0.3928433
Validation loss decreased (0.351115 --> 0.327855).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3449410
	speed: 0.6047s/iter; left time: 1749.4271s
Epoch: 9 cost time: 35.86144161224365
Epoch: 9, Steps: 136 | Train Loss: 0.3471942 Vali Loss: 0.3076351 Test Loss: 0.3694721
Validation loss decreased (0.327855 --> 0.307635).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3258537
	speed: 0.6149s/iter; left time: 1695.2540s
Epoch: 10 cost time: 35.956010818481445
Epoch: 10, Steps: 136 | Train Loss: 0.3255096 Vali Loss: 0.2916769 Test Loss: 0.3505031
Validation loss decreased (0.307635 --> 0.291677).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3016061
	speed: 0.6039s/iter; left time: 1582.8471s
Epoch: 11 cost time: 36.248579263687134
Epoch: 11, Steps: 136 | Train Loss: 0.3069545 Vali Loss: 0.2772337 Test Loss: 0.3336581
Validation loss decreased (0.291677 --> 0.277234).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2908187
	speed: 0.6202s/iter; left time: 1541.1470s
Epoch: 12 cost time: 36.646726846694946
Epoch: 12, Steps: 136 | Train Loss: 0.2910077 Vali Loss: 0.2650605 Test Loss: 0.3193128
Validation loss decreased (0.277234 --> 0.265061).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2700132
	speed: 0.6234s/iter; left time: 1464.4525s
Epoch: 13 cost time: 38.847362756729126
Epoch: 13, Steps: 136 | Train Loss: 0.2771549 Vali Loss: 0.2547276 Test Loss: 0.3066840
Validation loss decreased (0.265061 --> 0.254728).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2606659
	speed: 0.6065s/iter; left time: 1342.1782s
Epoch: 14 cost time: 35.83564496040344
Epoch: 14, Steps: 136 | Train Loss: 0.2654695 Vali Loss: 0.2455630 Test Loss: 0.2960486
Validation loss decreased (0.254728 --> 0.245563).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2509828
	speed: 0.6331s/iter; left time: 1314.9304s
Epoch: 15 cost time: 36.751368284225464
Epoch: 15, Steps: 136 | Train Loss: 0.2552088 Vali Loss: 0.2380227 Test Loss: 0.2867572
Validation loss decreased (0.245563 --> 0.238023).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2543368
	speed: 0.6016s/iter; left time: 1167.6997s
Epoch: 16 cost time: 36.34041428565979
Epoch: 16, Steps: 136 | Train Loss: 0.2465412 Vali Loss: 0.2315149 Test Loss: 0.2789949
Validation loss decreased (0.238023 --> 0.231515).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2351272
	speed: 0.6158s/iter; left time: 1111.5775s
Epoch: 17 cost time: 36.6445951461792
Epoch: 17, Steps: 136 | Train Loss: 0.2389110 Vali Loss: 0.2258014 Test Loss: 0.2721531
Validation loss decreased (0.231515 --> 0.225801).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2361085
	speed: 0.6235s/iter; left time: 1040.5757s
Epoch: 18 cost time: 37.60082244873047
Epoch: 18, Steps: 136 | Train Loss: 0.2323618 Vali Loss: 0.2207432 Test Loss: 0.2656852
Validation loss decreased (0.225801 --> 0.220743).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2179468
	speed: 0.6331s/iter; left time: 970.5898s
Epoch: 19 cost time: 36.27080178260803
Epoch: 19, Steps: 136 | Train Loss: 0.2266865 Vali Loss: 0.2169429 Test Loss: 0.2606913
Validation loss decreased (0.220743 --> 0.216943).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2232019
	speed: 0.6304s/iter; left time: 880.6463s
Epoch: 20 cost time: 39.155312061309814
Epoch: 20, Steps: 136 | Train Loss: 0.2216819 Vali Loss: 0.2129223 Test Loss: 0.2562038
Validation loss decreased (0.216943 --> 0.212922).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2148609
	speed: 0.6025s/iter; left time: 759.7383s
Epoch: 21 cost time: 36.83936047554016
Epoch: 21, Steps: 136 | Train Loss: 0.2174644 Vali Loss: 0.2100940 Test Loss: 0.2521983
Validation loss decreased (0.212922 --> 0.210094).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2092268
	speed: 0.6254s/iter; left time: 703.5631s
Epoch: 22 cost time: 37.114970207214355
Epoch: 22, Steps: 136 | Train Loss: 0.2137596 Vali Loss: 0.2070962 Test Loss: 0.2486261
Validation loss decreased (0.210094 --> 0.207096).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2081017
	speed: 0.6135s/iter; left time: 606.7585s
Epoch: 23 cost time: 35.87917232513428
Epoch: 23, Steps: 136 | Train Loss: 0.2106113 Vali Loss: 0.2052122 Test Loss: 0.2457051
Validation loss decreased (0.207096 --> 0.205212).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2084622
	speed: 0.6247s/iter; left time: 532.9027s
Epoch: 24 cost time: 36.325984477996826
Epoch: 24, Steps: 136 | Train Loss: 0.2077806 Vali Loss: 0.2031204 Test Loss: 0.2432225
Validation loss decreased (0.205212 --> 0.203120).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1995927
	speed: 0.6116s/iter; left time: 438.4844s
Epoch: 25 cost time: 36.5632381439209
Epoch: 25, Steps: 136 | Train Loss: 0.2053250 Vali Loss: 0.2013390 Test Loss: 0.2408590
Validation loss decreased (0.203120 --> 0.201339).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2118114
	speed: 0.6050s/iter; left time: 351.5124s
Epoch: 26 cost time: 35.20375323295593
Epoch: 26, Steps: 136 | Train Loss: 0.2032114 Vali Loss: 0.1999356 Test Loss: 0.2389017
Validation loss decreased (0.201339 --> 0.199936).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1958432
	speed: 0.6277s/iter; left time: 279.3352s
Epoch: 27 cost time: 37.67419147491455
Epoch: 27, Steps: 136 | Train Loss: 0.2013906 Vali Loss: 0.1985892 Test Loss: 0.2371444
Validation loss decreased (0.199936 --> 0.198589).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2038105
	speed: 0.6153s/iter; left time: 190.1182s
Epoch: 28 cost time: 36.67856502532959
Epoch: 28, Steps: 136 | Train Loss: 0.1997866 Vali Loss: 0.1979600 Test Loss: 0.2355575
Validation loss decreased (0.198589 --> 0.197960).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1956900
	speed: 0.6259s/iter; left time: 108.2767s
Epoch: 29 cost time: 36.347793102264404
Epoch: 29, Steps: 136 | Train Loss: 0.1983089 Vali Loss: 0.1965481 Test Loss: 0.2342826
Validation loss decreased (0.197960 --> 0.196548).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1978886
	speed: 0.6024s/iter; left time: 22.2891s
Epoch: 30 cost time: 35.84678316116333
Epoch: 30, Steps: 136 | Train Loss: 0.1971042 Vali Loss: 0.1956494 Test Loss: 0.2330831
Validation loss decreased (0.196548 --> 0.195649).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 17501
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=82, out_features=389, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1310625024.0
params:  32287.0
Trainable parameters:  32287
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2505340
	speed: 0.2618s/iter; left time: 1042.3374s
Epoch: 1 cost time: 35.17863845825195
Epoch: 1, Steps: 136 | Train Loss: 0.2392044 Vali Loss: 0.1915641 Test Loss: 0.2242011
Validation loss decreased (inf --> 0.191564).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2215157
	speed: 0.6210s/iter; left time: 2387.6950s
Epoch: 2 cost time: 38.303216218948364
Epoch: 2, Steps: 136 | Train Loss: 0.2371955 Vali Loss: 0.1919021 Test Loss: 0.2239388
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2317614
	speed: 0.6008s/iter; left time: 2228.5182s
Epoch: 3 cost time: 35.34036588668823
Epoch: 3, Steps: 136 | Train Loss: 0.2371429 Vali Loss: 0.1922294 Test Loss: 0.2239809
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2223532
	speed: 0.6223s/iter; left time: 2223.5246s
Epoch: 4 cost time: 37.30559182167053
Epoch: 4, Steps: 136 | Train Loss: 0.2371383 Vali Loss: 0.1915906 Test Loss: 0.2239837
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2431885
	speed: 0.6078s/iter; left time: 2088.9519s
Epoch: 5 cost time: 36.4982328414917
Epoch: 5, Steps: 136 | Train Loss: 0.2370640 Vali Loss: 0.1913923 Test Loss: 0.2239600
Validation loss decreased (0.191564 --> 0.191392).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2254469
	speed: 0.6276s/iter; left time: 2071.7900s
Epoch: 6 cost time: 36.50747060775757
Epoch: 6, Steps: 136 | Train Loss: 0.2370660 Vali Loss: 0.1916312 Test Loss: 0.2239094
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2444130
	speed: 0.6075s/iter; left time: 1922.6057s
Epoch: 7 cost time: 37.93896794319153
Epoch: 7, Steps: 136 | Train Loss: 0.2370855 Vali Loss: 0.1917389 Test Loss: 0.2239683
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2369059
	speed: 0.6185s/iter; left time: 1873.4074s
Epoch: 8 cost time: 35.27689528465271
Epoch: 8, Steps: 136 | Train Loss: 0.2370541 Vali Loss: 0.1911316 Test Loss: 0.2239442
Validation loss decreased (0.191392 --> 0.191132).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2359387
	speed: 0.6186s/iter; left time: 1789.5538s
Epoch: 9 cost time: 37.41048836708069
Epoch: 9, Steps: 136 | Train Loss: 0.2371136 Vali Loss: 0.1914520 Test Loss: 0.2239458
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2411586
	speed: 0.6001s/iter; left time: 1654.5473s
Epoch: 10 cost time: 35.403892993927
Epoch: 10, Steps: 136 | Train Loss: 0.2370712 Vali Loss: 0.1913724 Test Loss: 0.2239653
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2408055
	speed: 0.5974s/iter; left time: 1565.7205s
Epoch: 11 cost time: 36.170902729034424
Epoch: 11, Steps: 136 | Train Loss: 0.2370436 Vali Loss: 0.1916472 Test Loss: 0.2239908
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2378777
	speed: 0.6205s/iter; left time: 1541.9930s
Epoch: 12 cost time: 34.17061996459961
Epoch: 12, Steps: 136 | Train Loss: 0.2369718 Vali Loss: 0.1914783 Test Loss: 0.2239681
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2464688
	speed: 0.6006s/iter; left time: 1410.7195s
Epoch: 13 cost time: 33.91743588447571
Epoch: 13, Steps: 136 | Train Loss: 0.2369426 Vali Loss: 0.1914981 Test Loss: 0.2239376
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : electricity_192_720_FITS_custom_ftM_sl192_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.22235621511936188, mae:0.30667224526405334, rse:0.4703834354877472, corr:[0.4456086  0.44814533 0.44894838 0.44914633 0.44902617 0.44888908
 0.44903117 0.44873157 0.44854605 0.44843    0.44821852 0.44818836
 0.4480357  0.44801673 0.4478978  0.44786078 0.44771603 0.44750154
 0.4474994  0.44731125 0.44718206 0.4470561  0.44699565 0.44714656
 0.44713992 0.44712642 0.4471891  0.44713247 0.44709247 0.4470094
 0.44678178 0.44662407 0.44637406 0.44622055 0.44618824 0.44605082
 0.44597134 0.44595256 0.44591612 0.44584635 0.44578892 0.44584632
 0.44573504 0.44560856 0.4455892  0.4454939  0.4454993  0.44561753
 0.4456185  0.44569606 0.4457744  0.44562122 0.44555765 0.44545782
 0.44539535 0.4453036  0.44508177 0.44507504 0.44511    0.44501448
 0.444985   0.44496378 0.44488505 0.44483963 0.444865   0.44485134
 0.44487613 0.44477516 0.44462183 0.4446514  0.4446557  0.44465044
 0.44474968 0.44490692 0.4449093  0.44481277 0.44472444 0.4446052
 0.44454738 0.44445494 0.44435942 0.44432983 0.44430983 0.4442881
 0.44422767 0.44422993 0.44416076 0.44412312 0.4442163  0.44419974
 0.44417775 0.44412357 0.44400182 0.44389737 0.4439052  0.44409877
 0.44425246 0.44425032 0.4442301  0.4441801  0.44404092 0.44398516
 0.4439564  0.44386372 0.44385126 0.4437733  0.44364685 0.44359767
 0.44355133 0.44351992 0.44354188 0.44360992 0.44359595 0.4435862
 0.4435954  0.4435057  0.44351137 0.44357422 0.44359893 0.4437545
 0.44407558 0.44420364 0.44427833 0.444347   0.44426644 0.44417658
 0.44412267 0.44411162 0.44404262 0.44398415 0.44396582 0.44383293
 0.44376585 0.44374895 0.44381604 0.44385105 0.44383013 0.44393203
 0.44402653 0.44409826 0.44423878 0.44449574 0.44464198 0.4449163
 0.44556376 0.4458624  0.44592327 0.44604617 0.4460304  0.4459984
 0.4461053  0.4461151  0.44598052 0.4459167  0.44586393 0.4458305
 0.44582334 0.44575813 0.44573116 0.44568273 0.44557887 0.44559082
 0.445748   0.44571507 0.44560236 0.44554248 0.44546217 0.44531462
 0.44494146 0.44460085 0.4443907  0.4441146  0.44373134 0.44353217
 0.44342154 0.44317853 0.4430305  0.4429429  0.4427943  0.4427733
 0.44281    0.44271636 0.44268453 0.4427274  0.44265446 0.4425826
 0.44260657 0.44242546 0.44218627 0.44212866 0.4420893  0.44208744
 0.44189754 0.4416252  0.44164726 0.44159266 0.44137126 0.44142497
 0.4414266  0.44113922 0.4409848  0.4408973  0.44073445 0.44067317
 0.44062865 0.44051734 0.44050232 0.44053176 0.44055134 0.4406311
 0.44062555 0.4404279  0.440428   0.44041893 0.44035587 0.44054434
 0.4405169  0.4404042  0.44040734 0.44021428 0.44009978 0.44009364
 0.44010624 0.44007996 0.4399298  0.43983158 0.4398046  0.43974102
 0.43969065 0.43958452 0.43952206 0.43961665 0.43959022 0.43955457
 0.4396395  0.43942457 0.43930048 0.43927386 0.439248   0.43944824
 0.43943724 0.43945298 0.43958586 0.4394251  0.43930823 0.43924427
 0.43911377 0.43908957 0.439071   0.43906492 0.43906122 0.43893486
 0.43879786 0.43874562 0.4387363  0.43879047 0.438791   0.43871102
 0.4388044  0.4388056  0.4386745  0.43867126 0.43876174 0.43895966
 0.43905085 0.43899706 0.4389773  0.43899828 0.4389503  0.43884486
 0.438777   0.43877104 0.43874353 0.43862343 0.4385432  0.43847
 0.43837103 0.4384619  0.43848392 0.43835947 0.4384608  0.43856132
 0.43844363 0.438334   0.43838677 0.43847623 0.43856806 0.43881667
 0.43910974 0.43923804 0.439348   0.43937615 0.43928462 0.43928385
 0.43928316 0.43912452 0.43901286 0.43906805 0.43913966 0.4390528
 0.4390331  0.43914616 0.43912753 0.43906954 0.43913344 0.43920082
 0.43927065 0.43947935 0.43964016 0.43975803 0.4399786  0.4403939
 0.44098398 0.44122517 0.4413617  0.44148907 0.44144365 0.44140965
 0.44135216 0.44129378 0.4412314  0.44116428 0.4411226  0.44101298
 0.4410292  0.44108176 0.44099173 0.44095874 0.4409758  0.44103873
 0.44108617 0.44097376 0.44086283 0.44071537 0.4406015  0.44031298
 0.43978995 0.43951976 0.43932834 0.43909347 0.43884036 0.43850723
 0.43830615 0.4382029  0.4379852  0.43782473 0.43773168 0.4375562
 0.43753836 0.4375196  0.4373357  0.43732828 0.43734837 0.4373289
 0.43735468 0.43725532 0.4371623  0.4370849  0.4370695  0.437089
 0.4369113  0.43675306 0.43668616 0.43664417 0.43654212 0.43646562
 0.43641397 0.43621993 0.43594703 0.4358386  0.43579456 0.4356536
 0.43549913 0.43547058 0.4355889  0.43553993 0.4354006  0.43542004
 0.43538764 0.43532854 0.43528312 0.4352108  0.4353292  0.43544838
 0.43543178 0.4354683  0.43547785 0.43548855 0.4353408  0.43515155
 0.43525785 0.4352601  0.43505347 0.4349877  0.43486023 0.43479404
 0.43489158 0.43483374 0.43477222 0.43482786 0.43488002 0.43487632
 0.4348143  0.43474242 0.43467045 0.43457243 0.43458992 0.43469432
 0.43478638 0.4349367  0.4349545  0.4348303  0.4347154  0.43471676
 0.43477035 0.43466437 0.43450943 0.43452513 0.4344837  0.43440711
 0.4344438  0.4343868  0.43436897 0.43443972 0.43439496 0.43434015
 0.43438524 0.43437353 0.43429777 0.43420357 0.43432888 0.43457678
 0.43456167 0.43461394 0.434751   0.43454692 0.43442073 0.43450424
 0.43450588 0.43451926 0.43442637 0.43426663 0.4341972  0.43402398
 0.43393233 0.43406272 0.4341359  0.43411955 0.43413883 0.43415383
 0.43412375 0.4340504  0.434025   0.43408775 0.4342377  0.4345024
 0.4346837  0.4348252  0.4350592  0.4350806  0.43507496 0.43517452
 0.43516985 0.43513015 0.4351126  0.43506634 0.4350405  0.43509164
 0.43509802 0.43513325 0.4352568  0.43520606 0.43523067 0.43526608
 0.43530533 0.4354709  0.4355063  0.4356851  0.4360022  0.43628863
 0.43677378 0.43708527 0.4370803  0.437215   0.43727663 0.43712866
 0.43717253 0.43715602 0.43703353 0.4370681  0.43703678 0.4369346
 0.43696037 0.43702662 0.4369541  0.4369081  0.43694663 0.43694958
 0.43703803 0.43705934 0.43685302 0.43668693 0.43666023 0.43637767
 0.43577752 0.4353891  0.43511462 0.43484107 0.43455145 0.4343366
 0.43419063 0.43398026 0.4337971  0.43362162 0.43346804 0.43343127
 0.43342268 0.4334134  0.43335035 0.433342   0.4333477  0.43324482
 0.4332267  0.43302557 0.43290144 0.4329992  0.4328815  0.4328071
 0.43265697 0.43237388 0.43229258 0.43218383 0.43200156 0.4319913
 0.43201202 0.43178865 0.4314922  0.43131796 0.4312379  0.43117368
 0.43108717 0.4311228  0.43106315 0.43093303 0.43101165 0.431008
 0.43089923 0.43079188 0.43065715 0.4305836  0.43065414 0.43073562
 0.43063602 0.43056473 0.43055823 0.43036115 0.43020618 0.43018648
 0.4301764  0.43015227 0.43013567 0.43004826 0.42990166 0.42979175
 0.42982647 0.4298333  0.4297081  0.42958286 0.42957798 0.42955852
 0.42942885 0.4293386  0.42930093 0.42928    0.4293784  0.4293676
 0.42926353 0.42938927 0.42942268 0.42924002 0.42918423 0.42910734
 0.42906937 0.42911872 0.42903027 0.42897102 0.42896587 0.4288679
 0.42879826 0.4287918  0.4287955  0.42874485 0.42873108 0.4287778
 0.42872632 0.42870802 0.42874518 0.4286231  0.42865768 0.42883718
 0.42889774 0.4289716  0.42897627 0.42890918 0.4288747  0.42887443
 0.42887893 0.42875186 0.42864558 0.42858708 0.42849535 0.42846552
 0.42848158 0.42847243 0.42845577 0.42843157 0.42846537 0.42854366
 0.42846972 0.42839792 0.42848343 0.42851624 0.42859924 0.42886558
 0.42923835 0.42935333 0.42936352 0.4294523  0.42945588 0.42942917
 0.4294645  0.42941138 0.42929062 0.42917517 0.42908224 0.4290997
 0.42913374 0.4290499  0.42914325 0.4292047  0.42915973 0.42928237
 0.42940965 0.42959097 0.4297469  0.43001705 0.43037075 0.43073478
 0.4313642  0.43170702 0.4317811  0.431778   0.43180758 0.4317945
 0.4316813  0.43169174 0.43163916 0.43148273 0.43149677 0.4314163
 0.43136466 0.43150315 0.4314992  0.43147066 0.43142608 0.43145004
 0.43156835 0.4314934  0.431426   0.43134406 0.43121305 0.43097824
 0.43049362 0.43016386 0.4298687  0.42955908 0.4292683  0.42905203
 0.4289004  0.42865434 0.42825988 0.42808247 0.4280582  0.42800957
 0.4279843  0.42786577 0.427925   0.42787334 0.42778057 0.42786562
 0.42776218 0.4276667  0.4275864  0.42321494 0.42319953 0.4232149
 0.42290196 0.42273253 0.42269763 0.42250744 0.42238134 0.42212152
 0.42218244 0.42201418 0.42157325 0.42162117 0.42151716 0.42146903
 0.4214053  0.4214729  0.42160237 0.42148337 0.4213243  0.42132267
 0.42127645 0.42119974 0.42123726 0.42107263 0.42173502 0.42144135]
