Args in experiment:
Namespace(is_training=1, model_id='ETTh2_96_192', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=40, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_96_192_FITS_ETTh2_ftM_sl96_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8353
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=40, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4300800.0
params:  4920.0
Trainable parameters:  4920
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.9314446449279785
Epoch: 1, Steps: 65 | Train Loss: 0.6359265 Vali Loss: 0.3808719 Test Loss: 0.5407347
Validation loss decreased (inf --> 0.380872).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.3865034580230713
Epoch: 2, Steps: 65 | Train Loss: 0.5346794 Vali Loss: 0.3483216 Test Loss: 0.4931872
Validation loss decreased (0.380872 --> 0.348322).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.4640755653381348
Epoch: 3, Steps: 65 | Train Loss: 0.4786540 Vali Loss: 0.3295470 Test Loss: 0.4661252
Validation loss decreased (0.348322 --> 0.329547).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.5295345783233643
Epoch: 4, Steps: 65 | Train Loss: 0.4469145 Vali Loss: 0.3176777 Test Loss: 0.4495434
Validation loss decreased (0.329547 --> 0.317678).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.5155587196350098
Epoch: 5, Steps: 65 | Train Loss: 0.4257162 Vali Loss: 0.3101449 Test Loss: 0.4389484
Validation loss decreased (0.317678 --> 0.310145).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.3034756183624268
Epoch: 6, Steps: 65 | Train Loss: 0.4128811 Vali Loss: 0.3049804 Test Loss: 0.4320329
Validation loss decreased (0.310145 --> 0.304980).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.515960693359375
Epoch: 7, Steps: 65 | Train Loss: 0.4032042 Vali Loss: 0.3012246 Test Loss: 0.4271471
Validation loss decreased (0.304980 --> 0.301225).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.1776325702667236
Epoch: 8, Steps: 65 | Train Loss: 0.3970014 Vali Loss: 0.2984546 Test Loss: 0.4235768
Validation loss decreased (0.301225 --> 0.298455).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.3004167079925537
Epoch: 9, Steps: 65 | Train Loss: 0.3928492 Vali Loss: 0.2962816 Test Loss: 0.4208516
Validation loss decreased (0.298455 --> 0.296282).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2897655963897705
Epoch: 10, Steps: 65 | Train Loss: 0.3887365 Vali Loss: 0.2944813 Test Loss: 0.4187612
Validation loss decreased (0.296282 --> 0.294481).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.8781237602233887
Epoch: 11, Steps: 65 | Train Loss: 0.3852101 Vali Loss: 0.2930391 Test Loss: 0.4169715
Validation loss decreased (0.294481 --> 0.293039).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3460099697113037
Epoch: 12, Steps: 65 | Train Loss: 0.3841131 Vali Loss: 0.2917607 Test Loss: 0.4154377
Validation loss decreased (0.293039 --> 0.291761).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.185150623321533
Epoch: 13, Steps: 65 | Train Loss: 0.3819258 Vali Loss: 0.2906027 Test Loss: 0.4141390
Validation loss decreased (0.291761 --> 0.290603).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.277463436126709
Epoch: 14, Steps: 65 | Train Loss: 0.3803650 Vali Loss: 0.2895157 Test Loss: 0.4130062
Validation loss decreased (0.290603 --> 0.289516).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.1588268280029297
Epoch: 15, Steps: 65 | Train Loss: 0.3784861 Vali Loss: 0.2886029 Test Loss: 0.4119202
Validation loss decreased (0.289516 --> 0.288603).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.407315731048584
Epoch: 16, Steps: 65 | Train Loss: 0.3777230 Vali Loss: 0.2877633 Test Loss: 0.4110737
Validation loss decreased (0.288603 --> 0.287763).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.358961820602417
Epoch: 17, Steps: 65 | Train Loss: 0.3765809 Vali Loss: 0.2870848 Test Loss: 0.4102562
Validation loss decreased (0.287763 --> 0.287085).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.570967435836792
Epoch: 18, Steps: 65 | Train Loss: 0.3755907 Vali Loss: 0.2863788 Test Loss: 0.4095052
Validation loss decreased (0.287085 --> 0.286379).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.188760995864868
Epoch: 19, Steps: 65 | Train Loss: 0.3743415 Vali Loss: 0.2857908 Test Loss: 0.4088461
Validation loss decreased (0.286379 --> 0.285791).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.2545454502105713
Epoch: 20, Steps: 65 | Train Loss: 0.3723983 Vali Loss: 0.2851896 Test Loss: 0.4082687
Validation loss decreased (0.285791 --> 0.285190).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.807009696960449
Epoch: 21, Steps: 65 | Train Loss: 0.3728341 Vali Loss: 0.2847092 Test Loss: 0.4077119
Validation loss decreased (0.285190 --> 0.284709).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.20438814163208
Epoch: 22, Steps: 65 | Train Loss: 0.3723275 Vali Loss: 0.2842157 Test Loss: 0.4072260
Validation loss decreased (0.284709 --> 0.284216).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.339931011199951
Epoch: 23, Steps: 65 | Train Loss: 0.3714113 Vali Loss: 0.2837890 Test Loss: 0.4067481
Validation loss decreased (0.284216 --> 0.283789).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.1727793216705322
Epoch: 24, Steps: 65 | Train Loss: 0.3712763 Vali Loss: 0.2830639 Test Loss: 0.4063788
Validation loss decreased (0.283789 --> 0.283064).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.262305498123169
Epoch: 25, Steps: 65 | Train Loss: 0.3698461 Vali Loss: 0.2829627 Test Loss: 0.4059146
Validation loss decreased (0.283064 --> 0.282963).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.1331050395965576
Epoch: 26, Steps: 65 | Train Loss: 0.3700875 Vali Loss: 0.2826301 Test Loss: 0.4056153
Validation loss decreased (0.282963 --> 0.282630).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.0660006999969482
Epoch: 27, Steps: 65 | Train Loss: 0.3699364 Vali Loss: 0.2823534 Test Loss: 0.4053285
Validation loss decreased (0.282630 --> 0.282353).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.846008062362671
Epoch: 28, Steps: 65 | Train Loss: 0.3687508 Vali Loss: 0.2820280 Test Loss: 0.4050131
Validation loss decreased (0.282353 --> 0.282028).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.872886896133423
Epoch: 29, Steps: 65 | Train Loss: 0.3683746 Vali Loss: 0.2817737 Test Loss: 0.4047522
Validation loss decreased (0.282028 --> 0.281774).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.146901845932007
Epoch: 30, Steps: 65 | Train Loss: 0.3684910 Vali Loss: 0.2813606 Test Loss: 0.4044814
Validation loss decreased (0.281774 --> 0.281361).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 8353
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=40, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4300800.0
params:  4920.0
Trainable parameters:  4920
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.3022212982177734
Epoch: 1, Steps: 65 | Train Loss: 0.5419926 Vali Loss: 0.2780809 Test Loss: 0.4008715
Validation loss decreased (inf --> 0.278081).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.956545114517212
Epoch: 2, Steps: 65 | Train Loss: 0.5399696 Vali Loss: 0.2762133 Test Loss: 0.3991761
Validation loss decreased (0.278081 --> 0.276213).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.3755991458892822
Epoch: 3, Steps: 65 | Train Loss: 0.5375349 Vali Loss: 0.2750896 Test Loss: 0.3982328
Validation loss decreased (0.276213 --> 0.275090).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.2723324298858643
Epoch: 4, Steps: 65 | Train Loss: 0.5369243 Vali Loss: 0.2743051 Test Loss: 0.3978039
Validation loss decreased (0.275090 --> 0.274305).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.1800436973571777
Epoch: 5, Steps: 65 | Train Loss: 0.5358164 Vali Loss: 0.2738637 Test Loss: 0.3975003
Validation loss decreased (0.274305 --> 0.273864).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2328550815582275
Epoch: 6, Steps: 65 | Train Loss: 0.5341017 Vali Loss: 0.2734829 Test Loss: 0.3973840
Validation loss decreased (0.273864 --> 0.273483).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1719954013824463
Epoch: 7, Steps: 65 | Train Loss: 0.5318791 Vali Loss: 0.2731976 Test Loss: 0.3972276
Validation loss decreased (0.273483 --> 0.273198).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.8906044960021973
Epoch: 8, Steps: 65 | Train Loss: 0.5341229 Vali Loss: 0.2727131 Test Loss: 0.3971901
Validation loss decreased (0.273198 --> 0.272713).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.840067148208618
Epoch: 9, Steps: 65 | Train Loss: 0.5340952 Vali Loss: 0.2728352 Test Loss: 0.3970937
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.5191915035247803
Epoch: 10, Steps: 65 | Train Loss: 0.5346998 Vali Loss: 0.2727457 Test Loss: 0.3971737
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.4061436653137207
Epoch: 11, Steps: 65 | Train Loss: 0.5322884 Vali Loss: 0.2726185 Test Loss: 0.3971550
Validation loss decreased (0.272713 --> 0.272619).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.2267260551452637
Epoch: 12, Steps: 65 | Train Loss: 0.5331906 Vali Loss: 0.2723790 Test Loss: 0.3970635
Validation loss decreased (0.272619 --> 0.272379).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.0772290229797363
Epoch: 13, Steps: 65 | Train Loss: 0.5327845 Vali Loss: 0.2724651 Test Loss: 0.3971507
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.596804141998291
Epoch: 14, Steps: 65 | Train Loss: 0.5336432 Vali Loss: 0.2724257 Test Loss: 0.3971710
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.22643780708313
Epoch: 15, Steps: 65 | Train Loss: 0.5324593 Vali Loss: 0.2723415 Test Loss: 0.3971583
Validation loss decreased (0.272379 --> 0.272341).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.5204527378082275
Epoch: 16, Steps: 65 | Train Loss: 0.5338008 Vali Loss: 0.2722694 Test Loss: 0.3972042
Validation loss decreased (0.272341 --> 0.272269).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.267364740371704
Epoch: 17, Steps: 65 | Train Loss: 0.5337062 Vali Loss: 0.2722323 Test Loss: 0.3971858
Validation loss decreased (0.272269 --> 0.272232).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.375454902648926
Epoch: 18, Steps: 65 | Train Loss: 0.5319150 Vali Loss: 0.2722152 Test Loss: 0.3971920
Validation loss decreased (0.272232 --> 0.272215).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.9146132469177246
Epoch: 19, Steps: 65 | Train Loss: 0.5325043 Vali Loss: 0.2721541 Test Loss: 0.3971381
Validation loss decreased (0.272215 --> 0.272154).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.488443613052368
Epoch: 20, Steps: 65 | Train Loss: 0.5323612 Vali Loss: 0.2720966 Test Loss: 0.3971820
Validation loss decreased (0.272154 --> 0.272097).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.8648242950439453
Epoch: 21, Steps: 65 | Train Loss: 0.5322753 Vali Loss: 0.2721254 Test Loss: 0.3972255
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.7482171058654785
Epoch: 22, Steps: 65 | Train Loss: 0.5327990 Vali Loss: 0.2720152 Test Loss: 0.3972329
Validation loss decreased (0.272097 --> 0.272015).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.128958225250244
Epoch: 23, Steps: 65 | Train Loss: 0.5322657 Vali Loss: 0.2719626 Test Loss: 0.3972247
Validation loss decreased (0.272015 --> 0.271963).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.4251909255981445
Epoch: 24, Steps: 65 | Train Loss: 0.5324604 Vali Loss: 0.2719992 Test Loss: 0.3972180
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.639108657836914
Epoch: 25, Steps: 65 | Train Loss: 0.5329803 Vali Loss: 0.2719772 Test Loss: 0.3972106
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.551743030548096
Epoch: 26, Steps: 65 | Train Loss: 0.5330782 Vali Loss: 0.2719449 Test Loss: 0.3972335
Validation loss decreased (0.271963 --> 0.271945).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.827369213104248
Epoch: 27, Steps: 65 | Train Loss: 0.5332327 Vali Loss: 0.2719432 Test Loss: 0.3972399
Validation loss decreased (0.271945 --> 0.271943).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.7534635066986084
Epoch: 28, Steps: 65 | Train Loss: 0.5329843 Vali Loss: 0.2718854 Test Loss: 0.3971931
Validation loss decreased (0.271943 --> 0.271885).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.478276491165161
Epoch: 29, Steps: 65 | Train Loss: 0.5320145 Vali Loss: 0.2719332 Test Loss: 0.3972648
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.359076261520386
Epoch: 30, Steps: 65 | Train Loss: 0.5315291 Vali Loss: 0.2718416 Test Loss: 0.3973031
Validation loss decreased (0.271885 --> 0.271842).  Saving model ...
Updating learning rate to 0.00011296777049628277
>>>>>>>testing : ETTh2_96_192_FITS_ETTh2_ftM_sl96_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3752710521221161, mae:0.39018377661705017, rse:0.4912622570991516, corr:[0.2694728  0.2686833  0.26834446 0.26788375 0.26633173 0.26571476
 0.26436886 0.2639734  0.26272523 0.26198643 0.26066586 0.25916314
 0.2573347  0.25549582 0.25469103 0.25389132 0.25376365 0.25344956
 0.25266314 0.25169447 0.25073904 0.24980558 0.24862257 0.24697235
 0.2442512  0.24191798 0.24000533 0.238293   0.23666981 0.23487695
 0.2334471  0.2320743  0.23016313 0.2283096  0.22734997 0.22611181
 0.22439973 0.22271499 0.22171514 0.22073899 0.21998055 0.21996164
 0.2193861  0.21853349 0.21743788 0.21616226 0.21541789 0.21323805
 0.20991135 0.20737518 0.20572512 0.20369762 0.20173356 0.19989385
 0.19728383 0.19534649 0.19321485 0.19142996 0.19041051 0.1888424
 0.18807262 0.18702377 0.18689197 0.18689577 0.18620057 0.18570425
 0.18508764 0.18460332 0.18419644 0.18371786 0.1832247  0.18170574
 0.18001142 0.17870428 0.17769356 0.17638059 0.17477335 0.1744411
 0.17342868 0.17237231 0.17153265 0.17082472 0.17107631 0.17058523
 0.17003368 0.1692776  0.16933581 0.16956666 0.16872777 0.16845399
 0.16800138 0.16742995 0.16732393 0.16696589 0.16678178 0.16558294
 0.16410871 0.16258743 0.16108412 0.15997209 0.15840997 0.15737487
 0.15647857 0.15548548 0.15511642 0.15464592 0.15455863 0.15422271
 0.15378106 0.15301843 0.15226804 0.15188631 0.15111189 0.15078138
 0.15040469 0.1502508  0.14963455 0.14896747 0.14833738 0.14611079
 0.14405145 0.14262837 0.14138703 0.14032489 0.13867518 0.13738447
 0.13637291 0.13538954 0.13466614 0.13393553 0.1337329  0.13303338
 0.13213743 0.13139977 0.13128066 0.13095672 0.12992816 0.12982313
 0.12938027 0.12859692 0.12818864 0.12770319 0.12720564 0.12529814
 0.12258749 0.12057152 0.1190012  0.11748637 0.11587262 0.11472015
 0.1138031  0.11307374 0.11233253 0.1115405  0.11169843 0.11140813
 0.11084356 0.11011495 0.1104157  0.11027832 0.11005391 0.11055262
 0.11001816 0.11006948 0.11004541 0.10989714 0.1104705  0.10917991
 0.1070748  0.10584109 0.10497465 0.10414876 0.10295062 0.10183924
 0.10080895 0.09970758 0.09908465 0.09835289 0.09840493 0.09861039
 0.09793993 0.09790435 0.09787878 0.09789871 0.09827065 0.0976238
 0.09709176 0.09768951 0.09746814 0.09831416 0.09888188 0.10079385]
