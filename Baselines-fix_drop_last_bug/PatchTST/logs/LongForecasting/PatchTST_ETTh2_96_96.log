Args in experiment:
Namespace(random_seed=2021, is_training=1, model_id='96_96', model='PatchTST', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, fc_dropout=0.3, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=16, n_heads=4, e_layers=3, d_layers=1, d_ff=128, moving_avg=25, factor=1, distil=True, dropout=0.3, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=1, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)
Use GPU: cuda:1
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.
MACs: 1473024.0
Params: 34976.0
1.47M MACs
>>>>>>>start training : 96_96_PatchTST_ETTh2_ftM_sl96_ll48_pl96_dm16_nh4_el3_dl1_df128_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8449
val 2785
test 2785
Epoch: 1 cost time: 8.773151397705078
Epoch: 1, Steps: 67 | Train Loss: 0.5885730 Vali Loss: 0.2917615 Test Loss: 0.3831721
Validation loss decreased (inf --> 0.291761).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 7.863638162612915
Epoch: 2, Steps: 67 | Train Loss: 0.5551007 Vali Loss: 0.2667289 Test Loss: 0.3522001
Validation loss decreased (0.291761 --> 0.266729).  Saving model ...
Updating learning rate to 0.0001
Epoch: 3 cost time: 8.484234809875488
Epoch: 3, Steps: 67 | Train Loss: 0.5126156 Vali Loss: 0.2457607 Test Loss: 0.3281626
Validation loss decreased (0.266729 --> 0.245761).  Saving model ...
Updating learning rate to 0.0001
Epoch: 4 cost time: 8.538698434829712
Epoch: 4, Steps: 67 | Train Loss: 0.4828492 Vali Loss: 0.2316402 Test Loss: 0.3127219
Validation loss decreased (0.245761 --> 0.231640).  Saving model ...
Updating learning rate to 9e-05
Epoch: 5 cost time: 7.902555465698242
Epoch: 5, Steps: 67 | Train Loss: 0.4601256 Vali Loss: 0.2253663 Test Loss: 0.3050016
Validation loss decreased (0.231640 --> 0.225366).  Saving model ...
Updating learning rate to 8.1e-05
Epoch: 6 cost time: 8.30565333366394
Epoch: 6, Steps: 67 | Train Loss: 0.4500712 Vali Loss: 0.2221043 Test Loss: 0.3011624
Validation loss decreased (0.225366 --> 0.222104).  Saving model ...
Updating learning rate to 7.290000000000001e-05
Epoch: 7 cost time: 7.759033203125
Epoch: 7, Steps: 67 | Train Loss: 0.4442669 Vali Loss: 0.2205490 Test Loss: 0.2986902
Validation loss decreased (0.222104 --> 0.220549).  Saving model ...
Updating learning rate to 6.561e-05
Epoch: 8 cost time: 7.946361541748047
Epoch: 8, Steps: 67 | Train Loss: 0.4481104 Vali Loss: 0.2193315 Test Loss: 0.2972653
Validation loss decreased (0.220549 --> 0.219331).  Saving model ...
Updating learning rate to 5.904900000000001e-05
Epoch: 9 cost time: 8.226486682891846
Epoch: 9, Steps: 67 | Train Loss: 0.4503363 Vali Loss: 0.2184703 Test Loss: 0.2963211
Validation loss decreased (0.219331 --> 0.218470).  Saving model ...
Updating learning rate to 5.3144100000000005e-05
Epoch: 10 cost time: 8.1217520236969
Epoch: 10, Steps: 67 | Train Loss: 0.4403805 Vali Loss: 0.2181321 Test Loss: 0.2951677
Validation loss decreased (0.218470 --> 0.218132).  Saving model ...
Updating learning rate to 4.782969000000001e-05
Epoch: 11 cost time: 7.949502468109131
Epoch: 11, Steps: 67 | Train Loss: 0.4347250 Vali Loss: 0.2179798 Test Loss: 0.2946878
Validation loss decreased (0.218132 --> 0.217980).  Saving model ...
Updating learning rate to 4.304672100000001e-05
Epoch: 12 cost time: 7.766113042831421
Epoch: 12, Steps: 67 | Train Loss: 0.4337737 Vali Loss: 0.2177297 Test Loss: 0.2942874
Validation loss decreased (0.217980 --> 0.217730).  Saving model ...
Updating learning rate to 3.874204890000001e-05
Epoch: 13 cost time: 8.106819152832031
Epoch: 13, Steps: 67 | Train Loss: 0.4331591 Vali Loss: 0.2168730 Test Loss: 0.2939506
Validation loss decreased (0.217730 --> 0.216873).  Saving model ...
Updating learning rate to 3.486784401000001e-05
Epoch: 14 cost time: 8.260968685150146
Epoch: 14, Steps: 67 | Train Loss: 0.4370414 Vali Loss: 0.2168056 Test Loss: 0.2936138
Validation loss decreased (0.216873 --> 0.216806).  Saving model ...
Updating learning rate to 3.138105960900001e-05
Epoch: 15 cost time: 8.34245491027832
Epoch: 15, Steps: 67 | Train Loss: 0.4380641 Vali Loss: 0.2162018 Test Loss: 0.2934266
Validation loss decreased (0.216806 --> 0.216202).  Saving model ...
Updating learning rate to 2.824295364810001e-05
Epoch: 16 cost time: 8.47441816329956
Epoch: 16, Steps: 67 | Train Loss: 0.4484822 Vali Loss: 0.2166637 Test Loss: 0.2932608
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.541865828329001e-05
Epoch: 17 cost time: 8.19403624534607
Epoch: 17, Steps: 67 | Train Loss: 0.4313715 Vali Loss: 0.2162776 Test Loss: 0.2930739
EarlyStopping counter: 2 out of 5
Updating learning rate to 2.287679245496101e-05
Epoch: 18 cost time: 8.411455869674683
Epoch: 18, Steps: 67 | Train Loss: 0.4302384 Vali Loss: 0.2157833 Test Loss: 0.2929620
Validation loss decreased (0.216202 --> 0.215783).  Saving model ...
Updating learning rate to 2.0589113209464907e-05
Epoch: 19 cost time: 7.434417724609375
Epoch: 19, Steps: 67 | Train Loss: 0.4351916 Vali Loss: 0.2159222 Test Loss: 0.2927913
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.8530201888518416e-05
Epoch: 20 cost time: 8.641791582107544
Epoch: 20, Steps: 67 | Train Loss: 0.4373489 Vali Loss: 0.2161962 Test Loss: 0.2926030
EarlyStopping counter: 2 out of 5
Updating learning rate to 1.6677181699666577e-05
Epoch: 21 cost time: 8.888663053512573
Epoch: 21, Steps: 67 | Train Loss: 0.4283728 Vali Loss: 0.2156616 Test Loss: 0.2926356
Validation loss decreased (0.215783 --> 0.215662).  Saving model ...
Updating learning rate to 1.5009463529699919e-05
Epoch: 22 cost time: 8.39994215965271
Epoch: 22, Steps: 67 | Train Loss: 0.4367778 Vali Loss: 0.2157540 Test Loss: 0.2924368
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.3508517176729929e-05
Epoch: 23 cost time: 8.369051218032837
Epoch: 23, Steps: 67 | Train Loss: 0.4293756 Vali Loss: 0.2155053 Test Loss: 0.2925499
Validation loss decreased (0.215662 --> 0.215505).  Saving model ...
Updating learning rate to 1.2157665459056936e-05
Epoch: 24 cost time: 8.759384870529175
Epoch: 24, Steps: 67 | Train Loss: 0.4278648 Vali Loss: 0.2156047 Test Loss: 0.2923935
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.0941898913151242e-05
Epoch: 25 cost time: 8.21390986442566
Epoch: 25, Steps: 67 | Train Loss: 0.4455526 Vali Loss: 0.2155936 Test Loss: 0.2923321
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.847709021836118e-06
Epoch: 26 cost time: 9.241058111190796
Epoch: 26, Steps: 67 | Train Loss: 0.4265734 Vali Loss: 0.2152060 Test Loss: 0.2923492
Validation loss decreased (0.215505 --> 0.215206).  Saving model ...
Updating learning rate to 8.862938119652508e-06
Epoch: 27 cost time: 8.619349002838135
Epoch: 27, Steps: 67 | Train Loss: 0.4285751 Vali Loss: 0.2155089 Test Loss: 0.2922393
EarlyStopping counter: 1 out of 5
Updating learning rate to 7.976644307687255e-06
Epoch: 28 cost time: 8.606060266494751
Epoch: 28, Steps: 67 | Train Loss: 0.4269222 Vali Loss: 0.2152912 Test Loss: 0.2922256
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.178979876918531e-06
Epoch: 29 cost time: 8.108853816986084
Epoch: 29, Steps: 67 | Train Loss: 0.4274880 Vali Loss: 0.2155344 Test Loss: 0.2921272
EarlyStopping counter: 3 out of 5
Updating learning rate to 6.4610818892266776e-06
Epoch: 30 cost time: 8.181051254272461
Epoch: 30, Steps: 67 | Train Loss: 0.4268453 Vali Loss: 0.2151406 Test Loss: 0.2921605
Validation loss decreased (0.215206 --> 0.215141).  Saving model ...
Updating learning rate to 5.8149737003040096e-06
>>>>>>>testing : 96_96_PatchTST_ETTh2_ftM_sl96_ll48_pl96_dm16_nh4_el3_dl1_df128_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.29275327920913696, mae:0.34269747138023376, rse:0.43604588508605957
