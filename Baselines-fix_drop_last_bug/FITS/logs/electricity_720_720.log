Args in experiment:
Namespace(is_training=1, model_id='electricity_720_720', model='FITS', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=321, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=258, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : electricity_720_720_FITS_custom_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5469963264.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7962390
	speed: 0.3494s/iter; left time: 1348.9992s
Epoch: 1 cost time: 45.64615511894226
Epoch: 1, Steps: 132 | Train Loss: 0.9749518 Vali Loss: 0.6976737 Test Loss: 0.8156208
Validation loss decreased (inf --> 0.697674).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6680288
	speed: 0.7090s/iter; left time: 2644.0411s
Epoch: 2 cost time: 45.347193241119385
Epoch: 2, Steps: 132 | Train Loss: 0.7011448 Vali Loss: 0.6111193 Test Loss: 0.7176037
Validation loss decreased (0.697674 --> 0.611119).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5930548
	speed: 0.7265s/iter; left time: 2613.1022s
Epoch: 3 cost time: 43.52640628814697
Epoch: 3, Steps: 132 | Train Loss: 0.6116934 Vali Loss: 0.5498360 Test Loss: 0.6492333
Validation loss decreased (0.611119 --> 0.549836).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5260395
	speed: 0.7492s/iter; left time: 2595.9412s
Epoch: 4 cost time: 46.90912675857544
Epoch: 4, Steps: 132 | Train Loss: 0.5413678 Vali Loss: 0.4984875 Test Loss: 0.5917227
Validation loss decreased (0.549836 --> 0.498488).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4844533
	speed: 0.7153s/iter; left time: 2383.9667s
Epoch: 5 cost time: 43.73133611679077
Epoch: 5, Steps: 132 | Train Loss: 0.4832329 Vali Loss: 0.4564353 Test Loss: 0.5441594
Validation loss decreased (0.498488 --> 0.456435).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4265589
	speed: 0.7357s/iter; left time: 2354.9509s
Epoch: 6 cost time: 42.33833456039429
Epoch: 6, Steps: 132 | Train Loss: 0.4344572 Vali Loss: 0.4196295 Test Loss: 0.5023220
Validation loss decreased (0.456435 --> 0.419630).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3871194
	speed: 0.7011s/iter; left time: 2151.7250s
Epoch: 7 cost time: 40.037848234176636
Epoch: 7, Steps: 132 | Train Loss: 0.3931035 Vali Loss: 0.3877411 Test Loss: 0.4662515
Validation loss decreased (0.419630 --> 0.387741).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3545787
	speed: 0.6531s/iter; left time: 1918.0228s
Epoch: 8 cost time: 40.08597111701965
Epoch: 8, Steps: 132 | Train Loss: 0.3578489 Vali Loss: 0.3601444 Test Loss: 0.4340990
Validation loss decreased (0.387741 --> 0.360144).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3228767
	speed: 0.6464s/iter; left time: 1813.1764s
Epoch: 9 cost time: 40.009308099746704
Epoch: 9, Steps: 132 | Train Loss: 0.3276276 Vali Loss: 0.3369256 Test Loss: 0.4074996
Validation loss decreased (0.360144 --> 0.336926).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2854850
	speed: 0.6162s/iter; left time: 1647.0931s
Epoch: 10 cost time: 37.34459638595581
Epoch: 10, Steps: 132 | Train Loss: 0.3016457 Vali Loss: 0.3166687 Test Loss: 0.3840777
Validation loss decreased (0.336926 --> 0.316669).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2764532
	speed: 0.6238s/iter; left time: 1585.0640s
Epoch: 11 cost time: 38.63887071609497
Epoch: 11, Steps: 132 | Train Loss: 0.2791298 Vali Loss: 0.2993686 Test Loss: 0.3638058
Validation loss decreased (0.316669 --> 0.299369).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2566453
	speed: 0.6164s/iter; left time: 1485.0106s
Epoch: 12 cost time: 38.43640899658203
Epoch: 12, Steps: 132 | Train Loss: 0.2597510 Vali Loss: 0.2830296 Test Loss: 0.3450025
Validation loss decreased (0.299369 --> 0.283030).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2377317
	speed: 0.6253s/iter; left time: 1423.7992s
Epoch: 13 cost time: 39.583049058914185
Epoch: 13, Steps: 132 | Train Loss: 0.2427699 Vali Loss: 0.2702984 Test Loss: 0.3298309
Validation loss decreased (0.283030 --> 0.270298).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2291297
	speed: 0.6572s/iter; left time: 1409.7781s
Epoch: 14 cost time: 40.02015709877014
Epoch: 14, Steps: 132 | Train Loss: 0.2279947 Vali Loss: 0.2590803 Test Loss: 0.3169409
Validation loss decreased (0.270298 --> 0.259080).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2162294
	speed: 0.6099s/iter; left time: 1227.6522s
Epoch: 15 cost time: 37.486809968948364
Epoch: 15, Steps: 132 | Train Loss: 0.2150777 Vali Loss: 0.2496035 Test Loss: 0.3055137
Validation loss decreased (0.259080 --> 0.249604).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1986155
	speed: 0.5969s/iter; left time: 1122.7483s
Epoch: 16 cost time: 36.793118953704834
Epoch: 16, Steps: 132 | Train Loss: 0.2038171 Vali Loss: 0.2406126 Test Loss: 0.2949086
Validation loss decreased (0.249604 --> 0.240613).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1949374
	speed: 0.6115s/iter; left time: 1069.5632s
Epoch: 17 cost time: 38.16954016685486
Epoch: 17, Steps: 132 | Train Loss: 0.1939635 Vali Loss: 0.2328587 Test Loss: 0.2851444
Validation loss decreased (0.240613 --> 0.232859).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1865138
	speed: 0.6221s/iter; left time: 1005.8848s
Epoch: 18 cost time: 39.69532370567322
Epoch: 18, Steps: 132 | Train Loss: 0.1851874 Vali Loss: 0.2260470 Test Loss: 0.2768050
Validation loss decreased (0.232859 --> 0.226047).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1768285
	speed: 0.6671s/iter; left time: 990.6287s
Epoch: 19 cost time: 40.78867530822754
Epoch: 19, Steps: 132 | Train Loss: 0.1775159 Vali Loss: 0.2198994 Test Loss: 0.2693567
Validation loss decreased (0.226047 --> 0.219899).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1794291
	speed: 0.6289s/iter; left time: 850.9418s
Epoch: 20 cost time: 39.24577832221985
Epoch: 20, Steps: 132 | Train Loss: 0.1707568 Vali Loss: 0.2144505 Test Loss: 0.2631505
Validation loss decreased (0.219899 --> 0.214450).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1632772
	speed: 0.6241s/iter; left time: 762.0451s
Epoch: 21 cost time: 38.7465341091156
Epoch: 21, Steps: 132 | Train Loss: 0.1647525 Vali Loss: 0.2106960 Test Loss: 0.2575320
Validation loss decreased (0.214450 --> 0.210696).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1534600
	speed: 0.6232s/iter; left time: 678.6549s
Epoch: 22 cost time: 38.91029334068298
Epoch: 22, Steps: 132 | Train Loss: 0.1594899 Vali Loss: 0.2063682 Test Loss: 0.2525788
Validation loss decreased (0.210696 --> 0.206368).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1546723
	speed: 0.6259s/iter; left time: 598.9814s
Epoch: 23 cost time: 39.05722737312317
Epoch: 23, Steps: 132 | Train Loss: 0.1547894 Vali Loss: 0.2029905 Test Loss: 0.2481507
Validation loss decreased (0.206368 --> 0.202991).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1488780
	speed: 0.6599s/iter; left time: 544.4338s
Epoch: 24 cost time: 40.88765048980713
Epoch: 24, Steps: 132 | Train Loss: 0.1506266 Vali Loss: 0.1993754 Test Loss: 0.2438544
Validation loss decreased (0.202991 --> 0.199375).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1497986
	speed: 0.6223s/iter; left time: 431.2611s
Epoch: 25 cost time: 38.724650144577026
Epoch: 25, Steps: 132 | Train Loss: 0.1469478 Vali Loss: 0.1969142 Test Loss: 0.2404647
Validation loss decreased (0.199375 --> 0.196914).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1358893
	speed: 0.6266s/iter; left time: 351.5175s
Epoch: 26 cost time: 39.42294144630432
Epoch: 26, Steps: 132 | Train Loss: 0.1436597 Vali Loss: 0.1944175 Test Loss: 0.2374069
Validation loss decreased (0.196914 --> 0.194417).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1409589
	speed: 0.6128s/iter; left time: 262.9114s
Epoch: 27 cost time: 38.157663106918335
Epoch: 27, Steps: 132 | Train Loss: 0.1406699 Vali Loss: 0.1925369 Test Loss: 0.2348970
Validation loss decreased (0.194417 --> 0.192537).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1423832
	speed: 0.6316s/iter; left time: 187.5886s
Epoch: 28 cost time: 40.03199863433838
Epoch: 28, Steps: 132 | Train Loss: 0.1381456 Vali Loss: 0.1907088 Test Loss: 0.2321212
Validation loss decreased (0.192537 --> 0.190709).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1345169
	speed: 0.6592s/iter; left time: 108.7660s
Epoch: 29 cost time: 40.02535104751587
Epoch: 29, Steps: 132 | Train Loss: 0.1358201 Vali Loss: 0.1889509 Test Loss: 0.2298908
Validation loss decreased (0.190709 --> 0.188951).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1329341
	speed: 0.6176s/iter; left time: 20.3797s
Epoch: 30 cost time: 38.23518466949463
Epoch: 30, Steps: 132 | Train Loss: 0.1336962 Vali Loss: 0.1875345 Test Loss: 0.2278994
Validation loss decreased (0.188951 --> 0.187535).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5469963264.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2110036
	speed: 0.2963s/iter; left time: 1143.8708s
Epoch: 1 cost time: 38.42796778678894
Epoch: 1, Steps: 132 | Train Loss: 0.2244658 Vali Loss: 0.1764931 Test Loss: 0.2182123
Validation loss decreased (inf --> 0.176493).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2229968
	speed: 0.6211s/iter; left time: 2315.9278s
Epoch: 2 cost time: 38.65849471092224
Epoch: 2, Steps: 132 | Train Loss: 0.2197394 Vali Loss: 0.1770590 Test Loss: 0.2157082
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2216384
	speed: 0.6350s/iter; left time: 2284.1550s
Epoch: 3 cost time: 40.14447045326233
Epoch: 3, Steps: 132 | Train Loss: 0.2195665 Vali Loss: 0.1766812 Test Loss: 0.2155732
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2122313
	speed: 0.6575s/iter; left time: 2278.3005s
Epoch: 4 cost time: 39.94782376289368
Epoch: 4, Steps: 132 | Train Loss: 0.2194646 Vali Loss: 0.1764597 Test Loss: 0.2156022
Validation loss decreased (0.176493 --> 0.176460).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2156274
	speed: 0.6189s/iter; left time: 2062.8927s
Epoch: 5 cost time: 38.256375551223755
Epoch: 5, Steps: 132 | Train Loss: 0.2193644 Vali Loss: 0.1767754 Test Loss: 0.2155846
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2278480
	speed: 0.6231s/iter; left time: 1994.5156s
Epoch: 6 cost time: 38.54708003997803
Epoch: 6, Steps: 132 | Train Loss: 0.2192875 Vali Loss: 0.1763328 Test Loss: 0.2155287
Validation loss decreased (0.176460 --> 0.176333).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2192913
	speed: 0.6164s/iter; left time: 1891.8756s
Epoch: 7 cost time: 38.624961376190186
Epoch: 7, Steps: 132 | Train Loss: 0.2192275 Vali Loss: 0.1763268 Test Loss: 0.2155180
Validation loss decreased (0.176333 --> 0.176327).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2293247
	speed: 0.6331s/iter; left time: 1859.5212s
Epoch: 8 cost time: 40.041703939437866
Epoch: 8, Steps: 132 | Train Loss: 0.2192033 Vali Loss: 0.1768717 Test Loss: 0.2154966
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2252414
	speed: 0.6540s/iter; left time: 1834.3888s
Epoch: 9 cost time: 39.636921644210815
Epoch: 9, Steps: 132 | Train Loss: 0.2191852 Vali Loss: 0.1765693 Test Loss: 0.2155184
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2252600
	speed: 0.6230s/iter; left time: 1665.2694s
Epoch: 10 cost time: 38.5179328918457
Epoch: 10, Steps: 132 | Train Loss: 0.2191314 Vali Loss: 0.1764707 Test Loss: 0.2154700
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2143602
	speed: 0.6174s/iter; left time: 1568.9404s
Epoch: 11 cost time: 38.44987988471985
Epoch: 11, Steps: 132 | Train Loss: 0.2191312 Vali Loss: 0.1761842 Test Loss: 0.2154148
Validation loss decreased (0.176327 --> 0.176184).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2293766
	speed: 0.6136s/iter; left time: 1478.1594s
Epoch: 12 cost time: 38.500561475753784
Epoch: 12, Steps: 132 | Train Loss: 0.2191267 Vali Loss: 0.1767091 Test Loss: 0.2154600
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2165230
	speed: 0.6378s/iter; left time: 1452.1997s
Epoch: 13 cost time: 40.650349617004395
Epoch: 13, Steps: 132 | Train Loss: 0.2190448 Vali Loss: 0.1763159 Test Loss: 0.2154021
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2321535
	speed: 0.6512s/iter; left time: 1396.9145s
Epoch: 14 cost time: 39.44486880302429
Epoch: 14, Steps: 132 | Train Loss: 0.2190093 Vali Loss: 0.1762650 Test Loss: 0.2154483
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2198646
	speed: 0.6193s/iter; left time: 1246.5920s
Epoch: 15 cost time: 38.543580770492554
Epoch: 15, Steps: 132 | Train Loss: 0.2190608 Vali Loss: 0.1766779 Test Loss: 0.2154438
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2143104
	speed: 0.6178s/iter; left time: 1162.0252s
Epoch: 16 cost time: 37.879069089889526
Epoch: 16, Steps: 132 | Train Loss: 0.2190291 Vali Loss: 0.1762501 Test Loss: 0.2153825
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : electricity_720_720_FITS_custom_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2137224065065384, mae:0.30194284296035767, rse:0.4524475038051605, corr:[0.44631675 0.44883826 0.45033363 0.4504094  0.4509315  0.45097488
 0.45109197 0.45130092 0.45085573 0.45075327 0.450787   0.45057297
 0.45053092 0.45053074 0.45046514 0.45051098 0.4505588  0.45053074
 0.45047233 0.45047447 0.45044798 0.45037588 0.45049766 0.45065647
 0.45068035 0.45089966 0.4510382  0.4510063  0.45097196 0.45090947
 0.45079452 0.45066687 0.45044166 0.45025024 0.45011678 0.45006794
 0.45013008 0.45013693 0.45010623 0.45014167 0.450124   0.450156
 0.45024344 0.45006835 0.4499035  0.44996807 0.45003745 0.45011905
 0.4502088  0.4502498  0.45024192 0.45023477 0.4502263  0.45014805
 0.4499553  0.44990146 0.44995424 0.44984996 0.44971794 0.44967684
 0.4496925  0.44977328 0.44974253 0.44961238 0.4495737  0.44955587
 0.4495319  0.44942194 0.44929072 0.44928688 0.4493299  0.4492953
 0.44938812 0.44964015 0.44972438 0.44962147 0.44944328 0.4493609
 0.44927493 0.4491297  0.44916788 0.44919455 0.44905105 0.44909763
 0.449223   0.4491538  0.44909778 0.44913837 0.44914758 0.44913074
 0.44912747 0.4490729  0.4489723  0.44893545 0.44899014 0.4489938
 0.44898808 0.4491102  0.44913027 0.44904602 0.4489516  0.44884437
 0.44880882 0.44878647 0.44858953 0.44843367 0.44841594 0.44841084
 0.44846222 0.4485229  0.44848824 0.44846767 0.44852763 0.4487068
 0.4487759  0.44843945 0.44824004 0.44848067 0.44862196 0.44861686
 0.44876343 0.44882604 0.44882303 0.44881296 0.44873202 0.4487251
 0.44867614 0.44852862 0.44844195 0.44838667 0.44834486 0.4483019
 0.44821867 0.4482595  0.44834578 0.4483192  0.44845465 0.44867343
 0.4487116  0.44862795 0.44854245 0.44852474 0.44859853 0.44850996
 0.44836855 0.44854087 0.44869918 0.44860718 0.4485053  0.44851542
 0.4484929  0.44840747 0.4483668  0.44831645 0.44813544 0.4480192
 0.44810528 0.44813812 0.44807163 0.44807255 0.44812733 0.44814527
 0.44824535 0.4482911  0.44825485 0.44826213 0.44820064 0.4480096
 0.44779357 0.4477764  0.44783142 0.44784316 0.447776   0.447737
 0.44763598 0.44741672 0.44724274 0.4471602  0.44710106 0.44711602
 0.44715682 0.4471336  0.44712687 0.44720203 0.44723514 0.44718525
 0.44713175 0.44693553 0.44674328 0.44674677 0.4468123  0.44676426
 0.44663566 0.4466033  0.44658843 0.44655636 0.44654185 0.44660679
 0.44653118 0.446285   0.4461906  0.4462185  0.44615185 0.44605535
 0.4460425  0.44612995 0.44616434 0.4459888  0.44590393 0.4460419
 0.4460905  0.44591048 0.44583037 0.4458251  0.44579527 0.44580534
 0.44579718 0.44578758 0.4458162  0.44591194 0.44591525 0.44581857
 0.44576448 0.44574496 0.44575745 0.445758   0.4456425  0.44549158
 0.44546112 0.44544974 0.44540563 0.44532055 0.44519427 0.44515407
 0.44521824 0.44511878 0.44508323 0.4451909  0.44516605 0.44505867
 0.4451588  0.44535545 0.44533446 0.44522497 0.44518444 0.44520304
 0.4451178  0.44496456 0.44494736 0.4449583  0.4448532  0.44472393
 0.44470724 0.44481236 0.44485426 0.44481677 0.44483435 0.44479933
 0.44470695 0.444685   0.44468907 0.4446675  0.4447457  0.44482592
 0.4447839  0.4447785  0.44487226 0.4449395  0.4448612  0.4447714
 0.44468978 0.44454947 0.444485   0.44454494 0.44451758 0.44442925
 0.44445297 0.44447467 0.4444241  0.44437662 0.44441202 0.44443288
 0.4442816  0.44406036 0.44404918 0.44419044 0.4442857  0.44435918
 0.44457248 0.44474438 0.44475704 0.44472888 0.44476223 0.4448304
 0.44480342 0.44470018 0.44462642 0.44457176 0.44444266 0.44429222
 0.44428715 0.44435546 0.44434485 0.44430727 0.44427952 0.4442445
 0.44430777 0.4443432  0.44435585 0.44446266 0.4444559  0.44431284
 0.44428334 0.44435027 0.4443823  0.44442764 0.44443476 0.44443765
 0.44438988 0.44424206 0.4441222  0.4439789  0.44382533 0.44382343
 0.4438602  0.4438292  0.44385952 0.44392905 0.4440097  0.44406605
 0.4440664  0.44397125 0.44392478 0.4440129  0.44397128 0.44366068
 0.44337013 0.44340122 0.44347274 0.44348615 0.44340977 0.44328856
 0.4431748  0.44304198 0.44282055 0.4425932  0.4424443  0.4423915
 0.44236475 0.4422782  0.44224384 0.44229057 0.44226652 0.44223893
 0.44234574 0.44232818 0.4422526  0.44233245 0.4423486  0.44219482
 0.44210687 0.44218287 0.4421943  0.4422531  0.4423693  0.44242224
 0.44225904 0.44199362 0.4419059  0.44186005 0.44159183 0.44137472
 0.44143757 0.44148818 0.44139084 0.4413124  0.44136655 0.4414166
 0.44139743 0.44129625 0.44126764 0.44136667 0.44144788 0.44140264
 0.44136634 0.44146836 0.44153085 0.44152853 0.44160053 0.44168928
 0.44158557 0.44138616 0.44123834 0.44114447 0.44112074 0.4411062
 0.4409081  0.44072226 0.44081914 0.4408646  0.44065017 0.440603
 0.44078967 0.4407331  0.44060844 0.44075155 0.44085306 0.44074374
 0.44073528 0.4408924  0.44093186 0.44095835 0.44104725 0.4410591
 0.4409259  0.44077456 0.44064814 0.44053584 0.44047573 0.44052818
 0.44061863 0.4405439  0.44045302 0.44045696 0.44040656 0.440318
 0.4403387  0.4402765  0.44018912 0.44025537 0.4403145  0.44028753
 0.4403698  0.44052416 0.44052947 0.44049606 0.4405331  0.44055414
 0.44049594 0.44042248 0.44033378 0.44019398 0.44011363 0.44010898
 0.44011614 0.44017196 0.44020247 0.44011065 0.44003966 0.44001737
 0.4399256  0.43980917 0.43982318 0.43990692 0.4399552  0.43999976
 0.4401566  0.44025737 0.44027933 0.4402895  0.44030792 0.4404468
 0.44053283 0.440413   0.4402991  0.44022846 0.44009224 0.4400447
 0.440112   0.44004676 0.43997532 0.44003707 0.44009906 0.4401365
 0.4402018  0.4401468  0.44013494 0.44028658 0.4403126  0.44024223
 0.44026065 0.44032755 0.44037116 0.44042927 0.4404148  0.44040716
 0.4404521  0.440395   0.44024134 0.44018427 0.44016474 0.4399962
 0.4399263  0.4401269  0.4401938  0.44005898 0.44013706 0.4402808
 0.44024283 0.44007742 0.43996137 0.43998083 0.4400395  0.43986952
 0.43953475 0.43948853 0.43956938 0.43959904 0.43955806 0.4395202
 0.43936396 0.43915436 0.43907118 0.4389998  0.43883774 0.43876567
 0.43875018 0.4386769  0.43866992 0.43871307 0.43868545 0.4386519
 0.43860126 0.43837273 0.4381637  0.43810698 0.43811622 0.43815115
 0.43812534 0.43807223 0.4380895  0.43822742 0.438217   0.43813843
 0.43804947 0.4378366  0.43769187 0.43770918 0.4375247  0.43724918
 0.43730354 0.43744868 0.43737566 0.4373395  0.43744224 0.43741438
 0.4372702  0.4371199  0.437062   0.4371186  0.43720075 0.43711555
 0.43702304 0.43717098 0.43728828 0.43723413 0.4371802  0.4371247
 0.43698657 0.43694338 0.43702623 0.4369589  0.4367886  0.43664566
 0.43647674 0.4363958  0.4364717  0.4364066  0.43622947 0.43614852
 0.43607116 0.43586558 0.4357758  0.43589678 0.43588585 0.43567333
 0.43565145 0.43579158 0.43581793 0.4358728  0.4359488  0.43585384
 0.43563876 0.43545082 0.4353777  0.4353413  0.43526328 0.43525413
 0.43522897 0.4351579  0.43526584 0.43535888 0.43530384 0.43526667
 0.43512666 0.4348228  0.43485603 0.43510672 0.43506554 0.4349454
 0.4351126  0.43530443 0.43526158 0.43522212 0.43519267 0.4350633
 0.43495762 0.43496644 0.43488273 0.4347594  0.43462047 0.43436518
 0.4342213  0.43432516 0.43442598 0.43452388 0.4346679  0.4346138
 0.43444982 0.43439287 0.4344234  0.43448326 0.43453985 0.43463442
 0.43484855 0.43495592 0.43496063 0.4349323  0.43488804 0.43494532
 0.43492094 0.43473864 0.43465313 0.43453    0.43435606 0.43437666
 0.4343321  0.43413246 0.43412405 0.43421805 0.43430096 0.43436527
 0.43432263 0.43424523 0.43430763 0.43439555 0.43435892 0.43427607
 0.43427235 0.43438497 0.43446547 0.43447646 0.4344612  0.43449026
 0.43439555 0.4340878  0.43397793 0.43407688 0.4338991  0.4336883
 0.43376434 0.43372303 0.43370655 0.43404514 0.4342052  0.434072
 0.43413243 0.4340338  0.43385282 0.4339801  0.43403822 0.43373463
 0.4334749  0.43361396 0.43364406 0.43344915 0.4332973  0.4332115
 0.43294457 0.43268782 0.4326306  0.43249923 0.43235523 0.43231514
 0.43212017 0.4319257  0.43200472 0.4322054  0.43225548 0.43212742
 0.43218097 0.4322089  0.4318674  0.431757   0.4319931  0.4318872
 0.4317521  0.43201452 0.43206936 0.431924   0.4318559  0.43184516
 0.4316292  0.4313084  0.43129757 0.43122092 0.43099216 0.4310283
 0.4308799  0.43080065 0.4311186  0.4310663  0.43106323 0.43109575
 0.43078598 0.43075606 0.4265632  0.4311515  0.42666376 0.42725685]
