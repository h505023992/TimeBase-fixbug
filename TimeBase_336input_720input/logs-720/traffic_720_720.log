Args in experiment:
Namespace(is_training=1, model_id='traffic_720_720', model='LightTimeBaseTST', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=720, period_len=24, basis_num=8, use_period_norm=1, use_orthogonal=1, orthogonal_weight=0.04, fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=7, individual=0, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='learned', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.03, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=3, use_multi_gpu=0, devices='0,1', test_flop=False)
Use GPU: cuda:3
518
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
MACs: 9930240.0
Params: 518.0
9.93M MACs
>>>>>>>start training : traffic_720_720_LightTimeBaseTST_custom_ftM_sl720_pl720_test_0_seed2023>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Max Memory (MB): 2636.54443359375
Epoch: 1 cost time: 42.438793897628784
Epoch: 1, Steps: 85 | Train Loss: 0.4444801 Vali Loss: 0.5736561 Test Loss: 0.6912374
Validation loss decreased (inf --> 0.573656).  Saving model ...
Updating learning rate to 0.03
Max Memory (MB): 2716.55810546875
Epoch: 2 cost time: 41.359583377838135
Epoch: 2, Steps: 85 | Train Loss: 0.3486978 Vali Loss: 0.4254428 Test Loss: 0.5054109
Validation loss decreased (0.573656 --> 0.425443).  Saving model ...
Updating learning rate to 0.03
Max Memory (MB): 2716.55810546875
Epoch: 3 cost time: 42.827200412750244
Epoch: 3, Steps: 85 | Train Loss: 0.2990581 Vali Loss: 0.4061073 Test Loss: 0.4788576
Validation loss decreased (0.425443 --> 0.406107).  Saving model ...
Updating learning rate to 0.03
Max Memory (MB): 2716.55810546875
Epoch: 4 cost time: 42.920302629470825
Epoch: 4, Steps: 85 | Train Loss: 0.2872565 Vali Loss: 0.4024290 Test Loss: 0.4678885
Validation loss decreased (0.406107 --> 0.402429).  Saving model ...
Updating learning rate to 0.024
Max Memory (MB): 2716.55810546875
Epoch: 5 cost time: 42.15138554573059
Epoch: 5, Steps: 85 | Train Loss: 0.2824366 Vali Loss: 0.3938177 Test Loss: 0.4648058
Validation loss decreased (0.402429 --> 0.393818).  Saving model ...
Updating learning rate to 0.019200000000000002
Max Memory (MB): 2716.55810546875
Epoch: 6 cost time: 40.55967307090759
Epoch: 6, Steps: 85 | Train Loss: 0.2811606 Vali Loss: 0.3980945 Test Loss: 0.4634546
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.015360000000000004
Max Memory (MB): 2716.55810546875
Epoch: 7 cost time: 40.98132276535034
Epoch: 7, Steps: 85 | Train Loss: 0.2801231 Vali Loss: 0.4012772 Test Loss: 0.4627298
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.012288000000000002
Max Memory (MB): 2716.55810546875
Epoch: 8 cost time: 40.05896306037903
Epoch: 8, Steps: 85 | Train Loss: 0.2800024 Vali Loss: 0.3934572 Test Loss: 0.4606870
Validation loss decreased (0.393818 --> 0.393457).  Saving model ...
Updating learning rate to 0.009830400000000001
Max Memory (MB): 2716.55810546875
Epoch: 9 cost time: 40.9260790348053
Epoch: 9, Steps: 85 | Train Loss: 0.2788959 Vali Loss: 0.3940369 Test Loss: 0.4612457
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.007864320000000003
Max Memory (MB): 2716.55810546875
Epoch: 10 cost time: 40.58332133293152
Epoch: 10, Steps: 85 | Train Loss: 0.2787909 Vali Loss: 0.3900576 Test Loss: 0.4584454
Validation loss decreased (0.393457 --> 0.390058).  Saving model ...
Updating learning rate to 0.006291456000000002
Max Memory (MB): 2716.55810546875
Epoch: 11 cost time: 40.70603823661804
Epoch: 11, Steps: 85 | Train Loss: 0.2779911 Vali Loss: 0.3921021 Test Loss: 0.4576376
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.005033164800000003
Max Memory (MB): 2716.55810546875
Epoch: 12 cost time: 40.441569328308105
Epoch: 12, Steps: 85 | Train Loss: 0.2777972 Vali Loss: 0.3892188 Test Loss: 0.4572904
Validation loss decreased (0.390058 --> 0.389219).  Saving model ...
Updating learning rate to 0.004026531840000002
Max Memory (MB): 2716.55810546875
Epoch: 13 cost time: 40.663238286972046
Epoch: 13, Steps: 85 | Train Loss: 0.2776860 Vali Loss: 0.3935786 Test Loss: 0.4578409
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0032212254720000015
Max Memory (MB): 2716.55810546875
Epoch: 14 cost time: 40.32572960853577
Epoch: 14, Steps: 85 | Train Loss: 0.2774965 Vali Loss: 0.3894132 Test Loss: 0.4557777
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0025769803776000016
Max Memory (MB): 2716.55810546875
Epoch: 15 cost time: 41.48070526123047
Epoch: 15, Steps: 85 | Train Loss: 0.2773791 Vali Loss: 0.3937853 Test Loss: 0.4566715
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.002061584302080001
Max Memory (MB): 2716.55810546875
Epoch: 16 cost time: 41.73129105567932
Epoch: 16, Steps: 85 | Train Loss: 0.2773453 Vali Loss: 0.3959120 Test Loss: 0.4571169
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.001649267441664001
Max Memory (MB): 2716.55810546875
Epoch: 17 cost time: 40.540693521499634
Epoch: 17, Steps: 85 | Train Loss: 0.2772253 Vali Loss: 0.3917455 Test Loss: 0.4561580
EarlyStopping counter: 5 out of 5
Early stopping
Final Max Memory (MB): 2716.55810546875
>>>>>>>testing : traffic_720_720_LightTimeBaseTST_custom_ftM_sl720_pl720_test_0_seed2023<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4567689895629883, mae:0.30132603645324707, rse:0.552648663520813
