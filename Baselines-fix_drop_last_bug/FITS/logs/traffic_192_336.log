Args in experiment:
Namespace(is_training=1, model_id='traffic_192_336', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=82, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_192_336_FITS_custom_ftM_sl192_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11753
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=82, out_features=225, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2035699200.0
params:  18675.0
Trainable parameters:  18675
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 38.25602674484253
Epoch: 1, Steps: 91 | Train Loss: 1.3789403 Vali Loss: 1.3583142 Test Loss: 1.6074958
Validation loss decreased (inf --> 1.358314).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 42.058908462524414
Epoch: 2, Steps: 91 | Train Loss: 0.9116714 Vali Loss: 1.0719262 Test Loss: 1.2640338
Validation loss decreased (1.358314 --> 1.071926).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 40.38319993019104
Epoch: 3, Steps: 91 | Train Loss: 0.7416111 Vali Loss: 0.9616566 Test Loss: 1.1323637
Validation loss decreased (1.071926 --> 0.961657).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 40.980881452560425
Epoch: 4, Steps: 91 | Train Loss: 0.6626504 Vali Loss: 0.8972929 Test Loss: 1.0560772
Validation loss decreased (0.961657 --> 0.897293).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 40.02687430381775
Epoch: 5, Steps: 91 | Train Loss: 0.6100882 Vali Loss: 0.8462088 Test Loss: 0.9960135
Validation loss decreased (0.897293 --> 0.846209).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 39.24213528633118
Epoch: 6, Steps: 91 | Train Loss: 0.5678236 Vali Loss: 0.8017897 Test Loss: 0.9442821
Validation loss decreased (0.846209 --> 0.801790).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 41.24936127662659
Epoch: 7, Steps: 91 | Train Loss: 0.5317294 Vali Loss: 0.7643257 Test Loss: 0.9005999
Validation loss decreased (0.801790 --> 0.764326).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 40.01145839691162
Epoch: 8, Steps: 91 | Train Loss: 0.5002228 Vali Loss: 0.7305799 Test Loss: 0.8609840
Validation loss decreased (0.764326 --> 0.730580).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 38.191635608673096
Epoch: 9, Steps: 91 | Train Loss: 0.4724681 Vali Loss: 0.7005238 Test Loss: 0.8259991
Validation loss decreased (0.730580 --> 0.700524).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 39.31184124946594
Epoch: 10, Steps: 91 | Train Loss: 0.4479872 Vali Loss: 0.6742714 Test Loss: 0.7953349
Validation loss decreased (0.700524 --> 0.674271).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 39.84249472618103
Epoch: 11, Steps: 91 | Train Loss: 0.4262300 Vali Loss: 0.6502752 Test Loss: 0.7677290
Validation loss decreased (0.674271 --> 0.650275).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 42.05034780502319
Epoch: 12, Steps: 91 | Train Loss: 0.4069252 Vali Loss: 0.6294996 Test Loss: 0.7436430
Validation loss decreased (0.650275 --> 0.629500).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 39.61454176902771
Epoch: 13, Steps: 91 | Train Loss: 0.3895691 Vali Loss: 0.6103256 Test Loss: 0.7214980
Validation loss decreased (0.629500 --> 0.610326).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 41.18177628517151
Epoch: 14, Steps: 91 | Train Loss: 0.3740197 Vali Loss: 0.5936373 Test Loss: 0.7023295
Validation loss decreased (0.610326 --> 0.593637).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 40.470746994018555
Epoch: 15, Steps: 91 | Train Loss: 0.3600275 Vali Loss: 0.5779332 Test Loss: 0.6848028
Validation loss decreased (0.593637 --> 0.577933).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 39.489105463027954
Epoch: 16, Steps: 91 | Train Loss: 0.3474118 Vali Loss: 0.5647086 Test Loss: 0.6690316
Validation loss decreased (0.577933 --> 0.564709).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 41.42269468307495
Epoch: 17, Steps: 91 | Train Loss: 0.3360821 Vali Loss: 0.5512177 Test Loss: 0.6543903
Validation loss decreased (0.564709 --> 0.551218).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 39.47152352333069
Epoch: 18, Steps: 91 | Train Loss: 0.3257148 Vali Loss: 0.5406336 Test Loss: 0.6418239
Validation loss decreased (0.551218 --> 0.540634).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 41.11481308937073
Epoch: 19, Steps: 91 | Train Loss: 0.3163422 Vali Loss: 0.5298645 Test Loss: 0.6297031
Validation loss decreased (0.540634 --> 0.529864).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 39.338704347610474
Epoch: 20, Steps: 91 | Train Loss: 0.3078259 Vali Loss: 0.5207126 Test Loss: 0.6192899
Validation loss decreased (0.529864 --> 0.520713).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 38.15219473838806
Epoch: 21, Steps: 91 | Train Loss: 0.3000765 Vali Loss: 0.5121917 Test Loss: 0.6098421
Validation loss decreased (0.520713 --> 0.512192).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 40.19586133956909
Epoch: 22, Steps: 91 | Train Loss: 0.2930412 Vali Loss: 0.5041175 Test Loss: 0.6009520
Validation loss decreased (0.512192 --> 0.504118).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 39.689767837524414
Epoch: 23, Steps: 91 | Train Loss: 0.2866285 Vali Loss: 0.4969582 Test Loss: 0.5926160
Validation loss decreased (0.504118 --> 0.496958).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 40.46819019317627
Epoch: 24, Steps: 91 | Train Loss: 0.2806728 Vali Loss: 0.4905330 Test Loss: 0.5856678
Validation loss decreased (0.496958 --> 0.490533).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 39.485718727111816
Epoch: 25, Steps: 91 | Train Loss: 0.2751894 Vali Loss: 0.4841204 Test Loss: 0.5786469
Validation loss decreased (0.490533 --> 0.484120).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 40.73029088973999
Epoch: 26, Steps: 91 | Train Loss: 0.2702050 Vali Loss: 0.4790288 Test Loss: 0.5728998
Validation loss decreased (0.484120 --> 0.479029).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 39.98393154144287
Epoch: 27, Steps: 91 | Train Loss: 0.2656116 Vali Loss: 0.4734172 Test Loss: 0.5669454
Validation loss decreased (0.479029 --> 0.473417).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 39.14747977256775
Epoch: 28, Steps: 91 | Train Loss: 0.2614017 Vali Loss: 0.4689370 Test Loss: 0.5617576
Validation loss decreased (0.473417 --> 0.468937).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 41.39093542098999
Epoch: 29, Steps: 91 | Train Loss: 0.2574767 Vali Loss: 0.4647327 Test Loss: 0.5571265
Validation loss decreased (0.468937 --> 0.464733).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 39.47436165809631
Epoch: 30, Steps: 91 | Train Loss: 0.2539777 Vali Loss: 0.4608601 Test Loss: 0.5526947
Validation loss decreased (0.464733 --> 0.460860).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11753
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=82, out_features=225, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2035699200.0
params:  18675.0
Trainable parameters:  18675
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 39.54318952560425
Epoch: 1, Steps: 91 | Train Loss: 0.3266350 Vali Loss: 0.4083806 Test Loss: 0.4965900
Validation loss decreased (inf --> 0.408381).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 40.43876814842224
Epoch: 2, Steps: 91 | Train Loss: 0.2997266 Vali Loss: 0.3898894 Test Loss: 0.4795612
Validation loss decreased (0.408381 --> 0.389889).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 38.969207763671875
Epoch: 3, Steps: 91 | Train Loss: 0.2908753 Vali Loss: 0.3842551 Test Loss: 0.4756051
Validation loss decreased (0.389889 --> 0.384255).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 41.20816349983215
Epoch: 4, Steps: 91 | Train Loss: 0.2884343 Vali Loss: 0.3831990 Test Loss: 0.4750313
Validation loss decreased (0.384255 --> 0.383199).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 40.365538120269775
Epoch: 5, Steps: 91 | Train Loss: 0.2879959 Vali Loss: 0.3825023 Test Loss: 0.4749986
Validation loss decreased (0.383199 --> 0.382502).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 40.58322882652283
Epoch: 6, Steps: 91 | Train Loss: 0.2878244 Vali Loss: 0.3826402 Test Loss: 0.4752769
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 41.95092701911926
Epoch: 7, Steps: 91 | Train Loss: 0.2877389 Vali Loss: 0.3824075 Test Loss: 0.4752509
Validation loss decreased (0.382502 --> 0.382408).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 36.35109233856201
Epoch: 8, Steps: 91 | Train Loss: 0.2878616 Vali Loss: 0.3823121 Test Loss: 0.4750787
Validation loss decreased (0.382408 --> 0.382312).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 39.82589316368103
Epoch: 9, Steps: 91 | Train Loss: 0.2877611 Vali Loss: 0.3822932 Test Loss: 0.4752086
Validation loss decreased (0.382312 --> 0.382293).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 39.88454294204712
Epoch: 10, Steps: 91 | Train Loss: 0.2877839 Vali Loss: 0.3823107 Test Loss: 0.4750261
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 39.32931637763977
Epoch: 11, Steps: 91 | Train Loss: 0.2877134 Vali Loss: 0.3822469 Test Loss: 0.4750697
Validation loss decreased (0.382293 --> 0.382247).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 39.71660494804382
Epoch: 12, Steps: 91 | Train Loss: 0.2877305 Vali Loss: 0.3823206 Test Loss: 0.4751589
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 38.68168020248413
Epoch: 13, Steps: 91 | Train Loss: 0.2877076 Vali Loss: 0.3822707 Test Loss: 0.4750932
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 41.66396641731262
Epoch: 14, Steps: 91 | Train Loss: 0.2876740 Vali Loss: 0.3823047 Test Loss: 0.4750834
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 38.23981022834778
Epoch: 15, Steps: 91 | Train Loss: 0.2876446 Vali Loss: 0.3822753 Test Loss: 0.4749508
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 40.396769762039185
Epoch: 16, Steps: 91 | Train Loss: 0.2876699 Vali Loss: 0.3824232 Test Loss: 0.4751175
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_192_336_FITS_custom_ftM_sl192_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4730479419231415, mae:0.3067340850830078, rse:0.5652647018432617, corr:[0.26906896 0.28279772 0.2817889  0.28132552 0.2813763  0.2805383
 0.28126955 0.28078645 0.2810927  0.2809744  0.2807827  0.28164348
 0.28109556 0.28086668 0.28054842 0.28027365 0.28080198 0.28009522
 0.27988678 0.28001612 0.2796387  0.27983302 0.27962664 0.28011918
 0.28154093 0.28172356 0.28216183 0.28194937 0.28172383 0.28135788
 0.28069073 0.2809422  0.28056046 0.28036582 0.28087664 0.2805575
 0.28076234 0.2808805  0.2804849  0.2805554  0.28045574 0.2806514
 0.28058317 0.28020704 0.2805626  0.28060746 0.28054467 0.28058445
 0.2808691  0.28104806 0.2809986  0.2811566  0.28131574 0.28066683
 0.28052515 0.2807111  0.28042117 0.2804443  0.28051287 0.2805983
 0.28060582 0.2803955  0.28053015 0.28027207 0.28008997 0.28005725
 0.279775   0.27995503 0.2800317  0.2800002  0.28024614 0.28004536
 0.27996066 0.27997312 0.27989304 0.27999797 0.27987248 0.27982193
 0.2798751  0.27984065 0.2797708  0.27949867 0.27974322 0.2800927
 0.27975318 0.27967513 0.27982295 0.27991173 0.2801933  0.2799045
 0.27965087 0.2796571  0.27953005 0.279797   0.2797636  0.27959964
 0.27964965 0.27943033 0.2792511  0.27933833 0.27969012 0.28021988
 0.2799894  0.27987102 0.27973822 0.27915314 0.27919406 0.27921236
 0.27931195 0.27942705 0.27891657 0.27909276 0.27936858 0.27921057
 0.27949136 0.27940843 0.27935815 0.2795188  0.27934828 0.27932537
 0.2790573  0.2794696  0.28017056 0.28011218 0.2800258  0.28010148
 0.28030542 0.2803741  0.27977982 0.27970794 0.27973703 0.27935246
 0.2795326  0.2792963  0.278908   0.2793675  0.27942705 0.27945036
 0.27966535 0.27957866 0.2797115  0.2797675  0.280272   0.28150544
 0.2818794  0.2822313  0.2825769  0.28238958 0.28238255 0.2823293
 0.2822815  0.28232804 0.28219664 0.28213033 0.28192043 0.28205067
 0.28216454 0.281828   0.2817871  0.28172272 0.2817354  0.2820565
 0.28193042 0.28160024 0.28156924 0.2813485  0.28103474 0.2819748
 0.28353518 0.28280103 0.28246096 0.28261706 0.28205717 0.28203717
 0.28221563 0.28181332 0.28190094 0.28164345 0.28119153 0.28170645
 0.28178647 0.28151307 0.28124094 0.2809775  0.28137866 0.2810565
 0.28053576 0.2808105  0.2808422  0.28111118 0.28117627 0.28106895
 0.28189772 0.28180176 0.2819037  0.282032   0.28168014 0.28163242
 0.28135386 0.28094062 0.28059486 0.28001785 0.27999032 0.28012475
 0.27990556 0.27985913 0.27963838 0.27937645 0.2793779  0.2796152
 0.2799599  0.27971452 0.27965754 0.28000286 0.2799004  0.2801726
 0.28066996 0.28042302 0.2805264  0.28028283 0.28021842 0.28027487
 0.27994975 0.27991703 0.2796995  0.27976042 0.2800268  0.27954027
 0.27938887 0.27942914 0.2793773  0.2796211  0.27959836 0.27965724
 0.27963883 0.27964163 0.28005356 0.27984345 0.2800176  0.28063294
 0.28039002 0.28006667 0.27994525 0.2799466  0.2800465  0.2796312
 0.27960163 0.27957356 0.27905488 0.2789812  0.2787147  0.2786517
 0.27903908 0.27873755 0.27883595 0.27913457 0.27915376 0.27934867
 0.27876452 0.27847943 0.27880815 0.27901232 0.2795069  0.27941293
 0.2791193  0.2793854  0.2791423  0.27932337 0.2793468  0.27919087
 0.27909592 0.2785174  0.27873388 0.27868125 0.27791885 0.27831173
 0.27873018 0.27861047 0.2787504  0.27867207 0.2785702  0.27846068
 0.27851912 0.27875745 0.27845213 0.2784762  0.27857754 0.27840784
 0.2787842  0.27960312 0.2801955  0.28027126 0.2798277  0.27974716
 0.2792092  0.27877328 0.27886027 0.2787036  0.27893817 0.2789466
 0.2790428  0.2792457  0.27885547 0.27911294 0.27906862 0.27887657
 0.27929094 0.27877575 0.2788572  0.2794312  0.27957052 0.28075147
 0.2810122  0.2810593  0.2815829  0.28122407 0.28127435 0.28101045
 0.2805738  0.28082848 0.28048784 0.2808679  0.28095388 0.2808582
 0.28120843 0.2807503  0.28126803 0.2811371  0.28148505 0.28208032
 0.28060767 0.2814674  0.27986994 0.28009218 0.28059545 0.28217408]
