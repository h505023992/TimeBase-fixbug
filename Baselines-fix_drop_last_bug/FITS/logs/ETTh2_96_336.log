Args in experiment:
Namespace(is_training=1, model_id='ETTh2_96_336', model='FITS', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=40, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_96_336_FITS_ETTh2_ftM_sl96_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8209
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=40, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6451200.0
params:  7380.0
Trainable parameters:  7380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.672166109085083
Epoch: 1, Steps: 64 | Train Loss: 0.8608524 Vali Loss: 0.5145561 Test Loss: 0.5998949
Validation loss decreased (inf --> 0.514556).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.1472740173339844
Epoch: 2, Steps: 64 | Train Loss: 0.7196722 Vali Loss: 0.4665380 Test Loss: 0.5386933
Validation loss decreased (0.514556 --> 0.466538).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.701219081878662
Epoch: 3, Steps: 64 | Train Loss: 0.6462039 Vali Loss: 0.4382092 Test Loss: 0.5032058
Validation loss decreased (0.466538 --> 0.438209).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.6794135570526123
Epoch: 4, Steps: 64 | Train Loss: 0.6033248 Vali Loss: 0.4204177 Test Loss: 0.4817553
Validation loss decreased (0.438209 --> 0.420418).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.6271989345550537
Epoch: 5, Steps: 64 | Train Loss: 0.5787477 Vali Loss: 0.4102282 Test Loss: 0.4681493
Validation loss decreased (0.420418 --> 0.410228).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.7322349548339844
Epoch: 6, Steps: 64 | Train Loss: 0.5630604 Vali Loss: 0.4016292 Test Loss: 0.4594907
Validation loss decreased (0.410228 --> 0.401629).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.658736228942871
Epoch: 7, Steps: 64 | Train Loss: 0.5518731 Vali Loss: 0.3952804 Test Loss: 0.4537353
Validation loss decreased (0.401629 --> 0.395280).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.7634711265563965
Epoch: 8, Steps: 64 | Train Loss: 0.5441821 Vali Loss: 0.3933726 Test Loss: 0.4496178
Validation loss decreased (0.395280 --> 0.393373).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.9536120891571045
Epoch: 9, Steps: 64 | Train Loss: 0.5395646 Vali Loss: 0.3924028 Test Loss: 0.4466089
Validation loss decreased (0.393373 --> 0.392403).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.215997695922852
Epoch: 10, Steps: 64 | Train Loss: 0.5353446 Vali Loss: 0.3891618 Test Loss: 0.4441822
Validation loss decreased (0.392403 --> 0.389162).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.6640446186065674
Epoch: 11, Steps: 64 | Train Loss: 0.5323630 Vali Loss: 0.3867252 Test Loss: 0.4421816
Validation loss decreased (0.389162 --> 0.386725).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.6658737659454346
Epoch: 12, Steps: 64 | Train Loss: 0.5306700 Vali Loss: 0.3859648 Test Loss: 0.4406217
Validation loss decreased (0.386725 --> 0.385965).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.2553820610046387
Epoch: 13, Steps: 64 | Train Loss: 0.5277179 Vali Loss: 0.3870994 Test Loss: 0.4392536
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.9132721424102783
Epoch: 14, Steps: 64 | Train Loss: 0.5267122 Vali Loss: 0.3819196 Test Loss: 0.4380506
Validation loss decreased (0.385965 --> 0.381920).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.6617183685302734
Epoch: 15, Steps: 64 | Train Loss: 0.5255059 Vali Loss: 0.3807328 Test Loss: 0.4370505
Validation loss decreased (0.381920 --> 0.380733).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9562289714813232
Epoch: 16, Steps: 64 | Train Loss: 0.5236818 Vali Loss: 0.3809528 Test Loss: 0.4361202
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.4969067573547363
Epoch: 17, Steps: 64 | Train Loss: 0.5231477 Vali Loss: 0.3760380 Test Loss: 0.4352456
Validation loss decreased (0.380733 --> 0.376038).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.491482734680176
Epoch: 18, Steps: 64 | Train Loss: 0.5220388 Vali Loss: 0.3744918 Test Loss: 0.4345215
Validation loss decreased (0.376038 --> 0.374492).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.7648792266845703
Epoch: 19, Steps: 64 | Train Loss: 0.5210086 Vali Loss: 0.3792706 Test Loss: 0.4338354
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.5406551361083984
Epoch: 20, Steps: 64 | Train Loss: 0.5192963 Vali Loss: 0.3789585 Test Loss: 0.4331602
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.6326334476470947
Epoch: 21, Steps: 64 | Train Loss: 0.5186538 Vali Loss: 0.3785743 Test Loss: 0.4325987
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.65153169631958
Epoch: 22, Steps: 64 | Train Loss: 0.5169581 Vali Loss: 0.3766727 Test Loss: 0.4320534
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.634312152862549
Epoch: 23, Steps: 64 | Train Loss: 0.5178560 Vali Loss: 0.3746304 Test Loss: 0.4316116
EarlyStopping counter: 5 out of 5
Early stopping
train 8209
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=40, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6451200.0
params:  7380.0
Trainable parameters:  7380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.634902238845825
Epoch: 1, Steps: 64 | Train Loss: 0.6606422 Vali Loss: 0.3728193 Test Loss: 0.4297282
Validation loss decreased (inf --> 0.372819).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.480531930923462
Epoch: 2, Steps: 64 | Train Loss: 0.6564875 Vali Loss: 0.3700863 Test Loss: 0.4265568
Validation loss decreased (0.372819 --> 0.370086).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.3257224559783936
Epoch: 3, Steps: 64 | Train Loss: 0.6527177 Vali Loss: 0.3685987 Test Loss: 0.4244992
Validation loss decreased (0.370086 --> 0.368599).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.232592821121216
Epoch: 4, Steps: 64 | Train Loss: 0.6491006 Vali Loss: 0.3674132 Test Loss: 0.4230309
Validation loss decreased (0.368599 --> 0.367413).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.327037811279297
Epoch: 5, Steps: 64 | Train Loss: 0.6495116 Vali Loss: 0.3682024 Test Loss: 0.4219728
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.917874574661255
Epoch: 6, Steps: 64 | Train Loss: 0.6480196 Vali Loss: 0.3655913 Test Loss: 0.4213172
Validation loss decreased (0.367413 --> 0.365591).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.2890522480010986
Epoch: 7, Steps: 64 | Train Loss: 0.6475486 Vali Loss: 0.3642919 Test Loss: 0.4207862
Validation loss decreased (0.365591 --> 0.364292).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.2116732597351074
Epoch: 8, Steps: 64 | Train Loss: 0.6469406 Vali Loss: 0.3628389 Test Loss: 0.4203387
Validation loss decreased (0.364292 --> 0.362839).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.371976852416992
Epoch: 9, Steps: 64 | Train Loss: 0.6465163 Vali Loss: 0.3625076 Test Loss: 0.4200498
Validation loss decreased (0.362839 --> 0.362508).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.226506233215332
Epoch: 10, Steps: 64 | Train Loss: 0.6457395 Vali Loss: 0.3666041 Test Loss: 0.4199169
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.2130630016326904
Epoch: 11, Steps: 64 | Train Loss: 0.6445992 Vali Loss: 0.3636869 Test Loss: 0.4196708
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.295527458190918
Epoch: 12, Steps: 64 | Train Loss: 0.6453067 Vali Loss: 0.3648235 Test Loss: 0.4196268
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.0102670192718506
Epoch: 13, Steps: 64 | Train Loss: 0.6438716 Vali Loss: 0.3628556 Test Loss: 0.4193960
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.003437519073486
Epoch: 14, Steps: 64 | Train Loss: 0.6447507 Vali Loss: 0.3644174 Test Loss: 0.4193912
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh2_96_336_FITS_ETTh2_ftM_sl96_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.41532525420188904, mae:0.42504897713661194, rse:0.5152686238288879, corr:[0.26554927 0.26365566 0.26415128 0.26296452 0.26137727 0.2606063
 0.25864545 0.25849435 0.25676072 0.25586018 0.25434032 0.25276306
 0.2512565  0.24966608 0.24844506 0.2474527  0.24752808 0.24668457
 0.24557054 0.244402   0.24332096 0.24232619 0.2410297  0.239847
 0.23742303 0.2358855  0.23459445 0.23329525 0.23218237 0.23063019
 0.22966455 0.22854511 0.22701237 0.22536097 0.22453746 0.22383043
 0.22239716 0.22147958 0.22036302 0.21931733 0.21873194 0.2182108
 0.21765216 0.21676683 0.2157015  0.21404256 0.21305898 0.21094282
 0.20732976 0.20481506 0.20319234 0.20137557 0.19948028 0.19792913
 0.1955064  0.19348712 0.19178455 0.18998033 0.18926668 0.18799286
 0.18667181 0.18569143 0.18523906 0.18518741 0.18519776 0.18475784
 0.18338159 0.1824655  0.18206398 0.18119173 0.18041497 0.17889495
 0.17670086 0.17558278 0.17493841 0.17351137 0.17187534 0.1713148
 0.1704611  0.16964725 0.16857156 0.16761915 0.16785574 0.16754113
 0.16675602 0.16579679 0.16624302 0.16655141 0.16577064 0.16568449
 0.1649416  0.16433783 0.16478406 0.16414022 0.16328454 0.1621974
 0.16079421 0.15909208 0.15809545 0.15747009 0.15587452 0.15478276
 0.15398026 0.15323637 0.15289238 0.15222774 0.15218353 0.15177937
 0.15208094 0.15138108 0.14955732 0.1493927  0.14840008 0.14809233
 0.14792529 0.14687882 0.14672662 0.1462     0.14534485 0.14343861
 0.14104293 0.13998719 0.13861237 0.1373259  0.13597502 0.13428389
 0.13347687 0.13235298 0.13122179 0.13117544 0.13096803 0.12976913
 0.12899484 0.12865376 0.12820746 0.1275627  0.12687927 0.12639628
 0.12550546 0.12501661 0.12479684 0.12386632 0.12327001 0.12151155
 0.11834869 0.11647903 0.11501671 0.11354705 0.11256254 0.11122525
 0.1102936  0.10967569 0.10831956 0.10800018 0.10845602 0.10713992
 0.10654794 0.1066317  0.10620117 0.10643201 0.10645403 0.1057727
 0.10585912 0.10578585 0.10489911 0.1049445  0.10521516 0.10345803
 0.10125163 0.10054215 0.09966821 0.09852441 0.09763209 0.09662049
 0.0958553  0.09482581 0.09418087 0.09385238 0.09367911 0.09375512
 0.09340958 0.09300458 0.09280703 0.09328002 0.09405716 0.09424806
 0.0941337  0.09493966 0.095879   0.09548628 0.09530167 0.09516798
 0.09464334 0.09472251 0.09442811 0.09389145 0.09336036 0.09314588
 0.09382082 0.09326094 0.0924412  0.09248935 0.09275247 0.09291593
 0.09222096 0.09201279 0.09202029 0.09180604 0.0922619  0.09227946
 0.09228271 0.0924292  0.09250237 0.09256768 0.09214696 0.09091478
 0.0892505  0.08878619 0.08877714 0.08728853 0.08621432 0.08641675
 0.08662466 0.08654351 0.08616326 0.08564477 0.08574041 0.08584394
 0.0848812  0.08430365 0.08443902 0.08449164 0.08550725 0.08600964
 0.08600867 0.08660311 0.08702265 0.08742159 0.0875889  0.08727918
 0.08603773 0.08586088 0.08571725 0.08446686 0.08482186 0.0841233
 0.0830104  0.08425605 0.08451967 0.08418765 0.08515157 0.08536103
 0.08537623 0.08617567 0.08611292 0.08686564 0.08788279 0.0885151
 0.08932124 0.08986459 0.09012195 0.09076671 0.09106775 0.09094471
 0.09110182 0.09132756 0.09114271 0.09081616 0.09070507 0.09022203
 0.08992707 0.09027455 0.09069304 0.09075823 0.09125391 0.09125809
 0.0909783  0.09101052 0.09122893 0.09179117 0.092306   0.09220126
 0.09260379 0.09287852 0.09278619 0.09312598 0.09334273 0.09349815
 0.09374333 0.09348706 0.09290417 0.09216956 0.09184071 0.09155944
 0.09065146 0.09018248 0.09053216 0.09086152 0.09130918 0.09166399
 0.09178166 0.09193514 0.09268547 0.09359039 0.09422085 0.09520838
 0.09596713 0.09637284 0.09723058 0.09795662 0.09780701 0.09711504
 0.09670373 0.0971918  0.09719995 0.0970162  0.09620146 0.09576723
 0.09653865 0.0965911  0.09720653 0.09722769 0.09778633 0.09862845
 0.09867706 0.09965587 0.09855545 0.09891004 0.09970313 0.09882159
 0.09886072 0.09708542 0.09778176 0.09827726 0.09724053 0.10203553]
