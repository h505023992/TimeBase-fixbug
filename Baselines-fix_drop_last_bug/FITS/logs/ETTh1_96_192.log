Args in experiment:
Namespace(is_training=1, model_id='ETTh1_96_192', model='FITS', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=192, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=40, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_96_192_FITS_ETTh1_ftM_sl96_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8353
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=40, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4300800.0
params:  4920.0
Trainable parameters:  4920
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.004941701889038
Epoch: 1, Steps: 65 | Train Loss: 0.8009610 Vali Loss: 1.7485359 Test Loss: 1.0247391
Validation loss decreased (inf --> 1.748536).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.853846311569214
Epoch: 2, Steps: 65 | Train Loss: 0.6342045 Vali Loss: 1.5413377 Test Loss: 0.8504986
Validation loss decreased (1.748536 --> 1.541338).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.9881362915039062
Epoch: 3, Steps: 65 | Train Loss: 0.5382013 Vali Loss: 1.4248663 Test Loss: 0.7525553
Validation loss decreased (1.541338 --> 1.424866).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.330113410949707
Epoch: 4, Steps: 65 | Train Loss: 0.4829128 Vali Loss: 1.3551104 Test Loss: 0.6945933
Validation loss decreased (1.424866 --> 1.355110).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2146623134613037
Epoch: 5, Steps: 65 | Train Loss: 0.4487507 Vali Loss: 1.3105781 Test Loss: 0.6586028
Validation loss decreased (1.355110 --> 1.310578).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.1508941650390625
Epoch: 6, Steps: 65 | Train Loss: 0.4266764 Vali Loss: 1.2804389 Test Loss: 0.6350369
Validation loss decreased (1.310578 --> 1.280439).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3404452800750732
Epoch: 7, Steps: 65 | Train Loss: 0.4110902 Vali Loss: 1.2574695 Test Loss: 0.6169880
Validation loss decreased (1.280439 --> 1.257470).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.3556675910949707
Epoch: 8, Steps: 65 | Train Loss: 0.3992044 Vali Loss: 1.2396085 Test Loss: 0.6032333
Validation loss decreased (1.257470 --> 1.239609).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.3498244285583496
Epoch: 9, Steps: 65 | Train Loss: 0.3903569 Vali Loss: 1.2234437 Test Loss: 0.5911478
Validation loss decreased (1.239609 --> 1.223444).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.3436200618743896
Epoch: 10, Steps: 65 | Train Loss: 0.3824730 Vali Loss: 1.2098328 Test Loss: 0.5811278
Validation loss decreased (1.223444 --> 1.209833).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.846038579940796
Epoch: 11, Steps: 65 | Train Loss: 0.3758544 Vali Loss: 1.1976106 Test Loss: 0.5716835
Validation loss decreased (1.209833 --> 1.197611).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.902249574661255
Epoch: 12, Steps: 65 | Train Loss: 0.3703517 Vali Loss: 1.1867305 Test Loss: 0.5634533
Validation loss decreased (1.197611 --> 1.186731).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.613132953643799
Epoch: 13, Steps: 65 | Train Loss: 0.3653033 Vali Loss: 1.1775385 Test Loss: 0.5559677
Validation loss decreased (1.186731 --> 1.177539).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.206028938293457
Epoch: 14, Steps: 65 | Train Loss: 0.3606805 Vali Loss: 1.1678522 Test Loss: 0.5495224
Validation loss decreased (1.177539 --> 1.167852).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.1882810592651367
Epoch: 15, Steps: 65 | Train Loss: 0.3564820 Vali Loss: 1.1604122 Test Loss: 0.5430729
Validation loss decreased (1.167852 --> 1.160412).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.6045498847961426
Epoch: 16, Steps: 65 | Train Loss: 0.3529479 Vali Loss: 1.1522299 Test Loss: 0.5376388
Validation loss decreased (1.160412 --> 1.152230).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.425720453262329
Epoch: 17, Steps: 65 | Train Loss: 0.3497522 Vali Loss: 1.1461051 Test Loss: 0.5321583
Validation loss decreased (1.152230 --> 1.146105).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.4930906295776367
Epoch: 18, Steps: 65 | Train Loss: 0.3466734 Vali Loss: 1.1400717 Test Loss: 0.5275309
Validation loss decreased (1.146105 --> 1.140072).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.4088292121887207
Epoch: 19, Steps: 65 | Train Loss: 0.3436532 Vali Loss: 1.1339575 Test Loss: 0.5229702
Validation loss decreased (1.140072 --> 1.133958).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.0851359367370605
Epoch: 20, Steps: 65 | Train Loss: 0.3409804 Vali Loss: 1.1275690 Test Loss: 0.5187994
Validation loss decreased (1.133958 --> 1.127569).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.967332124710083
Epoch: 21, Steps: 65 | Train Loss: 0.3385393 Vali Loss: 1.1237103 Test Loss: 0.5150805
Validation loss decreased (1.127569 --> 1.123710).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.237252473831177
Epoch: 22, Steps: 65 | Train Loss: 0.3364829 Vali Loss: 1.1183404 Test Loss: 0.5115309
Validation loss decreased (1.123710 --> 1.118340).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.288827419281006
Epoch: 23, Steps: 65 | Train Loss: 0.3343578 Vali Loss: 1.1139323 Test Loss: 0.5082724
Validation loss decreased (1.118340 --> 1.113932).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.175292730331421
Epoch: 24, Steps: 65 | Train Loss: 0.3326692 Vali Loss: 1.1107954 Test Loss: 0.5053933
Validation loss decreased (1.113932 --> 1.110795).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.033126592636108
Epoch: 25, Steps: 65 | Train Loss: 0.3307315 Vali Loss: 1.1069586 Test Loss: 0.5024089
Validation loss decreased (1.110795 --> 1.106959).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.3725945949554443
Epoch: 26, Steps: 65 | Train Loss: 0.3294140 Vali Loss: 1.1034416 Test Loss: 0.4999171
Validation loss decreased (1.106959 --> 1.103442).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.3372719287872314
Epoch: 27, Steps: 65 | Train Loss: 0.3278418 Vali Loss: 1.1001478 Test Loss: 0.4973591
Validation loss decreased (1.103442 --> 1.100148).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.394890069961548
Epoch: 28, Steps: 65 | Train Loss: 0.3261383 Vali Loss: 1.0970927 Test Loss: 0.4951376
Validation loss decreased (1.100148 --> 1.097093).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.89445424079895
Epoch: 29, Steps: 65 | Train Loss: 0.3251186 Vali Loss: 1.0941683 Test Loss: 0.4931473
Validation loss decreased (1.097093 --> 1.094168).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.244981050491333
Epoch: 30, Steps: 65 | Train Loss: 0.3237988 Vali Loss: 1.0915076 Test Loss: 0.4910799
Validation loss decreased (1.094168 --> 1.091508).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 8353
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=40, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4300800.0
params:  4920.0
Trainable parameters:  4920
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.263043642044067
Epoch: 1, Steps: 65 | Train Loss: 0.4507566 Vali Loss: 1.0535190 Test Loss: 0.4612567
Validation loss decreased (inf --> 1.053519).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.1529901027679443
Epoch: 2, Steps: 65 | Train Loss: 0.4375860 Vali Loss: 1.0315371 Test Loss: 0.4465597
Validation loss decreased (1.053519 --> 1.031537).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.010270118713379
Epoch: 3, Steps: 65 | Train Loss: 0.4299052 Vali Loss: 1.0189871 Test Loss: 0.4388384
Validation loss decreased (1.031537 --> 1.018987).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.376195430755615
Epoch: 4, Steps: 65 | Train Loss: 0.4254276 Vali Loss: 1.0109116 Test Loss: 0.4348561
Validation loss decreased (1.018987 --> 1.010912).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.41336727142334
Epoch: 5, Steps: 65 | Train Loss: 0.4225074 Vali Loss: 1.0066507 Test Loss: 0.4332668
Validation loss decreased (1.010912 --> 1.006651).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2490077018737793
Epoch: 6, Steps: 65 | Train Loss: 0.4211780 Vali Loss: 1.0040472 Test Loss: 0.4329343
Validation loss decreased (1.006651 --> 1.004047).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.4319212436676025
Epoch: 7, Steps: 65 | Train Loss: 0.4202012 Vali Loss: 1.0017011 Test Loss: 0.4331236
Validation loss decreased (1.004047 --> 1.001701).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.9721720218658447
Epoch: 8, Steps: 65 | Train Loss: 0.4196311 Vali Loss: 1.0011171 Test Loss: 0.4334695
Validation loss decreased (1.001701 --> 1.001117).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.3518142700195312
Epoch: 9, Steps: 65 | Train Loss: 0.4191156 Vali Loss: 1.0005368 Test Loss: 0.4339181
Validation loss decreased (1.001117 --> 1.000537).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.090703248977661
Epoch: 10, Steps: 65 | Train Loss: 0.4189357 Vali Loss: 1.0000727 Test Loss: 0.4343951
Validation loss decreased (1.000537 --> 1.000073).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.122619152069092
Epoch: 11, Steps: 65 | Train Loss: 0.4187527 Vali Loss: 0.9997470 Test Loss: 0.4346963
Validation loss decreased (1.000073 --> 0.999747).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.034511089324951
Epoch: 12, Steps: 65 | Train Loss: 0.4187731 Vali Loss: 0.9997304 Test Loss: 0.4350924
Validation loss decreased (0.999747 --> 0.999730).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.091224908828735
Epoch: 13, Steps: 65 | Train Loss: 0.4186688 Vali Loss: 0.9999152 Test Loss: 0.4350601
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.4296655654907227
Epoch: 14, Steps: 65 | Train Loss: 0.4188783 Vali Loss: 0.9993428 Test Loss: 0.4352376
Validation loss decreased (0.999730 --> 0.999343).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.5962486267089844
Epoch: 15, Steps: 65 | Train Loss: 0.4187276 Vali Loss: 0.9992326 Test Loss: 0.4354843
Validation loss decreased (0.999343 --> 0.999233).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.7161495685577393
Epoch: 16, Steps: 65 | Train Loss: 0.4186940 Vali Loss: 0.9995826 Test Loss: 0.4356081
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.166592836380005
Epoch: 17, Steps: 65 | Train Loss: 0.4186770 Vali Loss: 0.9993404 Test Loss: 0.4355662
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.530593156814575
Epoch: 18, Steps: 65 | Train Loss: 0.4189026 Vali Loss: 0.9990673 Test Loss: 0.4357893
Validation loss decreased (0.999233 --> 0.999067).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.0963997840881348
Epoch: 19, Steps: 65 | Train Loss: 0.4187633 Vali Loss: 0.9988101 Test Loss: 0.4358229
Validation loss decreased (0.999067 --> 0.998810).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.256143093109131
Epoch: 20, Steps: 65 | Train Loss: 0.4187089 Vali Loss: 0.9986693 Test Loss: 0.4357951
Validation loss decreased (0.998810 --> 0.998669).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.2346417903900146
Epoch: 21, Steps: 65 | Train Loss: 0.4184034 Vali Loss: 0.9988163 Test Loss: 0.4357663
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.417477607727051
Epoch: 22, Steps: 65 | Train Loss: 0.4184642 Vali Loss: 0.9992305 Test Loss: 0.4357940
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.576423406600952
Epoch: 23, Steps: 65 | Train Loss: 0.4184294 Vali Loss: 0.9986590 Test Loss: 0.4358283
Validation loss decreased (0.998669 --> 0.998659).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.484511613845825
Epoch: 24, Steps: 65 | Train Loss: 0.4189356 Vali Loss: 0.9989652 Test Loss: 0.4358126
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.3933093547821045
Epoch: 25, Steps: 65 | Train Loss: 0.4185698 Vali Loss: 0.9989994 Test Loss: 0.4358402
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.997584581375122
Epoch: 26, Steps: 65 | Train Loss: 0.4186117 Vali Loss: 0.9989761 Test Loss: 0.4358940
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.813906669616699
Epoch: 27, Steps: 65 | Train Loss: 0.4185416 Vali Loss: 0.9988496 Test Loss: 0.4358146
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.213740348815918
Epoch: 28, Steps: 65 | Train Loss: 0.4187735 Vali Loss: 0.9991466 Test Loss: 0.4358850
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_96_192_FITS_ETTh1_ftM_sl96_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.43556514382362366, mae:0.42196962237358093, rse:0.6267349123954773, corr:[0.2642692  0.26521075 0.26618138 0.26543292 0.26343784 0.26112774
 0.2584058  0.25856158 0.25813213 0.2582989  0.25829342 0.2579892
 0.25834808 0.25791097 0.25777218 0.25762877 0.25730377 0.2573242
 0.25733137 0.2573232  0.25712794 0.25727293 0.25741822 0.25801316
 0.25768206 0.25710064 0.25669336 0.2563178  0.25569746 0.25493667
 0.25440893 0.2537399  0.25353414 0.253353   0.2527978  0.25337237
 0.25357768 0.2532408  0.25371033 0.25353655 0.25361556 0.2542982
 0.254486   0.25473362 0.25488845 0.25483158 0.25516668 0.25596282
 0.2559199  0.25492373 0.25379068 0.2524035  0.2511506  0.24970269
 0.24835417 0.24760267 0.24719301 0.24718007 0.24689941 0.2470414
 0.2473056  0.24731883 0.24753362 0.24718286 0.24683103 0.24731323
 0.24762516 0.24777278 0.2479874  0.24788801 0.24793728 0.24772042
 0.24629337 0.2451537  0.24422339 0.24340753 0.243127   0.24268362
 0.2422295  0.2416876  0.24141239 0.24113648 0.24076505 0.24089952
 0.24085027 0.24085933 0.24140872 0.24135815 0.24117452 0.24139908
 0.2413598  0.24141061 0.24136539 0.24132018 0.24161544 0.24200796
 0.24172403 0.24128407 0.24073529 0.23976071 0.23926622 0.23886669
 0.23829055 0.2378482  0.2378992  0.23762454 0.23729992 0.23718613
 0.23725599 0.23720592 0.23758203 0.2377958  0.23802151 0.2385349
 0.23866487 0.23891458 0.23899353 0.23879436 0.23895203 0.2387277
 0.23776892 0.23694915 0.23605101 0.23468585 0.23372538 0.23300786
 0.23236084 0.23213595 0.23225683 0.23203859 0.23150922 0.23147321
 0.23153238 0.23127672 0.231536   0.23139612 0.231389   0.231855
 0.23195419 0.2321824  0.23240311 0.23240706 0.23272991 0.23240365
 0.23137566 0.23080093 0.22987704 0.22849318 0.22756137 0.22659473
 0.2263036  0.2260239  0.22611898 0.22641613 0.22632104 0.22660379
 0.22654584 0.22638644 0.22661513 0.22613573 0.22606283 0.22648668
 0.22652777 0.22697572 0.22710466 0.22691385 0.22723575 0.22698733
 0.22595927 0.22517724 0.22430485 0.2234797  0.2226545  0.22181132
 0.2212126  0.22081774 0.22108507 0.22110353 0.22120483 0.22192506
 0.22187652 0.22169352 0.22212741 0.22132786 0.22137003 0.22154658
 0.22140798 0.22242305 0.22150725 0.22287686 0.22263393 0.22561274]
