[2024-11-04 21:33:36,238] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-04 21:33:38,936] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-11-04 21:33:38,936] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-04 21:33:41,344] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.2, git-hash=unknown, git-branch=unknown
[2024-11-04 21:33:41,345] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1
[2024-11-04 21:33:42,545] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-04 21:33:42,546] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-04 21:33:42,546] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-04 21:33:42,547] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-04 21:33:42,547] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-04 21:33:42,547] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-04 21:33:42,547] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-04 21:33:42,547] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-04 21:33:42,547] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-04 21:33:42,547] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-04 21:33:42,718] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-11-04 21:33:42,719] [INFO] [utils.py:782:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB 
[2024-11-04 21:33:42,719] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 610.83 GB, percent = 60.6%
[2024-11-04 21:33:42,786] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-11-04 21:33:42,787] [INFO] [utils.py:782:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-04 21:33:42,787] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 610.92 GB, percent = 60.6%
[2024-11-04 21:33:42,787] [INFO] [stage_1_and_2.py:544:__init__] optimizer state initialized
[2024-11-04 21:33:42,852] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-11-04 21:33:42,853] [INFO] [utils.py:782:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-04 21:33:42,853] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 610.83 GB, percent = 60.6%
[2024-11-04 21:33:42,853] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-11-04 21:33:42,853] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-04 21:33:42,853] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.OneCycleLR object at 0x1554a79283d0>
[2024-11-04 21:33:42,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-04 21:33:42,854] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554a6fbf520>
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-11-04 21:33:42,854] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-11-04 21:33:42,855] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   train_batch_size ............. 256
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  256
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   world_size ................... 1
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-04 21:33:42,856] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 2
[2024-11-04 21:33:42,856] [INFO] [config.py:989:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 256, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.6185970
	speed: 1.1678s/iter; left time: 34358.9220s
	iters: 200, epoch: 1 | loss: 1.0287246
	speed: 1.1451s/iter; left time: 33575.0555s
	iters: 300, epoch: 1 | loss: 0.4337538
	speed: 0.8947s/iter; left time: 26142.9854s
	iters: 400, epoch: 1 | loss: 0.5011612
	speed: 1.5126s/iter; left time: 44047.1799s
	iters: 500, epoch: 1 | loss: 0.3588304
	speed: 1.5138s/iter; left time: 43932.5834s
	iters: 600, epoch: 1 | loss: 0.3986471
	speed: 1.5145s/iter; left time: 43801.0587s
	iters: 700, epoch: 1 | loss: 0.4170628
	speed: 1.5129s/iter; left time: 43603.4763s
	iters: 800, epoch: 1 | loss: 0.3969284
	speed: 1.4841s/iter; left time: 42623.8605s
	iters: 900, epoch: 1 | loss: 0.3349645
	speed: 1.4929s/iter; left time: 42729.2270s
	iters: 1000, epoch: 1 | loss: 0.4330398
	speed: 1.5147s/iter; left time: 43201.1897s
	iters: 1100, epoch: 1 | loss: 0.4566983
	speed: 1.5157s/iter; left time: 43077.8745s
	iters: 1200, epoch: 1 | loss: 0.3191250
	speed: 1.5159s/iter; left time: 42932.4224s
	iters: 1300, epoch: 1 | loss: 0.3851061
	speed: 1.5127s/iter; left time: 42690.9658s
	iters: 1400, epoch: 1 | loss: 0.4641199
	speed: 1.5116s/iter; left time: 42508.3243s
	iters: 1500, epoch: 1 | loss: 0.3758968
	speed: 1.5138s/iter; left time: 42418.5319s
	iters: 1600, epoch: 1 | loss: 0.4514285
	speed: 1.5125s/iter; left time: 42231.3483s
	iters: 1700, epoch: 1 | loss: 0.3485023
	speed: 1.5068s/iter; left time: 41920.8262s
	iters: 1800, epoch: 1 | loss: 0.3242528
	speed: 1.4751s/iter; left time: 40891.2798s
	iters: 1900, epoch: 1 | loss: 0.8570780
	speed: 1.5187s/iter; left time: 41947.1225s
	iters: 2000, epoch: 1 | loss: 0.5958056
	speed: 1.2411s/iter; left time: 34156.4892s
	iters: 2100, epoch: 1 | loss: 0.4267905
	speed: 1.1707s/iter; left time: 32101.2689s
	iters: 2200, epoch: 1 | loss: 0.4589938
	speed: 0.8495s/iter; left time: 23210.2906s
	iters: 2300, epoch: 1 | loss: 0.4727920
	speed: 1.4410s/iter; left time: 39226.2967s
	iters: 2400, epoch: 1 | loss: 0.3537569
	speed: 1.5138s/iter; left time: 41054.5572s
	iters: 2500, epoch: 1 | loss: 0.3190621
	speed: 1.5146s/iter; left time: 40926.1251s
	iters: 2600, epoch: 1 | loss: 1.4326663
	speed: 1.5124s/iter; left time: 40714.5018s
	iters: 2700, epoch: 1 | loss: 1.1629964
	speed: 1.5134s/iter; left time: 40591.4933s
	iters: 2800, epoch: 1 | loss: 0.9534792
	speed: 1.4722s/iter; left time: 39337.5672s
	iters: 2900, epoch: 1 | loss: 0.3732083
	speed: 1.5066s/iter; left time: 40106.4702s
Epoch: 1 cost time: 4183.71816945076
Epoch: 1 | Train Loss: 0.5114507 Vali Loss: 0.4499118 Test Loss: 0.1913956 MAE Loss: 0.2418584
lr = 0.0005201277
Updating learning rate to 0.000520127728651188
	iters: 100, epoch: 2 | loss: 1.0967090
	speed: 14.3474s/iter; left time: 379761.9667s
	iters: 200, epoch: 2 | loss: 0.3925228
	speed: 1.6802s/iter; left time: 44306.4739s
	iters: 300, epoch: 2 | loss: 0.3976469
	speed: 1.6842s/iter; left time: 44242.2450s
	iters: 400, epoch: 2 | loss: 0.5223507
	speed: 1.6558s/iter; left time: 43331.9085s
	iters: 500, epoch: 2 | loss: 0.4759557
	speed: 1.6731s/iter; left time: 43615.4194s
	iters: 600, epoch: 2 | loss: 0.3508390
	speed: 1.6784s/iter; left time: 43586.1698s
	iters: 700, epoch: 2 | loss: 0.5074162
	speed: 1.6842s/iter; left time: 43569.7425s
	iters: 800, epoch: 2 | loss: 1.1572851
	speed: 1.6671s/iter; left time: 42959.0687s
	iters: 900, epoch: 2 | loss: 0.3931638
	speed: 1.6347s/iter; left time: 41960.9512s
	iters: 1000, epoch: 2 | loss: 0.6314049
	speed: 1.6674s/iter; left time: 42634.3993s
	iters: 1100, epoch: 2 | loss: 0.2992190
	speed: 1.6889s/iter; left time: 43014.4379s
	iters: 1200, epoch: 2 | loss: 0.5641208
	speed: 1.7062s/iter; left time: 43285.0915s
	iters: 1300, epoch: 2 | loss: 0.3843272
	speed: 1.6753s/iter; left time: 42332.5650s
	iters: 1400, epoch: 2 | loss: 0.3866045
	speed: 1.3983s/iter; left time: 35193.2603s
	iters: 1500, epoch: 2 | loss: 0.4174690
	speed: 1.2418s/iter; left time: 31129.6179s
	iters: 1600, epoch: 2 | loss: 0.6536233
	speed: 1.5028s/iter; left time: 37524.0807s
	iters: 1700, epoch: 2 | loss: 0.4376935
	speed: 1.6802s/iter; left time: 41785.2761s
	iters: 1800, epoch: 2 | loss: 0.4065953
	speed: 1.5968s/iter; left time: 39550.0384s
	iters: 1900, epoch: 2 | loss: 0.4899794
	speed: 1.6772s/iter; left time: 41374.0444s
	iters: 2000, epoch: 2 | loss: 0.3546086
	speed: 1.6727s/iter; left time: 41097.0701s
	iters: 2100, epoch: 2 | loss: 0.5375152
	speed: 1.6480s/iter; left time: 40323.8181s
	iters: 2200, epoch: 2 | loss: 0.4245054
	speed: 1.6542s/iter; left time: 40311.1808s
	iters: 2300, epoch: 2 | loss: 0.4479972
	speed: 1.6741s/iter; left time: 40629.3548s
	iters: 2400, epoch: 2 | loss: 0.4059142
	speed: 1.6728s/iter; left time: 40430.1645s
	iters: 2500, epoch: 2 | loss: 0.3532908
	speed: 1.6803s/iter; left time: 40443.3680s
	iters: 2600, epoch: 2 | loss: 0.4183631
	speed: 1.6352s/iter; left time: 39192.9543s
	iters: 2700, epoch: 2 | loss: 0.4479089
	speed: 1.6560s/iter; left time: 39527.3750s
	iters: 2800, epoch: 2 | loss: 0.4512901
	speed: 1.6115s/iter; left time: 38302.8051s
	iters: 2900, epoch: 2 | loss: 0.5477216
	speed: 1.3703s/iter; left time: 32433.5320s
Epoch: 2 cost time: 4738.954900741577
Epoch: 2 | Train Loss: 0.5001327 Vali Loss: 0.4870451 Test Loss: 0.2213611 MAE Loss: 0.2664602
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000260063864325594
	iters: 100, epoch: 3 | loss: 1.2176267
	speed: 15.4630s/iter; left time: 363643.5878s
	iters: 200, epoch: 3 | loss: 0.4704003
	speed: 1.6840s/iter; left time: 39434.2783s
	iters: 300, epoch: 3 | loss: 0.4451468
	speed: 1.7042s/iter; left time: 39735.9569s
	iters: 400, epoch: 3 | loss: 0.7001425
	speed: 1.6412s/iter; left time: 38104.5522s
	iters: 500, epoch: 3 | loss: 0.5488572
	speed: 1.3711s/iter; left time: 31695.5409s
	iters: 600, epoch: 3 | loss: 0.5189633
	speed: 1.1340s/iter; left time: 26101.0225s
	iters: 700, epoch: 3 | loss: 0.4797715
	speed: 1.6087s/iter; left time: 36865.8583s
	iters: 800, epoch: 3 | loss: 0.3738204
	speed: 1.6797s/iter; left time: 38325.5021s
	iters: 900, epoch: 3 | loss: 0.4119843
	speed: 1.7068s/iter; left time: 38772.3959s
	iters: 1000, epoch: 3 | loss: 0.4151532
	speed: 1.7264s/iter; left time: 39046.1771s
	iters: 1100, epoch: 3 | loss: 0.3919658
	speed: 1.6913s/iter; left time: 38083.4378s
	iters: 1200, epoch: 3 | loss: 0.3250466
	speed: 1.7033s/iter; left time: 38182.9364s
	iters: 1300, epoch: 3 | loss: 0.4678383
	speed: 1.7230s/iter; left time: 38452.6443s
	iters: 1400, epoch: 3 | loss: 0.9013758
	speed: 1.7076s/iter; left time: 37937.9072s
	iters: 1500, epoch: 3 | loss: 0.3579062
	speed: 1.7349s/iter; left time: 38370.9123s
	iters: 1600, epoch: 3 | loss: 0.4334851
	speed: 1.7172s/iter; left time: 37806.5189s
	iters: 1700, epoch: 3 | loss: 0.4417633
	speed: 1.7208s/iter; left time: 37713.9937s
	iters: 1800, epoch: 3 | loss: 0.6362986
	speed: 1.7272s/iter; left time: 37681.7726s
	iters: 1900, epoch: 3 | loss: 1.1600213
	speed: 1.4255s/iter; left time: 30957.7964s
	iters: 2000, epoch: 3 | loss: 0.8828108
	speed: 1.3317s/iter; left time: 28787.0862s
	iters: 2100, epoch: 3 | loss: 0.9243070
	speed: 1.3321s/iter; left time: 28662.3135s
	iters: 2200, epoch: 3 | loss: 0.3831476
	speed: 1.7402s/iter; left time: 37269.5484s
	iters: 2300, epoch: 3 | loss: 1.1263084
	speed: 1.7428s/iter; left time: 37151.5141s
	iters: 2400, epoch: 3 | loss: 0.4341890
	speed: 1.7396s/iter; left time: 36908.2230s
	iters: 2500, epoch: 3 | loss: 0.4170002
	speed: 1.7046s/iter; left time: 35995.3155s
	iters: 2600, epoch: 3 | loss: 0.4206192
	speed: 1.6833s/iter; left time: 35377.8583s
	iters: 2700, epoch: 3 | loss: 0.4374374
	speed: 1.6812s/iter; left time: 35165.4168s
	iters: 2800, epoch: 3 | loss: 0.4040597
	speed: 1.6947s/iter; left time: 35279.4309s
	iters: 2900, epoch: 3 | loss: 0.3684828
	speed: 1.7372s/iter; left time: 35988.8329s
Epoch: 3 cost time: 4835.612686634064
Epoch: 3 | Train Loss: 0.5124789 Vali Loss: 0.4805224 Test Loss: 0.2248743 MAE Loss: 0.2709472
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.000130031932162797
	iters: 100, epoch: 4 | loss: 0.3646601
	speed: 13.5147s/iter; left time: 277930.4357s
	iters: 200, epoch: 4 | loss: 1.1879768
	speed: 1.7461s/iter; left time: 35734.7709s
	iters: 300, epoch: 4 | loss: 0.4620228
	speed: 1.7409s/iter; left time: 35452.6194s
	iters: 400, epoch: 4 | loss: 0.5033364
	speed: 1.7126s/iter; left time: 34704.8681s
	iters: 500, epoch: 4 | loss: 0.4105859
	speed: 1.7230s/iter; left time: 34745.1440s
	iters: 600, epoch: 4 | loss: 0.4246981
	speed: 1.7001s/iter; left time: 34111.9547s
	iters: 700, epoch: 4 | loss: 0.3697750
	speed: 1.7321s/iter; left time: 34580.5060s
	iters: 800, epoch: 4 | loss: 0.4668794
	speed: 1.7241s/iter; left time: 34248.4679s
	iters: 900, epoch: 4 | loss: 1.1641743
	speed: 1.7048s/iter; left time: 33695.1438s
	iters: 1000, epoch: 4 | loss: 1.1573966
	speed: 1.5232s/iter; left time: 29954.2213s
	iters: 1100, epoch: 4 | loss: 0.6366490
	speed: 1.3641s/iter; left time: 26688.2046s
	iters: 1200, epoch: 4 | loss: 0.4726975
	speed: 1.2193s/iter; left time: 23734.3880s
	iters: 1300, epoch: 4 | loss: 0.3057582
	speed: 1.7461s/iter; left time: 33813.3117s
	iters: 1400, epoch: 4 | loss: 1.6175129
	speed: 1.7437s/iter; left time: 33593.0565s
	iters: 1500, epoch: 4 | loss: 0.4293536
	speed: 1.7064s/iter; left time: 32702.7164s
	iters: 1600, epoch: 4 | loss: 0.3350466
	speed: 1.7385s/iter; left time: 33144.3535s
	iters: 1700, epoch: 4 | loss: 0.4031690
	speed: 1.7446s/iter; left time: 33086.2603s
	iters: 1800, epoch: 4 | loss: 0.4177408
	speed: 1.7414s/iter; left time: 32850.9123s
	iters: 1900, epoch: 4 | loss: 0.4357782
	speed: 1.7218s/iter; left time: 32308.7328s
	iters: 2000, epoch: 4 | loss: 0.3782040
	speed: 1.7540s/iter; left time: 32737.5600s
	iters: 2100, epoch: 4 | loss: 0.4273685
	speed: 1.7311s/iter; left time: 32137.3904s
	iters: 2200, epoch: 4 | loss: 0.3868733
	speed: 1.6707s/iter; left time: 30849.1695s
	iters: 2300, epoch: 4 | loss: 0.3993577
	speed: 1.6766s/iter; left time: 30790.2722s
	iters: 2400, epoch: 4 | loss: 0.3317987
	speed: 1.5903s/iter; left time: 29047.2042s
	iters: 2500, epoch: 4 | loss: 0.4312374
	speed: 1.3665s/iter; left time: 24823.0149s
	iters: 2600, epoch: 4 | loss: 0.4348496
	speed: 1.1265s/iter; left time: 20350.9571s
	iters: 2700, epoch: 4 | loss: 1.0991564
	speed: 1.6953s/iter; left time: 30456.3838s
	iters: 2800, epoch: 4 | loss: 0.4822837
	speed: 1.6990s/iter; left time: 30352.4310s
	iters: 2900, epoch: 4 | loss: 0.4018552
	speed: 1.7019s/iter; left time: 30233.8734s
Epoch: 4 cost time: 4867.5520713329315
Epoch: 4 | Train Loss: 0.5103417 Vali Loss: 0.4823040 Test Loss: 0.2223749 MAE Loss: 0.2673134
EarlyStopping counter: 3 out of 3
Early stopping
mse:0.22237494785347145, mae:0.267313380944149
success delete checkpoints
