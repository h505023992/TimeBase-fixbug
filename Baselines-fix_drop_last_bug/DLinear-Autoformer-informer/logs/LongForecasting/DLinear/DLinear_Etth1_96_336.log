Args in experiment:
Namespace(is_training=1, train_only=False, model_id='DLinear_ETTh1_96_336', model='DLinear', data='ETTh1', root_path='./dataset/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=30, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)
Use GPU: cuda:0
>>>>>>>start training : DLinear_ETTh1_96_336_DLinear_ETTh1_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8209
val 2545
test 2545
	iters: 100, epoch: 1 | loss: 0.8220071
	speed: 0.0454s/iter; left time: 345.4244s
	iters: 200, epoch: 1 | loss: 0.7078680
	speed: 0.0224s/iter; left time: 168.2465s
Epoch: 1 cost time: 8.084898710250854
Epoch: 1, Steps: 257 | Train Loss: 0.7554218 Vali Loss: 1.2695529 Test Loss: 0.6051341
Validation loss decreased (inf --> 1.269553).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.4443314
	speed: 0.1032s/iter; left time: 759.0289s
	iters: 200, epoch: 2 | loss: 0.4809701
	speed: 0.0185s/iter; left time: 133.8683s
Epoch: 2 cost time: 5.464120864868164
Epoch: 2, Steps: 257 | Train Loss: 0.5343670 Vali Loss: 1.1988139 Test Loss: 0.5403593
Validation loss decreased (1.269553 --> 1.198814).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.4806474
	speed: 0.0941s/iter; left time: 667.9014s
	iters: 200, epoch: 3 | loss: 0.5041888
	speed: 0.0183s/iter; left time: 127.7790s
Epoch: 3 cost time: 5.2957072257995605
Epoch: 3, Steps: 257 | Train Loss: 0.5097247 Vali Loss: 1.1806892 Test Loss: 0.5270327
Validation loss decreased (1.198814 --> 1.180689).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4403337
	speed: 0.1112s/iter; left time: 760.6088s
	iters: 200, epoch: 4 | loss: 0.5575015
	speed: 0.0235s/iter; left time: 158.5705s
Epoch: 4 cost time: 6.640300750732422
Epoch: 4, Steps: 257 | Train Loss: 0.5019964 Vali Loss: 1.1757396 Test Loss: 0.5218692
Validation loss decreased (1.180689 --> 1.175740).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.5387511
	speed: 0.1283s/iter; left time: 844.4658s
	iters: 200, epoch: 5 | loss: 0.4853882
	speed: 0.0230s/iter; left time: 148.9434s
Epoch: 5 cost time: 6.652116537094116
Epoch: 5, Steps: 257 | Train Loss: 0.4983982 Vali Loss: 1.1704481 Test Loss: 0.5193567
Validation loss decreased (1.175740 --> 1.170448).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5603153
	speed: 0.1164s/iter; left time: 736.6198s
	iters: 200, epoch: 6 | loss: 0.5119465
	speed: 0.0211s/iter; left time: 131.5739s
Epoch: 6 cost time: 6.263292551040649
Epoch: 6, Steps: 257 | Train Loss: 0.4967614 Vali Loss: 1.1701325 Test Loss: 0.5182089
Validation loss decreased (1.170448 --> 1.170133).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.4466498
	speed: 0.0842s/iter; left time: 511.0026s
	iters: 200, epoch: 7 | loss: 0.5158516
	speed: 0.0140s/iter; left time: 83.2780s
Epoch: 7 cost time: 4.475529193878174
Epoch: 7, Steps: 257 | Train Loss: 0.4961199 Vali Loss: 1.1687168 Test Loss: 0.5176373
Validation loss decreased (1.170133 --> 1.168717).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.5065756
	speed: 0.0819s/iter; left time: 476.0196s
	iters: 200, epoch: 8 | loss: 0.5308516
	speed: 0.0163s/iter; left time: 93.2206s
Epoch: 8 cost time: 4.90471076965332
Epoch: 8, Steps: 257 | Train Loss: 0.4957035 Vali Loss: 1.1682270 Test Loss: 0.5173257
Validation loss decreased (1.168717 --> 1.168227).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.5413671
	speed: 0.0777s/iter; left time: 431.6392s
	iters: 200, epoch: 9 | loss: 0.4564752
	speed: 0.0158s/iter; left time: 86.0276s
Epoch: 9 cost time: 4.687092304229736
Epoch: 9, Steps: 257 | Train Loss: 0.4953985 Vali Loss: 1.1685948 Test Loss: 0.5171838
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.5270177
	speed: 0.0737s/iter; left time: 390.2641s
	iters: 200, epoch: 10 | loss: 0.5436126
	speed: 0.0159s/iter; left time: 82.7719s
Epoch: 10 cost time: 4.76689076423645
Epoch: 10, Steps: 257 | Train Loss: 0.4954372 Vali Loss: 1.1680292 Test Loss: 0.5171099
Validation loss decreased (1.168227 --> 1.168029).  Saving model ...
Updating learning rate to 1.953125e-07
	iters: 100, epoch: 11 | loss: 0.4623655
	speed: 0.0940s/iter; left time: 473.6048s
	iters: 200, epoch: 11 | loss: 0.4918803
	speed: 0.0203s/iter; left time: 100.3674s
Epoch: 11 cost time: 6.044686317443848
Epoch: 11, Steps: 257 | Train Loss: 0.4953402 Vali Loss: 1.1690854 Test Loss: 0.5170748
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.765625e-08
	iters: 100, epoch: 12 | loss: 0.5159963
	speed: 0.0968s/iter; left time: 462.9876s
	iters: 200, epoch: 12 | loss: 0.4883451
	speed: 0.0206s/iter; left time: 96.5082s
Epoch: 12 cost time: 6.085498571395874
Epoch: 12, Steps: 257 | Train Loss: 0.4951646 Vali Loss: 1.1682856 Test Loss: 0.5170543
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.8828125e-08
	iters: 100, epoch: 13 | loss: 0.4686755
	speed: 0.1027s/iter; left time: 464.7259s
	iters: 200, epoch: 13 | loss: 0.4826163
	speed: 0.0196s/iter; left time: 86.7036s
Epoch: 13 cost time: 5.746678590774536
Epoch: 13, Steps: 257 | Train Loss: 0.4951310 Vali Loss: 1.1688447 Test Loss: 0.5170436
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : DLinear_ETTh1_96_336_DLinear_ETTh1_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.5168237090110779, mae:0.4868124723434448
