Args in experiment:
Namespace(is_training=1, model_id='ETTm1_96_336', model='FITS', data='ETTm1', root_path='./dataset/', data_path='ETTm1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=40, base_T=24, H_order=6)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_96_336_FITS_ETTm1_ftM_sl96_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34129
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=40, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6451200.0
params:  7380.0
Trainable parameters:  7380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6599202
	speed: 0.0819s/iter; left time: 645.1559s
	iters: 200, epoch: 1 | loss: 0.5012130
	speed: 0.0650s/iter; left time: 505.9487s
Epoch: 1 cost time: 18.216814517974854
Epoch: 1, Steps: 266 | Train Loss: 0.6412147 Vali Loss: 0.9663641 Test Loss: 0.7015038
Validation loss decreased (inf --> 0.966364).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4382699
	speed: 0.2659s/iter; left time: 2024.4689s
	iters: 200, epoch: 2 | loss: 0.3801804
	speed: 0.0582s/iter; left time: 437.1981s
Epoch: 2 cost time: 16.252297401428223
Epoch: 2, Steps: 266 | Train Loss: 0.4036536 Vali Loss: 0.8255442 Test Loss: 0.5667670
Validation loss decreased (0.966364 --> 0.825544).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3422611
	speed: 0.2598s/iter; left time: 1909.5312s
	iters: 200, epoch: 3 | loss: 0.3518200
	speed: 0.0540s/iter; left time: 391.2305s
Epoch: 3 cost time: 16.40524458885193
Epoch: 3, Steps: 266 | Train Loss: 0.3639903 Vali Loss: 0.7705522 Test Loss: 0.5131255
Validation loss decreased (0.825544 --> 0.770552).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3529570
	speed: 0.2882s/iter; left time: 2040.9833s
	iters: 200, epoch: 4 | loss: 0.3321213
	speed: 0.0675s/iter; left time: 471.4360s
Epoch: 4 cost time: 18.66958236694336
Epoch: 4, Steps: 266 | Train Loss: 0.3482337 Vali Loss: 0.7387878 Test Loss: 0.4816301
Validation loss decreased (0.770552 --> 0.738788).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3172217
	speed: 0.2862s/iter; left time: 1951.3251s
	iters: 200, epoch: 5 | loss: 0.3135530
	speed: 0.0489s/iter; left time: 328.6138s
Epoch: 5 cost time: 14.117733478546143
Epoch: 5, Steps: 266 | Train Loss: 0.3398203 Vali Loss: 0.7215828 Test Loss: 0.4620004
Validation loss decreased (0.738788 --> 0.721583).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3270700
	speed: 0.2553s/iter; left time: 1672.2421s
	iters: 200, epoch: 6 | loss: 0.3360260
	speed: 0.0519s/iter; left time: 334.4937s
Epoch: 6 cost time: 16.097215175628662
Epoch: 6, Steps: 266 | Train Loss: 0.3352460 Vali Loss: 0.7109449 Test Loss: 0.4501695
Validation loss decreased (0.721583 --> 0.710945).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3341870
	speed: 0.2678s/iter; left time: 1683.1857s
	iters: 200, epoch: 7 | loss: 0.3022675
	speed: 0.0574s/iter; left time: 354.7948s
Epoch: 7 cost time: 16.476150512695312
Epoch: 7, Steps: 266 | Train Loss: 0.3328126 Vali Loss: 0.7047634 Test Loss: 0.4428758
Validation loss decreased (0.710945 --> 0.704763).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3234562
	speed: 0.3009s/iter; left time: 1811.3689s
	iters: 200, epoch: 8 | loss: 0.3110767
	speed: 0.0603s/iter; left time: 356.8477s
Epoch: 8 cost time: 17.559885025024414
Epoch: 8, Steps: 266 | Train Loss: 0.3310827 Vali Loss: 0.7009771 Test Loss: 0.4376352
Validation loss decreased (0.704763 --> 0.700977).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3213224
	speed: 0.2740s/iter; left time: 1576.3291s
	iters: 200, epoch: 9 | loss: 0.3063079
	speed: 0.0516s/iter; left time: 291.5715s
Epoch: 9 cost time: 14.84526777267456
Epoch: 9, Steps: 266 | Train Loss: 0.3304015 Vali Loss: 0.6974884 Test Loss: 0.4345866
Validation loss decreased (0.700977 --> 0.697488).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3640578
	speed: 0.2612s/iter; left time: 1433.1521s
	iters: 200, epoch: 10 | loss: 0.3091585
	speed: 0.0592s/iter; left time: 318.9807s
Epoch: 10 cost time: 16.367563486099243
Epoch: 10, Steps: 266 | Train Loss: 0.3298221 Vali Loss: 0.6966846 Test Loss: 0.4325510
Validation loss decreased (0.697488 --> 0.696685).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3316777
	speed: 0.2763s/iter; left time: 1442.5607s
	iters: 200, epoch: 11 | loss: 0.3352825
	speed: 0.0610s/iter; left time: 312.4814s
Epoch: 11 cost time: 17.555784940719604
Epoch: 11, Steps: 266 | Train Loss: 0.3295324 Vali Loss: 0.6955442 Test Loss: 0.4308359
Validation loss decreased (0.696685 --> 0.695544).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3146103
	speed: 0.2844s/iter; left time: 1409.0702s
	iters: 200, epoch: 12 | loss: 0.3677442
	speed: 0.0533s/iter; left time: 258.5806s
Epoch: 12 cost time: 14.802399396896362
Epoch: 12, Steps: 266 | Train Loss: 0.3293648 Vali Loss: 0.6956721 Test Loss: 0.4299422
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3381393
	speed: 0.2596s/iter; left time: 1217.3227s
	iters: 200, epoch: 13 | loss: 0.3278828
	speed: 0.0549s/iter; left time: 251.9782s
Epoch: 13 cost time: 15.935789108276367
Epoch: 13, Steps: 266 | Train Loss: 0.3293188 Vali Loss: 0.6950080 Test Loss: 0.4292121
Validation loss decreased (0.695544 --> 0.695008).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3361390
	speed: 0.2590s/iter; left time: 1145.6876s
	iters: 200, epoch: 14 | loss: 0.3066293
	speed: 0.0466s/iter; left time: 201.6375s
Epoch: 14 cost time: 14.511913776397705
Epoch: 14, Steps: 266 | Train Loss: 0.3291813 Vali Loss: 0.6949843 Test Loss: 0.4290175
Validation loss decreased (0.695008 --> 0.694984).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3369823
	speed: 0.2893s/iter; left time: 1202.7659s
	iters: 200, epoch: 15 | loss: 0.2963986
	speed: 0.0610s/iter; left time: 247.6171s
Epoch: 15 cost time: 18.567527055740356
Epoch: 15, Steps: 266 | Train Loss: 0.3290855 Vali Loss: 0.6950560 Test Loss: 0.4285537
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3494829
	speed: 0.2914s/iter; left time: 1133.9176s
	iters: 200, epoch: 16 | loss: 0.3365364
	speed: 0.0581s/iter; left time: 220.4407s
Epoch: 16 cost time: 16.2931547164917
Epoch: 16, Steps: 266 | Train Loss: 0.3291768 Vali Loss: 0.6951776 Test Loss: 0.4283208
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3133423
	speed: 0.2517s/iter; left time: 912.3710s
	iters: 200, epoch: 17 | loss: 0.3120192
	speed: 0.0502s/iter; left time: 177.0737s
Epoch: 17 cost time: 14.467643976211548
Epoch: 17, Steps: 266 | Train Loss: 0.3291394 Vali Loss: 0.6945285 Test Loss: 0.4281545
Validation loss decreased (0.694984 --> 0.694528).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3267895
	speed: 0.2523s/iter; left time: 847.5791s
	iters: 200, epoch: 18 | loss: 0.3128654
	speed: 0.0621s/iter; left time: 202.2940s
Epoch: 18 cost time: 17.904895544052124
Epoch: 18, Steps: 266 | Train Loss: 0.3290297 Vali Loss: 0.6950381 Test Loss: 0.4282045
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3276314
	speed: 0.3055s/iter; left time: 945.0072s
	iters: 200, epoch: 19 | loss: 0.3210305
	speed: 0.0645s/iter; left time: 193.0946s
Epoch: 19 cost time: 17.56855583190918
Epoch: 19, Steps: 266 | Train Loss: 0.3290538 Vali Loss: 0.6942937 Test Loss: 0.4283615
Validation loss decreased (0.694528 --> 0.694294).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3292234
	speed: 0.2522s/iter; left time: 712.9494s
	iters: 200, epoch: 20 | loss: 0.3054130
	speed: 0.0493s/iter; left time: 134.3713s
Epoch: 20 cost time: 15.264372110366821
Epoch: 20, Steps: 266 | Train Loss: 0.3291746 Vali Loss: 0.6948926 Test Loss: 0.4279710
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3171527
	speed: 0.2539s/iter; left time: 650.2674s
	iters: 200, epoch: 21 | loss: 0.3168655
	speed: 0.0469s/iter; left time: 115.4056s
Epoch: 21 cost time: 14.56768250465393
Epoch: 21, Steps: 266 | Train Loss: 0.3289122 Vali Loss: 0.6956851 Test Loss: 0.4281503
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3511458
	speed: 0.2746s/iter; left time: 630.1989s
	iters: 200, epoch: 22 | loss: 0.3266930
	speed: 0.0604s/iter; left time: 132.6303s
Epoch: 22 cost time: 17.605732679367065
Epoch: 22, Steps: 266 | Train Loss: 0.3290686 Vali Loss: 0.6948295 Test Loss: 0.4281732
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3421288
	speed: 0.2954s/iter; left time: 599.4450s
	iters: 200, epoch: 23 | loss: 0.3331209
	speed: 0.0515s/iter; left time: 99.3351s
Epoch: 23 cost time: 14.897186756134033
Epoch: 23, Steps: 266 | Train Loss: 0.3290213 Vali Loss: 0.6945335 Test Loss: 0.4282314
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3227642
	speed: 0.2471s/iter; left time: 435.6046s
	iters: 200, epoch: 24 | loss: 0.3105588
	speed: 0.0504s/iter; left time: 83.8668s
Epoch: 24 cost time: 14.987272500991821
Epoch: 24, Steps: 266 | Train Loss: 0.3289471 Vali Loss: 0.6949230 Test Loss: 0.4280026
EarlyStopping counter: 5 out of 5
Early stopping
train 34129
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=40, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6451200.0
params:  7380.0
Trainable parameters:  7380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3754426
	speed: 0.0676s/iter; left time: 532.5150s
	iters: 200, epoch: 1 | loss: 0.3975376
	speed: 0.0584s/iter; left time: 454.2124s
Epoch: 1 cost time: 17.04069209098816
Epoch: 1, Steps: 266 | Train Loss: 0.4191942 Vali Loss: 0.6899221 Test Loss: 0.4246168
Validation loss decreased (inf --> 0.689922).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4092000
	speed: 0.2968s/iter; left time: 2259.8158s
	iters: 200, epoch: 2 | loss: 0.4757831
	speed: 0.0628s/iter; left time: 472.0241s
Epoch: 2 cost time: 17.94083333015442
Epoch: 2, Steps: 266 | Train Loss: 0.4185682 Vali Loss: 0.6896030 Test Loss: 0.4238406
Validation loss decreased (0.689922 --> 0.689603).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4140771
	speed: 0.2650s/iter; left time: 1947.5295s
	iters: 200, epoch: 3 | loss: 0.4223234
	speed: 0.0526s/iter; left time: 381.6369s
Epoch: 3 cost time: 14.519044637680054
Epoch: 3, Steps: 266 | Train Loss: 0.4184992 Vali Loss: 0.6895061 Test Loss: 0.4240799
Validation loss decreased (0.689603 --> 0.689506).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3646690
	speed: 0.2656s/iter; left time: 1881.5610s
	iters: 200, epoch: 4 | loss: 0.4227950
	speed: 0.0569s/iter; left time: 397.4470s
Epoch: 4 cost time: 15.681456089019775
Epoch: 4, Steps: 266 | Train Loss: 0.4185736 Vali Loss: 0.6902170 Test Loss: 0.4244535
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4219660
	speed: 0.2652s/iter; left time: 1807.9214s
	iters: 200, epoch: 5 | loss: 0.3739254
	speed: 0.0636s/iter; left time: 427.3269s
Epoch: 5 cost time: 17.643701553344727
Epoch: 5, Steps: 266 | Train Loss: 0.4184389 Vali Loss: 0.6896256 Test Loss: 0.4244159
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4475676
	speed: 0.3043s/iter; left time: 1993.1728s
	iters: 200, epoch: 6 | loss: 0.4246718
	speed: 0.0516s/iter; left time: 332.8515s
Epoch: 6 cost time: 14.975492715835571
Epoch: 6, Steps: 266 | Train Loss: 0.4183298 Vali Loss: 0.6894978 Test Loss: 0.4244732
Validation loss decreased (0.689506 --> 0.689498).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4137400
	speed: 0.2591s/iter; left time: 1628.2880s
	iters: 200, epoch: 7 | loss: 0.4385014
	speed: 0.0501s/iter; left time: 309.8490s
Epoch: 7 cost time: 14.77774453163147
Epoch: 7, Steps: 266 | Train Loss: 0.4183448 Vali Loss: 0.6889501 Test Loss: 0.4247354
Validation loss decreased (0.689498 --> 0.688950).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4508965
	speed: 0.2392s/iter; left time: 1439.4588s
	iters: 200, epoch: 8 | loss: 0.4396157
	speed: 0.0493s/iter; left time: 291.8395s
Epoch: 8 cost time: 14.464618921279907
Epoch: 8, Steps: 266 | Train Loss: 0.4182933 Vali Loss: 0.6902860 Test Loss: 0.4246130
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3796792
	speed: 0.2832s/iter; left time: 1629.3047s
	iters: 200, epoch: 9 | loss: 0.4644714
	speed: 0.0606s/iter; left time: 342.3237s
Epoch: 9 cost time: 16.70459222793579
Epoch: 9, Steps: 266 | Train Loss: 0.4182144 Vali Loss: 0.6893628 Test Loss: 0.4241476
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4253898
	speed: 0.2808s/iter; left time: 1540.6703s
	iters: 200, epoch: 10 | loss: 0.4241367
	speed: 0.0605s/iter; left time: 325.9147s
Epoch: 10 cost time: 15.927720308303833
Epoch: 10, Steps: 266 | Train Loss: 0.4183644 Vali Loss: 0.6892749 Test Loss: 0.4242173
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4436396
	speed: 0.2644s/iter; left time: 1380.6140s
	iters: 200, epoch: 11 | loss: 0.4391827
	speed: 0.0522s/iter; left time: 267.3846s
Epoch: 11 cost time: 14.641789674758911
Epoch: 11, Steps: 266 | Train Loss: 0.4181764 Vali Loss: 0.6895791 Test Loss: 0.4244461
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4462700
	speed: 0.2569s/iter; left time: 1272.8843s
	iters: 200, epoch: 12 | loss: 0.4182931
	speed: 0.0562s/iter; left time: 272.7798s
Epoch: 12 cost time: 15.80303955078125
Epoch: 12, Steps: 266 | Train Loss: 0.4182977 Vali Loss: 0.6886637 Test Loss: 0.4245521
Validation loss decreased (0.688950 --> 0.688664).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4315868
	speed: 0.3068s/iter; left time: 1438.7300s
	iters: 200, epoch: 13 | loss: 0.4038782
	speed: 0.0661s/iter; left time: 303.1867s
Epoch: 13 cost time: 17.678672313690186
Epoch: 13, Steps: 266 | Train Loss: 0.4182546 Vali Loss: 0.6889858 Test Loss: 0.4244858
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4605663
	speed: 0.2491s/iter; left time: 1101.9269s
	iters: 200, epoch: 14 | loss: 0.3743022
	speed: 0.0547s/iter; left time: 236.5751s
Epoch: 14 cost time: 14.41875433921814
Epoch: 14, Steps: 266 | Train Loss: 0.4182455 Vali Loss: 0.6896613 Test Loss: 0.4246996
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4225589
	speed: 0.2533s/iter; left time: 1052.7934s
	iters: 200, epoch: 15 | loss: 0.4161265
	speed: 0.0521s/iter; left time: 211.5270s
Epoch: 15 cost time: 15.509447574615479
Epoch: 15, Steps: 266 | Train Loss: 0.4181914 Vali Loss: 0.6888639 Test Loss: 0.4244910
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4532163
	speed: 0.2789s/iter; left time: 1085.1242s
	iters: 200, epoch: 16 | loss: 0.4191932
	speed: 0.0641s/iter; left time: 243.1381s
Epoch: 16 cost time: 17.938664197921753
Epoch: 16, Steps: 266 | Train Loss: 0.4181855 Vali Loss: 0.6893722 Test Loss: 0.4247229
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3919427
	speed: 0.2895s/iter; left time: 1049.5461s
	iters: 200, epoch: 17 | loss: 0.4392074
	speed: 0.0565s/iter; left time: 199.2061s
Epoch: 17 cost time: 15.498608350753784
Epoch: 17, Steps: 266 | Train Loss: 0.4182116 Vali Loss: 0.6894546 Test Loss: 0.4245104
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTm1_96_336_FITS_ETTm1_ftM_sl96_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.4246536195278168, mae:0.4145638644695282, rse:0.6201056838035583, corr:[0.54686785 0.54368156 0.5431574  0.5402455  0.53910214 0.5365073
 0.53444576 0.534387   0.53182775 0.5309428  0.5292846  0.52794933
 0.5264029  0.523156   0.52054554 0.51781243 0.5148262  0.5122702
 0.5094574  0.50689524 0.5046874  0.50146693 0.4985296  0.49564394
 0.49212763 0.48892292 0.48522896 0.48228183 0.48109916 0.47935772
 0.47786263 0.47740665 0.47710332 0.47769678 0.47774282 0.4780266
 0.47835395 0.47770017 0.47731265 0.47688746 0.4768487  0.47672552
 0.47592294 0.47592616 0.4761617  0.4758691  0.47590995 0.47643366
 0.47723442 0.4774997  0.4774067  0.47780517 0.47813106 0.47784615
 0.47835812 0.47868517 0.47880086 0.47886807 0.47870827 0.47851107
 0.47821453 0.47759402 0.4775071  0.47738102 0.47707713 0.47670138
 0.476545   0.47717914 0.47801617 0.47888488 0.47994694 0.48107722
 0.48229584 0.4832804  0.4843786  0.48506156 0.48548365 0.48604462
 0.4869283  0.4873678  0.4876068  0.48836604 0.48918638 0.4898538
 0.49053526 0.4913927  0.49247646 0.4931683  0.4944492  0.495911
 0.49686992 0.49795136 0.49880928 0.49961448 0.50001484 0.5001325
 0.49956992 0.4985197  0.49755648 0.49590617 0.4939486  0.49294636
 0.49147335 0.49004343 0.48928782 0.4876408  0.48605585 0.48516044
 0.4840842  0.48252016 0.48115152 0.48040828 0.47924805 0.47827953
 0.4778     0.476712   0.47549987 0.47453788 0.47296488 0.47134912
 0.470609   0.46947923 0.46781656 0.46703437 0.46634528 0.46575767
 0.4655716  0.46494883 0.46478945 0.46496665 0.4650599  0.46529528
 0.4649743  0.46526176 0.46527416 0.46465826 0.46460742 0.46479887
 0.4646389  0.4644631  0.46453288 0.46429464 0.46425077 0.46482816
 0.46514648 0.46551567 0.46593195 0.4657108  0.46572447 0.46624693
 0.46662727 0.46681222 0.4670108  0.4669424  0.46694815 0.46721944
 0.46682757 0.4664367  0.46633482 0.46608788 0.46615535 0.46654636
 0.46709096 0.4675577  0.4683994  0.46919963 0.46958277 0.47085154
 0.4719932  0.47278684 0.47386745 0.47452015 0.47504002 0.47577843
 0.47637033 0.47740972 0.4780428  0.47852716 0.47953743 0.48036978
 0.4813146  0.4819924  0.48296908 0.48478594 0.48556125 0.48673487
 0.48876867 0.48966634 0.4907965  0.49176708 0.4930059  0.49506336
 0.49615213 0.49675077 0.4978238  0.49749157 0.49626985 0.49503562
 0.49432018 0.49291912 0.49117586 0.4901787  0.48899555 0.48725063
 0.4856304  0.4841839  0.48236492 0.48049292 0.47933802 0.4783666
 0.47676155 0.4758711  0.47435364 0.472416   0.47117704 0.46949276
 0.46761218 0.4662295  0.4640149  0.462445   0.461622   0.46094722
 0.4605129  0.46061    0.46122652 0.46119514 0.4609402  0.4609318
 0.46062905 0.46055302 0.45995334 0.4595533  0.45961946 0.45881382
 0.45913824 0.45926216 0.4590994  0.45925128 0.45893267 0.45948505
 0.45949152 0.459313   0.45983768 0.46003333 0.4604819  0.46081218
 0.46099967 0.46164793 0.46113762 0.4612908  0.46187705 0.4614245
 0.46125114 0.46152687 0.46075854 0.4605802  0.46090508 0.4612263
 0.4616871  0.46186465 0.46216488 0.46381807 0.46521258 0.46586245
 0.46731773 0.4684661  0.46928492 0.47000968 0.470745   0.4712882
 0.47200885 0.47298703 0.47353968 0.473674   0.47437754 0.4755282
 0.47662136 0.4777185  0.4786965  0.47923473 0.48005894 0.48102394
 0.48140627 0.48202845 0.4825831  0.48307544 0.48377463 0.4828707
 0.4817747  0.4810135  0.47998837 0.4786137  0.4766611  0.4753196
 0.47379237 0.47249538 0.47091803 0.46904048 0.46714717 0.46533802
 0.46382558 0.46228996 0.4611884  0.4602614  0.45889965 0.45799455
 0.45694774 0.45617315 0.4559198  0.4544451  0.4536492  0.45299616
 0.4517096  0.4508477  0.44955057 0.44897276 0.44764245 0.44714817
 0.44733465 0.4461435  0.44602224 0.44568855 0.44591796 0.44552273
 0.44471636 0.44545656 0.4438378  0.44424528 0.44450134 0.443827
 0.4446282  0.44463065 0.4460226  0.4452902  0.44829977 0.44585177]
