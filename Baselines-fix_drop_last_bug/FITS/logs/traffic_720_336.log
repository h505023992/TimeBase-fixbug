Args in experiment:
Namespace(is_training=1, model_id='traffic_720_336', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=720, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=258, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_720_336_FITS_custom_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=258, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10760408064.0
params:  97902.0
Trainable parameters:  97902
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 56.2351131439209
Epoch: 1, Steps: 87 | Train Loss: 1.1466808 Vali Loss: 1.2148057 Test Loss: 1.4078773
Validation loss decreased (inf --> 1.214806).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 56.36388301849365
Epoch: 2, Steps: 87 | Train Loss: 0.8576456 Vali Loss: 1.0793283 Test Loss: 1.2494292
Validation loss decreased (1.214806 --> 1.079328).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 52.364686250686646
Epoch: 3, Steps: 87 | Train Loss: 0.7573897 Vali Loss: 1.0100536 Test Loss: 1.1689043
Validation loss decreased (1.079328 --> 1.010054).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 59.02942609786987
Epoch: 4, Steps: 87 | Train Loss: 0.6876597 Vali Loss: 0.9541546 Test Loss: 1.1050590
Validation loss decreased (1.010054 --> 0.954155).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 60.26450228691101
Epoch: 5, Steps: 87 | Train Loss: 0.6305370 Vali Loss: 0.9047379 Test Loss: 1.0479674
Validation loss decreased (0.954155 --> 0.904738).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 59.758851051330566
Epoch: 6, Steps: 87 | Train Loss: 0.5819761 Vali Loss: 0.8607903 Test Loss: 0.9970930
Validation loss decreased (0.904738 --> 0.860790).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 59.97792840003967
Epoch: 7, Steps: 87 | Train Loss: 0.5401908 Vali Loss: 0.8223950 Test Loss: 0.9530423
Validation loss decreased (0.860790 --> 0.822395).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 57.9710156917572
Epoch: 8, Steps: 87 | Train Loss: 0.5037122 Vali Loss: 0.7886533 Test Loss: 0.9138870
Validation loss decreased (0.822395 --> 0.788653).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 59.45322346687317
Epoch: 9, Steps: 87 | Train Loss: 0.4714719 Vali Loss: 0.7572043 Test Loss: 0.8781329
Validation loss decreased (0.788653 --> 0.757204).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 59.11007332801819
Epoch: 10, Steps: 87 | Train Loss: 0.4431350 Vali Loss: 0.7291844 Test Loss: 0.8453845
Validation loss decreased (0.757204 --> 0.729184).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 59.210657835006714
Epoch: 11, Steps: 87 | Train Loss: 0.4278644 Vali Loss: 0.7036909 Test Loss: 0.8164865
Validation loss decreased (0.729184 --> 0.703691).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 59.30683517456055
Epoch: 12, Steps: 87 | Train Loss: 0.3952253 Vali Loss: 0.6809180 Test Loss: 0.7903531
Validation loss decreased (0.703691 --> 0.680918).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 59.573493003845215
Epoch: 13, Steps: 87 | Train Loss: 0.3749872 Vali Loss: 0.6609874 Test Loss: 0.7674057
Validation loss decreased (0.680918 --> 0.660987).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 58.781545639038086
Epoch: 14, Steps: 87 | Train Loss: 0.3567410 Vali Loss: 0.6432217 Test Loss: 0.7467772
Validation loss decreased (0.660987 --> 0.643222).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 58.360286235809326
Epoch: 15, Steps: 87 | Train Loss: 0.3402126 Vali Loss: 0.6237429 Test Loss: 0.7246581
Validation loss decreased (0.643222 --> 0.623743).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 58.307271242141724
Epoch: 16, Steps: 87 | Train Loss: 0.3252481 Vali Loss: 0.6080337 Test Loss: 0.7069192
Validation loss decreased (0.623743 --> 0.608034).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 58.19612169265747
Epoch: 17, Steps: 87 | Train Loss: 0.3116177 Vali Loss: 0.5937384 Test Loss: 0.6910728
Validation loss decreased (0.608034 --> 0.593738).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 58.48611855506897
Epoch: 18, Steps: 87 | Train Loss: 0.2992607 Vali Loss: 0.5802707 Test Loss: 0.6749027
Validation loss decreased (0.593738 --> 0.580271).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 59.46139645576477
Epoch: 19, Steps: 87 | Train Loss: 0.2879126 Vali Loss: 0.5692528 Test Loss: 0.6624830
Validation loss decreased (0.580271 --> 0.569253).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 58.75297737121582
Epoch: 20, Steps: 87 | Train Loss: 0.2775851 Vali Loss: 0.5571266 Test Loss: 0.6485173
Validation loss decreased (0.569253 --> 0.557127).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 60.30825924873352
Epoch: 21, Steps: 87 | Train Loss: 0.2680646 Vali Loss: 0.5478854 Test Loss: 0.6374494
Validation loss decreased (0.557127 --> 0.547885).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 61.09832143783569
Epoch: 22, Steps: 87 | Train Loss: 0.2593082 Vali Loss: 0.5370308 Test Loss: 0.6256632
Validation loss decreased (0.547885 --> 0.537031).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 59.796711444854736
Epoch: 23, Steps: 87 | Train Loss: 0.2512853 Vali Loss: 0.5285697 Test Loss: 0.6158968
Validation loss decreased (0.537031 --> 0.528570).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 59.1819281578064
Epoch: 24, Steps: 87 | Train Loss: 0.2438610 Vali Loss: 0.5205910 Test Loss: 0.6066730
Validation loss decreased (0.528570 --> 0.520591).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 58.909709215164185
Epoch: 25, Steps: 87 | Train Loss: 0.2369792 Vali Loss: 0.5130394 Test Loss: 0.5987207
Validation loss decreased (0.520591 --> 0.513039).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 59.92237043380737
Epoch: 26, Steps: 87 | Train Loss: 0.2306516 Vali Loss: 0.5065838 Test Loss: 0.5903112
Validation loss decreased (0.513039 --> 0.506584).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 61.201791763305664
Epoch: 27, Steps: 87 | Train Loss: 0.2247818 Vali Loss: 0.5000800 Test Loss: 0.5832729
Validation loss decreased (0.506584 --> 0.500080).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 60.425403118133545
Epoch: 28, Steps: 87 | Train Loss: 0.2193163 Vali Loss: 0.4941131 Test Loss: 0.5765887
Validation loss decreased (0.500080 --> 0.494113).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 58.90998673439026
Epoch: 29, Steps: 87 | Train Loss: 0.2142723 Vali Loss: 0.4878132 Test Loss: 0.5698092
Validation loss decreased (0.494113 --> 0.487813).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 58.937912940979004
Epoch: 30, Steps: 87 | Train Loss: 0.2095828 Vali Loss: 0.4831641 Test Loss: 0.5640969
Validation loss decreased (0.487813 --> 0.483164).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=258, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10760408064.0
params:  97902.0
Trainable parameters:  97902
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 54.69326066970825
Epoch: 1, Steps: 87 | Train Loss: 0.3040214 Vali Loss: 0.3654626 Test Loss: 0.4356260
Validation loss decreased (inf --> 0.365463).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 58.715031147003174
Epoch: 2, Steps: 87 | Train Loss: 0.2564055 Vali Loss: 0.3446203 Test Loss: 0.4262886
Validation loss decreased (0.365463 --> 0.344620).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 58.703410625457764
Epoch: 3, Steps: 87 | Train Loss: 0.2495653 Vali Loss: 0.3416879 Test Loss: 0.4250031
Validation loss decreased (0.344620 --> 0.341688).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 56.588799476623535
Epoch: 4, Steps: 87 | Train Loss: 0.2488567 Vali Loss: 0.3409740 Test Loss: 0.4247085
Validation loss decreased (0.341688 --> 0.340974).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 57.0649139881134
Epoch: 5, Steps: 87 | Train Loss: 0.2488599 Vali Loss: 0.3411930 Test Loss: 0.4242903
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 59.19384407997131
Epoch: 6, Steps: 87 | Train Loss: 0.2487118 Vali Loss: 0.3406247 Test Loss: 0.4248252
Validation loss decreased (0.340974 --> 0.340625).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 59.71935439109802
Epoch: 7, Steps: 87 | Train Loss: 0.2486147 Vali Loss: 0.3409477 Test Loss: 0.4246174
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 58.07913565635681
Epoch: 8, Steps: 87 | Train Loss: 0.2486875 Vali Loss: 0.3409251 Test Loss: 0.4245326
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 57.664353132247925
Epoch: 9, Steps: 87 | Train Loss: 0.2485933 Vali Loss: 0.3407136 Test Loss: 0.4242945
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 57.745805978775024
Epoch: 10, Steps: 87 | Train Loss: 0.2485680 Vali Loss: 0.3406390 Test Loss: 0.4243005
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 57.21636605262756
Epoch: 11, Steps: 87 | Train Loss: 0.2485823 Vali Loss: 0.3409218 Test Loss: 0.4242194
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_720_336_FITS_custom_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4226739799976349, mae:0.29179001808166504, rse:0.5279621481895447, corr:[0.2756121  0.2818019  0.28388384 0.28263655 0.28375664 0.284728
 0.28406116 0.28492987 0.28550622 0.28495353 0.28511813 0.28451726
 0.28330725 0.28346714 0.28365737 0.28333357 0.28354925 0.2835263
 0.2833391  0.283771   0.28365487 0.28303725 0.28317028 0.28356573
 0.2848501  0.28576362 0.28605318 0.28536302 0.2851107  0.28533804
 0.2847297  0.28427237 0.2848697  0.28533396 0.28537503 0.28551972
 0.2850397  0.28415617 0.28392988 0.28395745 0.2839444  0.28413618
 0.2844109  0.28473592 0.28503287 0.28463766 0.28405875 0.28430915
 0.28466162 0.2845718  0.2853139  0.285785   0.28526378 0.28512284
 0.28514528 0.2844366  0.28413934 0.28465506 0.28494534 0.2853803
 0.28600886 0.28552628 0.28458908 0.284563   0.28489873 0.28501847
 0.28525847 0.28512198 0.28491974 0.28513423 0.285102   0.28499112
 0.2852821  0.2853818  0.28510377 0.28526065 0.28535482 0.28468674
 0.28441668 0.2847131  0.28434268 0.283689   0.28363663 0.2836111
 0.28346163 0.28360814 0.28368837 0.28362587 0.28398278 0.28434154
 0.28433815 0.2844218  0.28449008 0.283957   0.28347376 0.28383034
 0.28423792 0.28471634 0.28528076 0.28525865 0.28458744 0.28365353
 0.28296402 0.28319654 0.28398356 0.28422034 0.2842375  0.28449577
 0.2841981  0.2835754  0.2837804  0.28419483 0.2838795  0.28354013
 0.28358608 0.28345814 0.28325665 0.2831578  0.28328416 0.28369746
 0.28391552 0.2840395  0.28447863 0.2848412  0.28447735 0.28412554
 0.2843759  0.28443345 0.28446722 0.28499642 0.28510502 0.28462732
 0.28455567 0.28462735 0.28443214 0.2845742  0.28475508 0.284352
 0.2839521  0.28378475 0.28359154 0.28387782 0.28450167 0.28472656
 0.28485972 0.28482667 0.28451544 0.2841627  0.28414676 0.28412116
 0.2839969  0.28440112 0.285123   0.28526524 0.28486443 0.28434488
 0.28382102 0.28362414 0.28384548 0.28423113 0.28465012 0.2846839
 0.2842774  0.28413764 0.28429118 0.2842826  0.28441238 0.28519118
 0.28647342 0.28622913 0.2857149  0.28543934 0.28548193 0.28560475
 0.28527117 0.28488335 0.2853333  0.28604794 0.28598255 0.285604
 0.28542578 0.28510246 0.28516018 0.28601024 0.28639337 0.2857483
 0.2852731  0.2852351  0.28470537 0.28407887 0.28427035 0.28506747
 0.28582186 0.28536868 0.28521326 0.2853839  0.28521582 0.28509092
 0.28522953 0.28513077 0.28514916 0.2856768  0.28550705 0.28442544
 0.2839755  0.28418383 0.2840574  0.28410542 0.2846584  0.2847079
 0.28419733 0.28395814 0.28383365 0.2836638  0.2837853  0.28415427
 0.2846314  0.28499818 0.28533602 0.28528637 0.28515446 0.28489017
 0.28417248 0.28375176 0.284079   0.28443044 0.2847141  0.28504416
 0.28472495 0.28391725 0.28347224 0.28330463 0.2835277  0.28423426
 0.28432307 0.28362238 0.28345522 0.2835161  0.28332838 0.2838869
 0.2847343  0.28455532 0.2842523  0.28457633 0.28461874 0.28415534
 0.2838055  0.2834603  0.28326365 0.28377378 0.28442344 0.28458613
 0.28463292 0.2843938  0.2835452  0.2830324  0.28351057 0.28406945
 0.28411672 0.2839289  0.28339288 0.28260848 0.28232923 0.28263578
 0.28286734 0.28333583 0.28370848 0.2836401  0.28302208 0.28209278
 0.28191102 0.2825818  0.2829673  0.2831167  0.283613   0.2837627
 0.2835014  0.2835971  0.2838183  0.2836104  0.2831562  0.28275633
 0.28269854 0.28301996 0.28314674 0.2830821  0.28338513 0.28385162
 0.28412265 0.28466868 0.28478116 0.2840794  0.28353575 0.28337336
 0.28294587 0.28271905 0.28306648 0.28301564 0.28301156 0.28392458
 0.2841985  0.2832236  0.28288886 0.28334296 0.28351232 0.28372356
 0.28384376 0.2834076  0.28333357 0.2836992  0.28370276 0.2837573
 0.28397664 0.28371048 0.28376752 0.28402203 0.28345686 0.2830271
 0.28334093 0.28301516 0.2827038  0.28358123 0.28413612 0.28436112
 0.28502667 0.28454608 0.28389946 0.2846627  0.28411967 0.2828985
 0.28342634 0.28250384 0.28155676 0.28313887 0.2808884  0.28565794]
