Args in experiment:
Namespace(is_training=1, model_id='traffic_192_720', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=48, pred_len=720, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=82, base_T=24, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_192_720_FITS_custom_ftM_sl192_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=82, out_features=389, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3519497728.0
params:  32287.0
Trainable parameters:  32287
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 55.51622152328491
Epoch: 1, Steps: 88 | Train Loss: 1.7457255 Vali Loss: 1.5374470 Test Loss: 1.8554997
Validation loss decreased (inf --> 1.537447).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 55.5584876537323
Epoch: 2, Steps: 88 | Train Loss: 1.0098405 Vali Loss: 1.0871018 Test Loss: 1.3022532
Validation loss decreased (1.537447 --> 1.087102).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 56.857972145080566
Epoch: 3, Steps: 88 | Train Loss: 0.7655537 Vali Loss: 0.9355500 Test Loss: 1.1162592
Validation loss decreased (1.087102 --> 0.935550).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 58.32067799568176
Epoch: 4, Steps: 88 | Train Loss: 0.6715297 Vali Loss: 0.8643975 Test Loss: 1.0291065
Validation loss decreased (0.935550 --> 0.864398).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 56.450180530548096
Epoch: 5, Steps: 88 | Train Loss: 0.6189751 Vali Loss: 0.8167915 Test Loss: 0.9707051
Validation loss decreased (0.864398 --> 0.816792).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 57.351508140563965
Epoch: 6, Steps: 88 | Train Loss: 0.5801222 Vali Loss: 0.7777935 Test Loss: 0.9233148
Validation loss decreased (0.816792 --> 0.777794).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 59.08187937736511
Epoch: 7, Steps: 88 | Train Loss: 0.5478443 Vali Loss: 0.7449806 Test Loss: 0.8830324
Validation loss decreased (0.777794 --> 0.744981).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 59.65530300140381
Epoch: 8, Steps: 88 | Train Loss: 0.5200056 Vali Loss: 0.7151790 Test Loss: 0.8472896
Validation loss decreased (0.744981 --> 0.715179).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 61.221020221710205
Epoch: 9, Steps: 88 | Train Loss: 0.4957089 Vali Loss: 0.6900300 Test Loss: 0.8166815
Validation loss decreased (0.715179 --> 0.690030).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 57.80620288848877
Epoch: 10, Steps: 88 | Train Loss: 0.4743582 Vali Loss: 0.6674034 Test Loss: 0.7895274
Validation loss decreased (0.690030 --> 0.667403).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 62.80420923233032
Epoch: 11, Steps: 88 | Train Loss: 0.4555189 Vali Loss: 0.6481380 Test Loss: 0.7657349
Validation loss decreased (0.667403 --> 0.648138).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 57.71434140205383
Epoch: 12, Steps: 88 | Train Loss: 0.4388295 Vali Loss: 0.6303922 Test Loss: 0.7446291
Validation loss decreased (0.648138 --> 0.630392).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 58.356438398361206
Epoch: 13, Steps: 88 | Train Loss: 0.4238399 Vali Loss: 0.6143939 Test Loss: 0.7258008
Validation loss decreased (0.630392 --> 0.614394).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 60.4598388671875
Epoch: 14, Steps: 88 | Train Loss: 0.4104675 Vali Loss: 0.6003472 Test Loss: 0.7088978
Validation loss decreased (0.614394 --> 0.600347).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 58.34183359146118
Epoch: 15, Steps: 88 | Train Loss: 0.3984970 Vali Loss: 0.5880767 Test Loss: 0.6935434
Validation loss decreased (0.600347 --> 0.588077).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 58.2676477432251
Epoch: 16, Steps: 88 | Train Loss: 0.3877191 Vali Loss: 0.5765930 Test Loss: 0.6803564
Validation loss decreased (0.588077 --> 0.576593).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 57.78493595123291
Epoch: 17, Steps: 88 | Train Loss: 0.3780253 Vali Loss: 0.5659366 Test Loss: 0.6676561
Validation loss decreased (0.576593 --> 0.565937).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 59.21991157531738
Epoch: 18, Steps: 88 | Train Loss: 0.3691726 Vali Loss: 0.5568883 Test Loss: 0.6569597
Validation loss decreased (0.565937 --> 0.556888).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 61.02090096473694
Epoch: 19, Steps: 88 | Train Loss: 0.3612310 Vali Loss: 0.5486392 Test Loss: 0.6469095
Validation loss decreased (0.556888 --> 0.548639).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 62.18027663230896
Epoch: 20, Steps: 88 | Train Loss: 0.3540298 Vali Loss: 0.5413418 Test Loss: 0.6377956
Validation loss decreased (0.548639 --> 0.541342).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 59.73539185523987
Epoch: 21, Steps: 88 | Train Loss: 0.3473992 Vali Loss: 0.5343208 Test Loss: 0.6295376
Validation loss decreased (0.541342 --> 0.534321).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 59.17147636413574
Epoch: 22, Steps: 88 | Train Loss: 0.3413577 Vali Loss: 0.5283630 Test Loss: 0.6220881
Validation loss decreased (0.534321 --> 0.528363).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 57.83605074882507
Epoch: 23, Steps: 88 | Train Loss: 0.3358212 Vali Loss: 0.5221097 Test Loss: 0.6151493
Validation loss decreased (0.528363 --> 0.522110).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 57.7386953830719
Epoch: 24, Steps: 88 | Train Loss: 0.3307227 Vali Loss: 0.5170721 Test Loss: 0.6089549
Validation loss decreased (0.522110 --> 0.517072).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 58.567655086517334
Epoch: 25, Steps: 88 | Train Loss: 0.3261477 Vali Loss: 0.5119115 Test Loss: 0.6031393
Validation loss decreased (0.517072 --> 0.511912).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 58.10691213607788
Epoch: 26, Steps: 88 | Train Loss: 0.3218641 Vali Loss: 0.5076544 Test Loss: 0.5976104
Validation loss decreased (0.511912 --> 0.507654).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 56.15657353401184
Epoch: 27, Steps: 88 | Train Loss: 0.3179657 Vali Loss: 0.5031704 Test Loss: 0.5928032
Validation loss decreased (0.507654 --> 0.503170).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 58.96813750267029
Epoch: 28, Steps: 88 | Train Loss: 0.3143006 Vali Loss: 0.4990441 Test Loss: 0.5882004
Validation loss decreased (0.503170 --> 0.499044).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 59.241228342056274
Epoch: 29, Steps: 88 | Train Loss: 0.3109369 Vali Loss: 0.4964710 Test Loss: 0.5841292
Validation loss decreased (0.499044 --> 0.496471).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 61.11884045600891
Epoch: 30, Steps: 88 | Train Loss: 0.3078751 Vali Loss: 0.4931853 Test Loss: 0.5801684
Validation loss decreased (0.496471 --> 0.493185).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11369
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=82, out_features=389, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3519497728.0
params:  32287.0
Trainable parameters:  32287
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 61.03334355354309
Epoch: 1, Steps: 88 | Train Loss: 0.3478958 Vali Loss: 0.4526151 Test Loss: 0.5325851
Validation loss decreased (inf --> 0.452615).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 59.21803641319275
Epoch: 2, Steps: 88 | Train Loss: 0.3234451 Vali Loss: 0.4363584 Test Loss: 0.5132339
Validation loss decreased (0.452615 --> 0.436358).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 57.958608865737915
Epoch: 3, Steps: 88 | Train Loss: 0.3135915 Vali Loss: 0.4305515 Test Loss: 0.5057647
Validation loss decreased (0.436358 --> 0.430552).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 56.62916588783264
Epoch: 4, Steps: 88 | Train Loss: 0.3100193 Vali Loss: 0.4287968 Test Loss: 0.5036771
Validation loss decreased (0.430552 --> 0.428797).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 58.47015690803528
Epoch: 5, Steps: 88 | Train Loss: 0.3087965 Vali Loss: 0.4282405 Test Loss: 0.5030700
Validation loss decreased (0.428797 --> 0.428240).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 58.547555685043335
Epoch: 6, Steps: 88 | Train Loss: 0.3084907 Vali Loss: 0.4277653 Test Loss: 0.5027711
Validation loss decreased (0.428240 --> 0.427765).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 59.68227934837341
Epoch: 7, Steps: 88 | Train Loss: 0.3083181 Vali Loss: 0.4278938 Test Loss: 0.5027425
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 61.722641706466675
Epoch: 8, Steps: 88 | Train Loss: 0.3082905 Vali Loss: 0.4276403 Test Loss: 0.5028445
Validation loss decreased (0.427765 --> 0.427640).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 57.9252450466156
Epoch: 9, Steps: 88 | Train Loss: 0.3083326 Vali Loss: 0.4277769 Test Loss: 0.5025663
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 56.139259338378906
Epoch: 10, Steps: 88 | Train Loss: 0.3082744 Vali Loss: 0.4274712 Test Loss: 0.5025793
Validation loss decreased (0.427640 --> 0.427471).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 57.13945555686951
Epoch: 11, Steps: 88 | Train Loss: 0.3081931 Vali Loss: 0.4274624 Test Loss: 0.5024231
Validation loss decreased (0.427471 --> 0.427462).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 58.486130237579346
Epoch: 12, Steps: 88 | Train Loss: 0.3081952 Vali Loss: 0.4275376 Test Loss: 0.5025595
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 58.0844521522522
Epoch: 13, Steps: 88 | Train Loss: 0.3082430 Vali Loss: 0.4273512 Test Loss: 0.5026853
Validation loss decreased (0.427462 --> 0.427351).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 58.04643130302429
Epoch: 14, Steps: 88 | Train Loss: 0.3082368 Vali Loss: 0.4278856 Test Loss: 0.5024683
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 59.816519260406494
Epoch: 15, Steps: 88 | Train Loss: 0.3081609 Vali Loss: 0.4274788 Test Loss: 0.5022606
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 55.13935995101929
Epoch: 16, Steps: 88 | Train Loss: 0.3081663 Vali Loss: 0.4275348 Test Loss: 0.5024270
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 58.08101940155029
Epoch: 17, Steps: 88 | Train Loss: 0.3081662 Vali Loss: 0.4275997 Test Loss: 0.5024374
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 59.081138610839844
Epoch: 18, Steps: 88 | Train Loss: 0.3081854 Vali Loss: 0.4275326 Test Loss: 0.5023581
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_192_720_FITS_custom_ftM_sl192_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.5005558133125305, mae:0.3234402537345886, rse:0.5785315632820129, corr:[0.25332034 0.2667694  0.2650592  0.26522353 0.26558888 0.26492068
 0.26589316 0.2652849  0.26621974 0.2663264  0.26560387 0.2656404
 0.26488712 0.26526996 0.26513892 0.26467896 0.26518166 0.26501372
 0.26542684 0.2654422  0.26497287 0.2650247  0.26435903 0.2648556
 0.26620087 0.26577446 0.2658407  0.26534405 0.26487577 0.26514542
 0.26523256 0.2657034  0.26546913 0.26499373 0.2649861  0.26444182
 0.26429653 0.2645515  0.26491213 0.26521987 0.2648382  0.26451764
 0.2642097  0.2638784  0.26390952 0.26400396 0.2642842  0.2642244
 0.26436523 0.26448166 0.2642179  0.26427716 0.26418486 0.26371923
 0.2641493  0.26416826 0.2640208  0.26436415 0.2639902  0.26407626
 0.26425278 0.26371887 0.2636282  0.26330936 0.2631336  0.26339203
 0.26336926 0.26338577 0.26313636 0.26323235 0.26368886 0.26360312
 0.2634688  0.26318356 0.26282734 0.26290557 0.2628289  0.2628749
 0.26275826 0.26261178 0.26312062 0.26298442 0.26283038 0.26329744
 0.26294705 0.2627672  0.26288462 0.2627158  0.2630815  0.26326007
 0.2633143  0.26329333 0.26312906 0.26300967 0.26278695 0.26295668
 0.2630528  0.2630015  0.26325962 0.26317596 0.26271856 0.26263598
 0.26277268 0.26307827 0.26320443 0.26298624 0.26285356 0.26273382
 0.2626379  0.2627127  0.2627733  0.2628518  0.2627222  0.26269916
 0.26269203 0.26207995 0.26193488 0.26221758 0.26271945 0.26324564
 0.26295608 0.26298937 0.26296538 0.2627086  0.26287082 0.26265642
 0.26264265 0.26284882 0.2625925  0.262833   0.26289892 0.2627221
 0.26317284 0.26345757 0.2636128  0.26378202 0.26356122 0.26309192
 0.26298535 0.26305073 0.26335552 0.26341787 0.26398426 0.26477456
 0.2651478  0.26565427 0.26578075 0.2658799  0.26604187 0.26573703
 0.2658291  0.26608953 0.2661711  0.2662265  0.2656752  0.2655775
 0.2659289  0.26608324 0.26652318 0.26676062 0.26672852 0.26678774
 0.2668928  0.26687443 0.26647213 0.26605722 0.266023   0.26681697
 0.26835772 0.2681807  0.26751068 0.26717955 0.26698294 0.26658165
 0.26622114 0.26642895 0.26683548 0.26695034 0.26706123 0.2669106
 0.26633343 0.26608926 0.26609612 0.2663395  0.2667799  0.26672152
 0.26677832 0.2669049  0.26657537 0.2664669  0.26618382 0.26613772
 0.26711416 0.26696852 0.26664397 0.26631048 0.26591206 0.26625288
 0.2662877  0.2657424  0.26570073 0.2659374  0.2659208  0.26572144
 0.2656172  0.26576138 0.2658318  0.2658718  0.2661082  0.2659341
 0.26575324 0.26593432 0.26562417 0.26551676 0.26584324 0.26577404
 0.2657355  0.26536843 0.26539764 0.26575825 0.265757   0.26574698
 0.26559135 0.26524827 0.26537123 0.2655199  0.2653573  0.2654657
 0.26543424 0.2650089  0.26500103 0.2651016  0.26529318 0.26565418
 0.26545286 0.2651342  0.26498842 0.26473317 0.26472893 0.26496074
 0.2653672  0.26542017 0.26511985 0.26483333 0.26451087 0.26451385
 0.26450628 0.26412976 0.26409253 0.26423958 0.26442003 0.26486477
 0.26497668 0.2649006  0.2647127  0.26426053 0.26438475 0.2647779
 0.26480785 0.26463255 0.26455334 0.26497254 0.26505005 0.2646781
 0.26473507 0.26487383 0.26477236 0.2650682  0.26517057 0.26498973
 0.2648645  0.2647764  0.2647426  0.26469088 0.2647595  0.2648218
 0.2646252  0.2650468  0.26555    0.26529044 0.26520672 0.26517582
 0.2650035  0.26482603 0.26464427 0.26479694 0.26501122 0.26479107
 0.2643628  0.2646965  0.26515916 0.26527125 0.26524603 0.2651209
 0.26514938 0.26535007 0.26527676 0.26535007 0.26556557 0.26558602
 0.26555794 0.2654657  0.2654051  0.26549453 0.2654809  0.26525205
 0.26538655 0.26553914 0.265743   0.26562443 0.26574317 0.26668596
 0.26686472 0.2668752  0.26742256 0.26771045 0.2674459  0.26705018
 0.26740932 0.26807114 0.26779872 0.26752627 0.26756698 0.26774785
 0.26799724 0.26771605 0.2673154  0.26724243 0.26762694 0.26816565
 0.26818037 0.26790687 0.26775882 0.26759672 0.26752844 0.26790753
 0.2685555  0.2682876  0.2677822  0.26780775 0.26766458 0.26706335
 0.26717997 0.26747072 0.26741233 0.2674137  0.2672861  0.26733917
 0.26734263 0.2671849  0.2670906  0.2668996  0.26730284 0.26776332
 0.26781958 0.26808262 0.26789305 0.26740474 0.26692316 0.26659074
 0.26723608 0.2670278  0.266739   0.26699522 0.2666583  0.26666585
 0.2668376  0.26617226 0.26622328 0.26665974 0.26667067 0.26681063
 0.26647872 0.2662553  0.26623806 0.26599446 0.2663275  0.2667111
 0.2668757  0.26688436 0.26649597 0.2664328  0.26610944 0.26565975
 0.26587188 0.26572627 0.26588556 0.26610512 0.26596218 0.26636627
 0.26653042 0.26599282 0.26567382 0.2657096  0.26606178 0.26619694
 0.26592872 0.26574677 0.2654033  0.26531565 0.2655262  0.26554707
 0.26574585 0.26549843 0.2648377  0.26489353 0.26526272 0.26550198
 0.26525304 0.26508424 0.26533923 0.26523545 0.26524907 0.26547348
 0.26538008 0.26541203 0.26530656 0.26520386 0.26560855 0.26570475
 0.26538107 0.26515457 0.26499686 0.26489475 0.2649768  0.2652599
 0.26515746 0.26453152 0.2645168  0.26494974 0.26487294 0.26479375
 0.26478466 0.26481274 0.264629   0.26472336 0.26484805 0.2644599
 0.26469916 0.26520088 0.2650161  0.26481235 0.26488054 0.2651966
 0.26537135 0.26526594 0.2653806  0.2653647  0.26531684 0.26535863
 0.26507002 0.26464987 0.26450002 0.2647098  0.26505902 0.26498827
 0.2644369  0.26437485 0.26466116 0.26529372 0.26529267 0.26492402
 0.2651464  0.26533234 0.26522702 0.2650845  0.26511285 0.26538962
 0.26519677 0.2649815  0.2652153  0.26537788 0.2657401  0.266009
 0.2659353  0.26614684 0.26630476 0.26629433 0.2665733  0.2671503
 0.26726925 0.26702675 0.26715818 0.2676787  0.26766092 0.26743302
 0.26773477 0.2682212  0.26808748 0.2676845  0.26785365 0.26797944
 0.26766098 0.26753885 0.26722357 0.26697516 0.26736712 0.26761743
 0.26768348 0.26789725 0.26783252 0.26729527 0.2668045  0.2675304
 0.26844642 0.2680125  0.2679737  0.2680408  0.26731423 0.26667586
 0.26675197 0.26711634 0.267158   0.26705894 0.2670539  0.2670012
 0.2669923  0.2668861  0.26662722 0.26656902 0.2666217  0.26663733
 0.26657286 0.2665092  0.26657218 0.2662446  0.26566297 0.26575628
 0.26647854 0.26686007 0.26726815 0.26711464 0.26680374 0.266825
 0.26684645 0.26685438 0.26691756 0.26675788 0.26652235 0.26668748
 0.2666849  0.26650128 0.26639017 0.26595968 0.26572818 0.26578778
 0.2658901  0.2660513  0.2658238  0.26583976 0.26599583 0.2656891
 0.26603755 0.2662613  0.26610214 0.26634917 0.26639622 0.26623985
 0.26616305 0.26585406 0.26600286 0.2660119  0.26562622 0.26585162
 0.2656996  0.2651505  0.26494995 0.26495415 0.2653788  0.26547348
 0.26547283 0.26552513 0.26495603 0.26500565 0.26529643 0.2650421
 0.26510066 0.26513588 0.26502627 0.26497295 0.26502615 0.26518452
 0.2647866  0.26444387 0.26455387 0.2642049  0.26438007 0.26486927
 0.26443338 0.2643952  0.26451805 0.26396677 0.26366884 0.26364422
 0.26377252 0.26390573 0.26412106 0.2643094  0.26388547 0.26375112
 0.26368666 0.2635859  0.263767   0.26366636 0.26363453 0.26368973
 0.26354584 0.26356384 0.26320392 0.26283264 0.2631252  0.2634146
 0.26356888 0.26369774 0.2638365  0.26398918 0.26367718 0.26339972
 0.26339594 0.2632661  0.26345417 0.263529   0.26344156 0.2635254
 0.26329115 0.26374128 0.26401702 0.26384124 0.26382497 0.2635386
 0.26367268 0.2637095  0.26320434 0.26345918 0.2636056  0.263512
 0.26345494 0.2631137  0.26351666 0.26371384 0.26335636 0.2635755
 0.26361993 0.2641163  0.2643279  0.26354116 0.26414454 0.26516435
 0.26514465 0.26533815 0.2655806  0.26598126 0.26548228 0.26477844
 0.26522976 0.264966   0.26446998 0.26469958 0.26478234 0.26503703
 0.26503918 0.26515606 0.26554868 0.26534998 0.26536095 0.26552424
 0.26556703 0.265676   0.26520047 0.26475474 0.2646126  0.26510268
 0.26666293 0.26673597 0.26627985 0.26585826 0.26504916 0.26506996
 0.2651577  0.26484418 0.26495808 0.26521674 0.26573774 0.26566926
 0.26555294 0.26601887 0.2658547  0.26600787 0.2662299  0.26606262
 0.26620704 0.26582325 0.26577204 0.26569292 0.26479995 0.26534006
 0.26676586 0.2669119  0.26678315 0.26606196 0.26608852 0.26623103
 0.2661597  0.26594225 0.26514637 0.2660191  0.26611447 0.26603732
 0.26722947 0.26663002 0.26749557 0.26746538 0.26661134 0.26759464
 0.26591635 0.26582184 0.26463446 0.2643089  0.26536483 0.26585656]
