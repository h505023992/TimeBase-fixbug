Args in experiment:
Namespace(is_training=1, model_id='ECL_96_192', model='iTransformer', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=192, enc_in=321, dec_in=321, c_out=321, d_model=512, n_heads=8, e_layers=3, d_layers=1, d_ff=512, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=64, patience=3, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=2, use_multi_gpu=False, devices='0,1,2,3', exp_name='MTSF', channel_independence=False, inverse=False, class_strategy='projection', target_root_path='./data/electricity/', target_data_path='electricity.csv', efficient_training=False, use_norm=True, partial_start_index=0)
Use GPU: cuda:2
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.
MACs: 1595885568.0
Params: 4883136.0
1.60G MACs
>>>>>>>start training : ECL_96_192_iTransformer_custom_M_ft96_sl48_ll192_pl512_dm8_nh3_el1_dl512_df1_fctimeF_ebTrue_dtExp_projection_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 18125
val 2441
test 5069
	iters: 100, epoch: 1 | loss: 0.2554885
	speed: 0.1298s/iter; left time: 354.5396s
	iters: 200, epoch: 1 | loss: 0.2193654
	speed: 0.1116s/iter; left time: 293.5226s
Max Memory (MB): 3048.3017578125
Epoch: 1 cost time: 33.527180671691895
Epoch: 1, Steps: 283 | Train Loss: 0.2689443 Vali Loss: 0.1817505 Test Loss: 0.2048093
Validation loss decreased (inf --> 0.181750).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2158497
	speed: 0.8713s/iter; left time: 2132.9496s
	iters: 200, epoch: 2 | loss: 0.2145218
	speed: 0.1163s/iter; left time: 273.0470s
Max Memory (MB): 3048.3017578125
Epoch: 2 cost time: 33.88310432434082
Epoch: 2, Steps: 283 | Train Loss: 0.2082410 Vali Loss: 0.1685392 Test Loss: 0.1913745
Validation loss decreased (0.181750 --> 0.168539).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.1968503
	speed: 1.1350s/iter; left time: 2457.2226s
	iters: 200, epoch: 3 | loss: 0.1853759
	speed: 0.1137s/iter; left time: 234.7330s
Max Memory (MB): 3048.3017578125
Epoch: 3 cost time: 33.110459089279175
Epoch: 3, Steps: 283 | Train Loss: 0.1982741 Vali Loss: 0.1642262 Test Loss: 0.1870251
Validation loss decreased (0.168539 --> 0.164226).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1922531
	speed: 1.1402s/iter; left time: 2145.8661s
	iters: 200, epoch: 4 | loss: 0.1997972
	speed: 0.1129s/iter; left time: 201.1276s
Max Memory (MB): 3048.3017578125
Epoch: 4 cost time: 32.66169595718384
Epoch: 4, Steps: 283 | Train Loss: 0.1945060 Vali Loss: 0.1620204 Test Loss: 0.1842254
Validation loss decreased (0.164226 --> 0.162020).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.1944414
	speed: 1.0294s/iter; left time: 1646.0227s
	iters: 200, epoch: 5 | loss: 0.1940054
	speed: 0.1113s/iter; left time: 166.7849s
Max Memory (MB): 3048.3017578125
Epoch: 5 cost time: 32.17539668083191
Epoch: 5, Steps: 283 | Train Loss: 0.1926872 Vali Loss: 0.1612881 Test Loss: 0.1833912
Validation loss decreased (0.162020 --> 0.161288).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.1838484
	speed: 1.1884s/iter; left time: 1563.9097s
	iters: 200, epoch: 6 | loss: 0.1918318
	speed: 0.1111s/iter; left time: 135.0699s
Max Memory (MB): 3048.3017578125
Epoch: 6 cost time: 32.49426317214966
Epoch: 6, Steps: 283 | Train Loss: 0.1918151 Vali Loss: 0.1607700 Test Loss: 0.1828363
Validation loss decreased (0.161288 --> 0.160770).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.1880240
	speed: 0.9659s/iter; left time: 997.7892s
	iters: 200, epoch: 7 | loss: 0.1918445
	speed: 0.1145s/iter; left time: 106.8225s
Max Memory (MB): 3048.3017578125
Epoch: 7 cost time: 33.233612060546875
Epoch: 7, Steps: 283 | Train Loss: 0.1912972 Vali Loss: 0.1606906 Test Loss: 0.1825303
Validation loss decreased (0.160770 --> 0.160691).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.1902040
	speed: 0.8686s/iter; left time: 651.4126s
	iters: 200, epoch: 8 | loss: 0.1896666
	speed: 0.1126s/iter; left time: 73.2127s
Max Memory (MB): 3048.3017578125
Epoch: 8 cost time: 32.97063684463501
Epoch: 8, Steps: 283 | Train Loss: 0.1911143 Vali Loss: 0.1606723 Test Loss: 0.1824535
Validation loss decreased (0.160691 --> 0.160672).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.2146366
	speed: 1.2299s/iter; left time: 574.3463s
	iters: 200, epoch: 9 | loss: 0.2000980
	speed: 0.1162s/iter; left time: 42.6628s
Max Memory (MB): 3048.3017578125
Epoch: 9 cost time: 34.1177613735199
Epoch: 9, Steps: 283 | Train Loss: 0.1909828 Vali Loss: 0.1604622 Test Loss: 0.1823695
Validation loss decreased (0.160672 --> 0.160462).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.1922584
	speed: 1.0679s/iter; left time: 196.4857s
	iters: 200, epoch: 10 | loss: 0.1958542
	speed: 0.1151s/iter; left time: 9.6681s
Max Memory (MB): 3048.3017578125
Epoch: 10 cost time: 33.59730553627014
Epoch: 10, Steps: 283 | Train Loss: 0.1908855 Vali Loss: 0.1604344 Test Loss: 0.1823446
Validation loss decreased (0.160462 --> 0.160434).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : ECL_96_192_iTransformer_custom_M_ft96_sl48_ll192_pl512_dm8_nh3_el1_dl512_df1_fctimeF_ebTrue_dtExp_projection_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5069
test shape: (5069, 1, 192, 321) (5069, 1, 192, 321)
test shape: (5069, 192, 321) (5069, 192, 321)
mse:0.18234466016292572, mae:0.2710482180118561
