Args in experiment:
Namespace(is_training=1, model_id='traffic_96_336', model='FITS', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=336, individual=False, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, ab=2, hidden_size=1, kernel=5, groups=1, levels=3, stacks=1, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=5, learning_rate=0.0005, des='Exp', loss='mse', lradj='type3', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, aug_method='NA', aug_rate=0.5, in_batch_augmentation=False, in_dataset_augmentation=False, data_size=1, aug_data_size=1, seed=0, testset_div=2, test_time_train=False, train_mode=2, cut_freq=34, base_T=36, H_order=8)
Use GPU: cuda:0
>>>>>>>start training : traffic_96_336_FITS_custom_ftM_sl96_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11849
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=34, out_features=153, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  573967872.0
params:  5355.0
Trainable parameters:  5355
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 25.892982006072998
Epoch: 1, Steps: 92 | Train Loss: 1.7502662 Vali Loss: 1.7093071 Test Loss: 2.0421853
Validation loss decreased (inf --> 1.709307).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 26.038805723190308
Epoch: 2, Steps: 92 | Train Loss: 1.1493702 Vali Loss: 1.2528710 Test Loss: 1.4925274
Validation loss decreased (1.709307 --> 1.252871).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 25.674757957458496
Epoch: 3, Steps: 92 | Train Loss: 0.8682585 Vali Loss: 1.0399735 Test Loss: 1.2369901
Validation loss decreased (1.252871 --> 1.039973).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 27.038057804107666
Epoch: 4, Steps: 92 | Train Loss: 0.7306682 Vali Loss: 0.9292204 Test Loss: 1.1050100
Validation loss decreased (1.039973 --> 0.929220).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 27.95795488357544
Epoch: 5, Steps: 92 | Train Loss: 0.6542776 Vali Loss: 0.8630835 Test Loss: 1.0265369
Validation loss decreased (0.929220 --> 0.863083).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 27.312732934951782
Epoch: 6, Steps: 92 | Train Loss: 0.6049940 Vali Loss: 0.8168244 Test Loss: 0.9724912
Validation loss decreased (0.863083 --> 0.816824).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 28.375373363494873
Epoch: 7, Steps: 92 | Train Loss: 0.5689848 Vali Loss: 0.7810959 Test Loss: 0.9306285
Validation loss decreased (0.816824 --> 0.781096).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 25.120837926864624
Epoch: 8, Steps: 92 | Train Loss: 0.5401843 Vali Loss: 0.7510634 Test Loss: 0.8961136
Validation loss decreased (0.781096 --> 0.751063).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 24.848793029785156
Epoch: 9, Steps: 92 | Train Loss: 0.5158839 Vali Loss: 0.7255766 Test Loss: 0.8666540
Validation loss decreased (0.751063 --> 0.725577).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 24.522932052612305
Epoch: 10, Steps: 92 | Train Loss: 0.4949161 Vali Loss: 0.7034838 Test Loss: 0.8409792
Validation loss decreased (0.725577 --> 0.703484).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 25.727975130081177
Epoch: 11, Steps: 92 | Train Loss: 0.4767534 Vali Loss: 0.6845381 Test Loss: 0.8187910
Validation loss decreased (0.703484 --> 0.684538).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 26.79768395423889
Epoch: 12, Steps: 92 | Train Loss: 0.4606050 Vali Loss: 0.6670457 Test Loss: 0.7994086
Validation loss decreased (0.684538 --> 0.667046).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 26.886621713638306
Epoch: 13, Steps: 92 | Train Loss: 0.4463043 Vali Loss: 0.6516194 Test Loss: 0.7820259
Validation loss decreased (0.667046 --> 0.651619).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 26.451688766479492
Epoch: 14, Steps: 92 | Train Loss: 0.4337522 Vali Loss: 0.6384871 Test Loss: 0.7665200
Validation loss decreased (0.651619 --> 0.638487).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 28.374633073806763
Epoch: 15, Steps: 92 | Train Loss: 0.4224251 Vali Loss: 0.6266220 Test Loss: 0.7529942
Validation loss decreased (0.638487 --> 0.626622).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 24.788602828979492
Epoch: 16, Steps: 92 | Train Loss: 0.4123847 Vali Loss: 0.6160698 Test Loss: 0.7409586
Validation loss decreased (0.626622 --> 0.616070).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 25.05699110031128
Epoch: 17, Steps: 92 | Train Loss: 0.4033763 Vali Loss: 0.6059446 Test Loss: 0.7301047
Validation loss decreased (0.616070 --> 0.605945).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 26.18392539024353
Epoch: 18, Steps: 92 | Train Loss: 0.3952579 Vali Loss: 0.5973602 Test Loss: 0.7202949
Validation loss decreased (0.605945 --> 0.597360).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 25.85923957824707
Epoch: 19, Steps: 92 | Train Loss: 0.3879632 Vali Loss: 0.5897214 Test Loss: 0.7117562
Validation loss decreased (0.597360 --> 0.589721).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 27.34304404258728
Epoch: 20, Steps: 92 | Train Loss: 0.3814174 Vali Loss: 0.5826039 Test Loss: 0.7038496
Validation loss decreased (0.589721 --> 0.582604).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 25.618601322174072
Epoch: 21, Steps: 92 | Train Loss: 0.3754396 Vali Loss: 0.5760360 Test Loss: 0.6967932
Validation loss decreased (0.582604 --> 0.576036).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 24.507215976715088
Epoch: 22, Steps: 92 | Train Loss: 0.3700576 Vali Loss: 0.5706106 Test Loss: 0.6902975
Validation loss decreased (0.576036 --> 0.570611).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 24.7450168132782
Epoch: 23, Steps: 92 | Train Loss: 0.3651534 Vali Loss: 0.5652105 Test Loss: 0.6845466
Validation loss decreased (0.570611 --> 0.565210).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 24.687135219573975
Epoch: 24, Steps: 92 | Train Loss: 0.3606358 Vali Loss: 0.5603790 Test Loss: 0.6792924
Validation loss decreased (0.565210 --> 0.560379).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 24.12117052078247
Epoch: 25, Steps: 92 | Train Loss: 0.3565758 Vali Loss: 0.5562644 Test Loss: 0.6746370
Validation loss decreased (0.560379 --> 0.556264).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 25.16915202140808
Epoch: 26, Steps: 92 | Train Loss: 0.3528274 Vali Loss: 0.5525134 Test Loss: 0.6703063
Validation loss decreased (0.556264 --> 0.552513).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 26.438197135925293
Epoch: 27, Steps: 92 | Train Loss: 0.3494112 Vali Loss: 0.5481559 Test Loss: 0.6662449
Validation loss decreased (0.552513 --> 0.548156).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 26.636762857437134
Epoch: 28, Steps: 92 | Train Loss: 0.3463080 Vali Loss: 0.5446737 Test Loss: 0.6626331
Validation loss decreased (0.548156 --> 0.544674).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 25.912909269332886
Epoch: 29, Steps: 92 | Train Loss: 0.3433996 Vali Loss: 0.5418549 Test Loss: 0.6593931
Validation loss decreased (0.544674 --> 0.541855).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 25.114466667175293
Epoch: 30, Steps: 92 | Train Loss: 0.3407661 Vali Loss: 0.5393468 Test Loss: 0.6563157
Validation loss decreased (0.541855 --> 0.539347).  Saving model ...
Updating learning rate to 0.00011296777049628277
train 11849
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=34, out_features=153, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  573967872.0
params:  5355.0
Trainable parameters:  5355
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 24.629241466522217
Epoch: 1, Steps: 92 | Train Loss: 0.4012355 Vali Loss: 0.5069645 Test Loss: 0.6231549
Validation loss decreased (inf --> 0.506965).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 25.452059984207153
Epoch: 2, Steps: 92 | Train Loss: 0.3829657 Vali Loss: 0.4966342 Test Loss: 0.6132287
Validation loss decreased (0.506965 --> 0.496634).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 25.61382222175598
Epoch: 3, Steps: 92 | Train Loss: 0.3771048 Vali Loss: 0.4934418 Test Loss: 0.6115254
Validation loss decreased (0.496634 --> 0.493442).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 26.536969661712646
Epoch: 4, Steps: 92 | Train Loss: 0.3756031 Vali Loss: 0.4933373 Test Loss: 0.6117063
Validation loss decreased (0.493442 --> 0.493337).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 27.074559450149536
Epoch: 5, Steps: 92 | Train Loss: 0.3751393 Vali Loss: 0.4936670 Test Loss: 0.6117752
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 25.85513973236084
Epoch: 6, Steps: 92 | Train Loss: 0.3750231 Vali Loss: 0.4937255 Test Loss: 0.6119466
EarlyStopping counter: 2 out of 5
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 24.821062088012695
Epoch: 7, Steps: 92 | Train Loss: 0.3750279 Vali Loss: 0.4934119 Test Loss: 0.6118920
EarlyStopping counter: 3 out of 5
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 25.095993757247925
Epoch: 8, Steps: 92 | Train Loss: 0.3749801 Vali Loss: 0.4939196 Test Loss: 0.6118321
EarlyStopping counter: 4 out of 5
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 25.205355405807495
Epoch: 9, Steps: 92 | Train Loss: 0.3749807 Vali Loss: 0.4936478 Test Loss: 0.6118978
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : traffic_96_336_FITS_custom_ftM_sl96_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.6098471283912659, mae:0.3693353235721588, rse:0.6418148875236511, corr:[0.27047282 0.28218836 0.28323996 0.28166273 0.27991092 0.28163916
 0.28137198 0.28087774 0.28243643 0.2824494  0.28162137 0.28217354
 0.28153825 0.28057456 0.28095162 0.28059635 0.28002632 0.28093016
 0.28095657 0.28008986 0.2799122  0.27910995 0.2784202  0.2845853
 0.29748398 0.29896963 0.2968288  0.29590642 0.2951531  0.2946922
 0.29465622 0.2941073  0.2933188  0.2936473  0.29375097 0.2933707
 0.29375976 0.2936778  0.2928298  0.29303747 0.29375952 0.2938285
 0.29432753 0.2948355  0.29516843 0.29678404 0.29852915 0.29717606
 0.2873667  0.28199562 0.28087893 0.28068465 0.2816611  0.28203616
 0.28184354 0.28185245 0.28144923 0.2807158  0.28045207 0.28042737
 0.28078505 0.2817863  0.28203404 0.28131217 0.28116643 0.28201142
 0.28273645 0.28303438 0.2841596  0.28652653 0.28838667 0.28565714
 0.2779104  0.2758721  0.2774103  0.27835122 0.27835724 0.27844876
 0.2783955  0.2780082  0.2779585  0.27786696 0.27744895 0.27744237
 0.2779096  0.2779822  0.2776707  0.2776951  0.2779603  0.27828726
 0.27877423 0.27882963 0.27857983 0.27879298 0.2791862  0.27895224
 0.2781532  0.2787215  0.27900413 0.27870014 0.27874312 0.27900633
 0.27904433 0.27875504 0.2784503  0.27846867 0.27865812 0.27860916
 0.27858377 0.2787658  0.27865338 0.2782656  0.27823684 0.27860686
 0.27900162 0.27920997 0.27917117 0.27888584 0.27874386 0.2787705
 0.27867126 0.27862585 0.2788643  0.2789713  0.2787472  0.2786431
 0.27844325 0.27798045 0.27798474 0.27823168 0.27795318 0.2777947
 0.27821133 0.27834293 0.27810928 0.2781459  0.2782838  0.2784271
 0.27864033 0.2784721  0.2779316  0.277708   0.2774462  0.27719203
 0.27839735 0.27898142 0.2789117  0.27898985 0.27931738 0.27943385
 0.27890784 0.27808264 0.27758545 0.27778298 0.27807987 0.27783316
 0.27766404 0.27821314 0.278642   0.27858472 0.27877203 0.27885866
 0.27849647 0.27856588 0.2786968  0.27774975 0.27659744 0.2782769
 0.2848236  0.2874012  0.2865034  0.28498355 0.28345326 0.28270558
 0.28233218 0.28164577 0.28141263 0.2818198  0.28138554 0.28089693
 0.2813713  0.28139699 0.28072262 0.280817   0.2813824  0.28171864
 0.2821464  0.28216168 0.28104866 0.28000373 0.28030264 0.28586227
 0.2962288  0.29692575 0.2939473  0.29232943 0.2925122  0.29245898
 0.29184484 0.2911731  0.2905559  0.29065996 0.2911507  0.29119512
 0.29104367 0.2910948  0.29100764 0.29086587 0.29137084 0.292168
 0.2925852  0.2927182  0.29300854 0.29425338 0.29642048 0.29574722
 0.28550205 0.278918   0.27804524 0.27802017 0.27833366 0.27905923
 0.27926937 0.27857602 0.27800676 0.27783045 0.27739975 0.27708733
 0.2776329  0.27815863 0.2778222  0.27749157 0.2776383  0.27802852
 0.27865428 0.27927783 0.28047478 0.28286162 0.28522947 0.2831048
 0.27660057 0.2752888  0.27644756 0.27707106 0.27732262 0.2775675
 0.27777937 0.27771336 0.27741218 0.2772503  0.27738938 0.2772527
 0.27692002 0.27702016 0.2772216  0.27717844 0.27749586 0.27828673
 0.2786136  0.27828237 0.27806464 0.2781707  0.27831694 0.27823943
 0.2775746  0.27790868 0.27826813 0.2779678  0.27765548 0.27782837
 0.2777033  0.27716702 0.2774448  0.27811635 0.27807474 0.27800974
 0.27839687 0.27836367 0.2780451  0.27808747 0.2781548  0.27826595
 0.27854958 0.2781882  0.27742574 0.27744168 0.27773392 0.27744403
 0.27751565 0.27822644 0.27833188 0.27806342 0.27805546 0.2778406
 0.27745223 0.2773864  0.27737603 0.27746388 0.27776444 0.27750048
 0.27712935 0.27766493 0.27792192 0.277221   0.27703476 0.2774154
 0.27734518 0.27734122 0.27740443 0.2767919  0.27615464 0.27630833
 0.2771908  0.2774103  0.2778426  0.27767473 0.27707842 0.2773495
 0.27720988 0.27639344 0.276759   0.27733123 0.2770245  0.2774705
 0.27813482 0.27801707 0.27856588 0.27863798 0.27729118 0.2776896
 0.27815035 0.27575016 0.27568454 0.27622923 0.27213275 0.2795684 ]
